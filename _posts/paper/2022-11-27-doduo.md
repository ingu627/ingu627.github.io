---
layout: single
title: "[논문 리뷰] Annotating Columns with Pre-trained Language Models"
excerpt: "DODUO는 테이블의 전체를 입력으로 받아 테이블의 여러 컬럼 정보들을 이용하여 컬럼 유형과 컬럼 관계 예측을 출력하는 트랜스포머 기반 멀티태스크 학습 아키텍처입니다. 본 논문을 리뷰해보았습니다."
categories: paper
tags : [리뷰, 논문, 정리, 설명, 란, 뜻, pretrained, bert, 주석, 시맨틱, 테이블, transformer, 트랜스포머, 어텐션]
toc: true
toc_sticky: true
sidebar_main: false

last_modified_at: 2022-11-28
---

<img width="750" alt="image" src="https://user-images.githubusercontent.com/78655692/204124980-aad727de-7dc1-4694-92ef-6f1515688b2f.png">

"Annotating Columns with Pre-trained Language Models" 논문을 개인 공부 및 리뷰를 위해 쓴 글입니다. <br><br>
논문 링크 : [DODUO Paper](https://dl.acm.org/doi/abs/10.1145/3514221.3517906)
{: .notice--info}

<br>
<br>
<br>

## 1. Introduction

- 컬럼 타입과 컬럼 간 관계같은 테이블의 메타 정보는 데이터 관리의 다양성에 중요하다.
- 최근에는 **시맨틱(semantic)** 컬럼 타입과 관계를 식별하는 것에 관심이 높다.
  - **시맨틱(semantic)** : html의 `<header>`같이 XML에 기반한 시맨틱 마크업 언어를 기반을 말한다. [^1]
- 여기서는 인구(population), 도시(city), 탄생날짜(birth_date)같은 것을 시맨틱 컬럼 타입으로 예시를 들고 있는데, 이러한 정보들은 테이블의 시맨틱을 이해하는 데 중요하다.
- 하지만, 이러한 시맨틱 타입과 관계는 테이블에 표시되어 있지 않지만, 수동으로 주석(annotating)을 다는 것은 굉장히 많은 비용이 든다.
- 그래서, 테이블에 따라 메타 정보를 자동으로 할당할 수 있는 모델을 구축하는 것이 필수이다.

<br>

<img width="750" alt="image" src="https://user-images.githubusercontent.com/78655692/204125717-1d78b2ba-f279-446b-92bf-b3b04f21ce21.png">

- 위 그림는 컬럼 타입과 컬럼 관계가 표시되지 않은 두 테이블을 보여준다.
- 위 그림의 첫번째는 차례로 보면 영화, 감독, 프로듀서, 관련 나라이다.
  - 두번째 컬럼에서 `George Miller`는 세번째 컬럼에서도 나타나는데, 이와 같이 컬럼에서 다른 이름을 확인하는 것은 컬럼의 시맨틱을 더 잘 이해할 수 있다.
- 따라서, 컨텍스트(context) 정보를 고려하여 모델은 테이블의 토픽이 영화이고, 두번째와 세번째 컬럼이 정치 또는 운동선수가 아닐 것이라고 이해할 것이다.
- 위 그림의 두번째는 컬럼 타입과 컬럼 관계가 나타난 테이블을 보여준다.
  - `person`과 `location` 컬럼 타입은 `place_of_birth` 관계를 예측하는 데 도움이 된다.

<br>

<img width="699" alt="image" src="https://user-images.githubusercontent.com/78655692/204128407-f4194426-a5c6-4bdf-a795-81551ece5bcf.png">

- 컬럼 타입 예측과 컬럼 관계 예측의 시너지를 결합하기 위해 **DODUO**는 1) 컬럼 관계를 배우고, 2) 테이블 컨텍스트를 통합하고, 3) 컬럼 주석을 처리하며, 4) **두 태스크(task) 간의 지식을 공유한다**.
- 위 그림은 Doduo의 모델 구조를 나타낸 것이다.
  - Doduo는 직렬화(serialization) 후 테이블의 여러 컬럼을 입력 값으로 사용하고 컬럼 타입과 컬럼 관계 예측을 출력한다.
  - 각 컬럼의 시작 부분에 `[CLS]`를 추가하고 해당 임베딩을 컬럼의 학습된 컬럼 표현으로 사용한다.
  - `[CLS]`는 컬럼 타입 예측에 사용되고, 컬럼 관계 예측을 위한 출력은 각 컬럼 쌍의 `[CLS]`를 이용한다.

<br>
<br>

## 2. Background

### 2.1 Problem Formulation

<img width="499" alt="image" src="https://user-images.githubusercontent.com/78655692/204131672-c2be156a-baf9-4522-aeca-cb9621e02954.png">

- 테이블 $T$는 $T=(c_1,...,c_n)$ 컬럼의 속성으로 구성되어 있다.
- $c_i$에는 일련의 데이터 $v\in val(T.c_i)$ 가 저장되어 있다.
- $v$를 문자열이라 가정하고 입력 토큰 $v=[w_1,...,w_k]$로 나눈다.

<br>

- **Problem 1 (컬럼 타입 예측)**
  - 테이블 $T$와 그 안의 컬럼 $c_i$이 주어졌을 때, 타입 어휘 $C_{type}$를 가진 컬럼 타입 예측 모델 $M$은 $c_i$의 시맨틱을 가장 잘 설명해주는 컬럼 타입 $M(T,c_i)\in C_{type}$을 예측한다.
- **Problem 2 (컬럼 관계 예측)**
  - 테이블 $T$와 그 안의 컬럼 쌍 $(c_i,c_j)$이 주어졌을 때, 관계 어휘 $C_{rel}$을 가진 컬럼 관계 예측 모델 $M$은 $c_i$와 $c_j$ 간의 관계의 시맨틱을 가장 잘 설명해주는 관계 $M(T,c_i,c_j)\in C_{rel}$을 예측한다.

<br>

### 2.2 Pre-trained Language Models

- **BERT**같은 대표적인 LM(language model)은 위키피디아와 같은 대규모 텍스트 말뭉치에서 사전 학습되며(pre-trained), 일반적으로 멀티 레이어 트랜스포머(transformer) 블록을 사용하여 정보성 있는 단어에 더 많은 가중치를 주고 원시 텍스트를 처리하기 위한 단어를 중지하는 데 더 적은 가중치를 준다.
- 사전 훈련 동안 모델은 누락 토큰 예측 및 다음 문장 예측과 같은 셀프 지도(self-supervised) 언어 예측에 대해 훈련한다.
- 이것의 목적은 단어 토큰의 시맨틱 상관관계를 학습하여 상관된 토큰을 유사한 벡터 표현에 투영할 수 있도록 하는 것이다.
- 사전 학습된 LM의 임베딩은 두 가지 강점을 제공한다.
  1. 다의성을 식별할 수 있다.
  2. 동의어를 잘 처리한다.

<br>

### 2.3 Multi-task learning

<img width="548" alt="image" src="https://user-images.githubusercontent.com/78655692/204135196-b84de564-6ec9-4397-9346-6ee1c9b50920.png"> <br> 이미지 출처[^2]

<br>

- 멀티 태스크(multi-task) 러닝은 하나의 모델을 사용하여 다양한 태스크를 처리하기 위해 사용된다. [^2]
- 멀티 태스크 러닝은 태스크들이 지식을 공유하므로 동일한 기본 모델을 사용한 학습이 서로에게 이익이 된다.
- 멀티태스킹 학습은 학습 데이터가 충분하지 않을 때 모델의 일반화 성능을 향상시킬 수 있다.
- 하드 매개 변수 공유는 여러 태스크에 대한 모델이 동일한 매개 변수를 공유하기 때문에 비용 효율적일 수 있다.

<br>
<br>

## 3. Model

### 3.1 Single-column Model 

- 언어 모델(LM)은 토큰 시퀀스를 입력으로 받기 때문에 먼저 테이블을 토큰 시퀀스로 변환해준다.
- 즉, 컬럼 $C$이 컬럼 값 $v_1,...,v_m$을 가지고 있다면, 직렬화된 시퀀스는 다음과 같다.
  - $serialize_{single}(C)$ $::== [CLS]\ v_1...v_m\ [SEP]$
- 이 직렬화는 문제를 시퀀스 분류 태스크로 변환하므로, 학습 데이터를 이용하여 BERT 모델을 미세조정(fine-tune)하는 것은 쉽다.

<br>

- 컬럼 관계 예측 태스크는 한 쌍의 컬럼을 토큰 시퀀스로 변환하여 시퀀스 분류 태스크로 표현할 수 있다.
- 이 경우 사전 학습된 LM이 두 컬럼을 구별하는 데 도움이 되도록 두 컬럼 값 사이에 `[SEP]`를 삽입한다.
- 즉 두 컬럼 $C=v_1,...,v_m$과 $C'=v_1',...,v_m'$이 주어진다면, 싱글 컬럼 모델은 쌍을 다음과 같이 직렬화할 수 있다. 
  - $serialize_{single}(C,C')::==$ $[CLS]\ v_1...v_m\ [SEP]\ v_1'...v_m'\ [SEP]$
- 하지만, 이 방법은 타입이 동일한 테이블이 있더라도 독립적으로 컬럼 타입을 예측하기 때문에 싱글 컬럼 모델이므로, 테이블 컨텍스트를 캡처하지 못한다.

<br>

### 3.2 Table Serialization

- 싱글 컬럼 모델과는 달리, Doduo는 멀티 컬럼 (테이블별) 모델로 전체 테이블을 입력으로 받는다.
- Doduo는 데이터 항목들을 다음과 같이 직렬화한다.
  - 각 테이블은 $n$ 컬럼들 $T=(c_i)_{i=1}^n$을 가지고 있으며, 각 컬럼은 $N_m$ 컬럼값 $c_i=(v_j^i)_{j=1}^m$을 가지고 있다.
  - $serialize(T)::==$ $[CLS]\ v_1^1...[CLS]\ v_1^n...v_m^n\ [SEP]$
- 섹션 1에서 소개한 테이블을 예시로 들면, 직렬화는 다음과 같다.
  - [CLS]Happy Feet,...[CLS] George Miller,... [CLS]USA,..., France[SEP].
- 입력에 항상 싱글 [CLS] 토근이 있는 싱글 컬럼 모델과 달리, Doduo의 직렬화 방법은 입력 테이블의 컬럼 수 만큼 많은 [CLS] 토큰을 삽입한다.
  - Doduo는 입력 시퀀스의 [CLS] 토큰 수 만큼의 레이블을 예측한다.
- Doduo의 학습 절차는 데이터셋의 모든 테이블을 직렬화하고 토큰화함으로써 시작된다. (알고리즘 1의 3~4 라인)

<img width="573" alt="image" src="https://user-images.githubusercontent.com/78655692/204139181-30af7965-f578-4556-bf01-5a3df9c2d901.png">

<br>

### 3.3 Contextualized Column Representations

- 다음은 Doduo가 트랜스포머 아키텍처를 사용하여 컨텍스트된 컬럼 임베딩을 통해 테이블 컨텍스트를 얻는 방법을 설명한다.

<img width="538" alt="image" src="https://user-images.githubusercontent.com/78655692/204139444-01beeb9e-07c4-4020-b05f-855a4c3d64dd.png">

- 위 그림은 Doduo의 각 트랜스포머 블록이 동일한 테이블에 있는 모든 컬럼 값의 컨텍스트 정보를 집계하는 방법을 보여준다.
- 첫 번째 트랜스포머 레이어가 두 번째 컬럼의 [CLS] 토큰에 대한 유사성을 기반으로 다른 토큰의 임베딩을 집계하여 어텐션(attention) 벡터를 계산하는 것을 보여준다.
  - 따라서 동일한 컬럼값에 대한 어텐션 벡터는 다른 컨텍스트에서 나타날 때 다를 수 있다.
- 토큰을 토큰 임베딩([CLS]와 같이)으로 인코딩한 후 트랜스포머 레이어는 토큰 임베딩을 키(K), 쿼리(Q), 값(V) 임베딩으로 변환한다.
- 토큰에 대한 컨텍스트된 임베딩은 모든 토큰 임베딩의 임베딩 값의 가중 평균에 의해 계산되며, 여기서 가중치는 쿼리(Q) 임베딩과 키(K) 임베딩 간의 유사성에 의해 계산된다.

<br>

- 트랜스포머 기반 모델은 일반적으로 여러 개의 어텐션 헤드를 가지고 있다.
- 다른 어텐션 헤드는 입력 데이터의 다른 특성을 전체적으로 캡처할 수 있도록 K, Q, V 계산을 위한 다른 파라미터를 갖는다.
- 마지막으로 트랜스포머 블록의 출력은 입력과 동일한 차원 크기로 변환되어 이전 트랜스포머 블록의 출력을 다음 트랜스포머 블록의 입력으로 직접 사용할 수 있다.

<br>

- **Column representations**
  - Doduo는 전체 테이블을 입력으로 받아 컨텍스트에 맞는 컬럼 표현(representation)이 전체적인 방식으로 테이블 컨텍스트를 고려하는 테이블별(table-wise) 모델이다.
    - **표현(representation)** : 데이터를 인코딩하거나 묘사하기 위해 데이터를 바라보는 다른 방법을 말한다. [^3]
  - 컬럼 타입 예측을 위해 Doduo는 $\mid C_{type}\mid$ 크기의 출력 레이어 다음에 추가 밀도 레이러를 부착한다.

<br>

- **Column-pair representations**
  - 컬럼 관계 예측을 위해 Doduo는 컨텍스트에 맞는 컬럼 표현 쌍을 컨텍스트에 맞는 컬럼-쌍 표현으로 연결한다.
  - 추가 밀도 레이어는 두 컬럼 표현 사이의 조합 정보를 캡처해야 한다.
  - 테이블 $T$가 주어지면, LM은 $p$ 토큰의 직렬화된 시퀀스 $serialize(T)={t_1,...,t_p}$를 입력을 받고, 각 요소 $LM(T)_i$이 토큰 $i$의 d차원 컨텍스트-aware 임베딩인 시퀀스 $LM(T)$를 출력한다.
  - $g_{type}$이 $d\ \times \mid C_{type}\mid$ 차원의 컬럼 타입 예측 밀도 레이어라고 한다면, 컬럼 타입 모델은 다음과 같이 j번째 컬럼의 예측된 컬럼 타입으로써 계산된다.
    - $softmax(g_{type}(LM(T)_{i_j})$
  - 유사하게, $2d\ \times \mid C_{rel}\mid$ 차원의 밀도 레이어 $g_{rel}$을 가진 컬럼 관계 예측을 위해 컬럼 관계 모델은 다음과 같이 테이블 $T$의 j번째와 k번째 간의 예측된 관계로써 계산된다.
    - $softmax(g_{rel}(LM(T)_{i_j}\oplus LM(T)_{i_k}))$
      - $\oplus$ : 두 벡터의 결합을 의미한다.
  - 그런 다음, Doduo는 모델 매개 변수를 업데이트하기 위해 예측 및 groundtruth 레이블을 교차 엔트로피 손실 함수에 보낸다. (알고리즘 1의 9~10 라인)

<br>

### 3.4 Learning from Multiple Tasks

- 학습 단계에서 Doduo는 두 가지 다른 학습 데이터와 두 가지 다른 목표를 사용하여 사전 학습된 LM을 미세 조정한다.
- 알고리즘 1에서 볼수 있듯이, Doduo는 매 에폭마다 태스크를 전환하고 다른 목표에 대한 매개 변수를 업데이트한다.





<br>
<br>
<br>
<br>

## References

[^1]: [위키백과 - 시맨틱 웹](https://ko.wikipedia.org/wiki/시맨틱_웹)
[^2]: [멀티 태스크 러닝의 소개 - JINSOL KIM](https://gaussian37.github.io/dl-concept-mtl/)
[^3]: [표현(representation)을 학습하다 의미](https://gggggeun.tistory.com/111)


