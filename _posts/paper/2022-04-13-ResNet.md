---
layout: single
title: "[논문 리뷰] Deep Residual Learning for Image Recognition"
excerpt: "그 유명한 ResNet은 152층까지 네트워크를 쌓아올리는 데 성공하며 ILSVRC 2015 classification competition에서 우승하게 됩니다. 그리고 처음으로 3.57%의 top5 에러율을 달성했습니다."
categories: paper
tag : [resnet, residual, 잔차, shortcut connection, skipping, 매핑, identity, mapping, bottleneck, 논문, 리뷰, 정리, 후기, design, 설명, layer, 딥러닝, cv, 컴퓨티 비전]
toc: true
toc_sticky: true
sidebar_main: true

last_modified_at: 2022-04-28
---

![image](https://user-images.githubusercontent.com/78655692/163178626-0d491aa2-1634-4a28-8876-b58c5322a463.png)

"Deep Residual Learning for Image Recognition" 논문을 개인 공부 및 리뷰를 위해 쓴 글입니다. <br>
논문 링크 : <https://arxiv.org/abs/1512.03385>
{: .notice--info}

<br>
<br>

## 1. Introduction

- 네트워크 깊이는 ImageNet 데이터셋의 결과에 미칠만큼 매우 중요한 요소이다. 
- 깊이가 중요해지면서, 한 가지 질문이 떠오른다.

    > Is learning better networks as easy as stacking more layers?

- 이 질문에 답하기 위해서는 **vashing gradients** 문제를 해결해야 한다.
- 또한 deeper network가 **수렴(convergence)**를 시작할 때, **성능 저하(degradation)** 문제가 발생할 수 있다.
  - 네트워크 깊이가 증가함에 따라 정확도가 saturated 나서 빠르게 저하되는 현상
  - 이러한 성능 저하는 과적합(overfitting)이 원인이 아니고, 더 많은 층을 추가하면 높은 학습 에러(training error)가 발생한다. (Fig. 1)

    <img src='https://user-images.githubusercontent.com/78655692/163181920-a1d5cb78-cefb-465a-b2f4-526ae84bfc44.png' width=450>


<br>

- 이 논문에서는, 성능 저하 문제를 **deep residual learning framework**를 도입하여 해결하려고 한다.
- 각 층이 직접 매핑되기를 바라기 보다는, 이런 층들이 **잔차(residual)** 매핑에 맞게 허용한다.
- $x$ : 기존 input
- $\mathcal{H}(x)$ : 원하는 기본 매핑
- $\mathcal{F}(x) := \mathcal{H}(x) - x$ : 쌓인 비선형 층이 다른 매핑에 적합하도록 한다.
  - 즉, weight layer-> relu -> weight layer
- $\mathcal{F}(x) + x$ : 원본 매핑
  - 즉, weight layer -> relu -> weight layer -> relu

<img src='https://user-images.githubusercontent.com/78655692/163184908-cd6aed01-94d8-458b-bc3a-2a40a9833f01.png' width=540>

<br>

- $\mathcal{F}(x) + x$는 shortcut connection(**=skipping one**)을 가진 피드포워드(feedforward) 네트워크로 실현된다.
  - **피드포워드(feedforward)** : 입력 층(input layer)에 온 데이터가 1개 이상으로 구성된 은닉 층(hidden layer)을 거쳐 마지막에 있는 출력 층(output layer)으로 출력 값을 내보내는 과정 [^5]
- shortcut connection은 단순히 **identity** 매핑을 수행하고, 그 출력은 쌓아올린 층의 output에 추가된다. (Fig. 2)
  - 이 연결선은 추가 매개 변수나 계산 복잡성을 추가하지 않는다.
  - 즉, 일정 층들을 건너뛰어 출력에 더할 수 있게 해준다.
-> 요약 : $\mathcal{F}(x)$ 만 학습 수행!

<br>

- 이 논문은 2가지를 보여주려고 한다.

  1. **deep residual net**은 최적화하기 쉽다
     - 반면, plain net은 깊이가 증가하면 더 높은 학습 오류를 보인다.
  2. **deep residual net**은 크게 증가한 깊이(depth)에서 정확도 향상을 보여준다.

<br>
<br>

## 2. Related Work

### Residual Representations

- **VLAD** : dictionary와 관련하여 residual vector로 인코딩되는 표현
- **Fisher Vector** : VLAD의 확률(probabilistic) 버전

<br>

### Shortcut Connections

- gated shortcut이 닫힐(approaching zero)때, highway network의 층들은 non-residual 함수를 나타낸다.
- 하지만, 여기에 나오는 공식은 항상 residual 함수를 학습한다.
- identity shortcut은 결코 닫히지 않으며, 모든 정보는 항상 통과된다. 

<br>
<br>

## 3. Deep Residual Learning

### 3.1 Residual Learning

- residual 함수 $\mathcal{H}(x) - x$가 여러 비선형 층(non-linearity layers)이 복잡한 함수에 점근적으로 근사(approximate)할 수 있다고 본다.
- 따라서, residual 함수 $\mathcal{F}(x) := \mathcal{H}(x) - x$에 근접하도록 명시적으로 허용한다.
- 따라서, 원래 함수는 $\mathcal{F}(x) + x$가 된다.

> 기존 신경망이 H(x) - x = 0을 만들려 했다면 ResNet은 H(x) - x = F(x) 로 두어 F(x)를 최소화 시키려고 합니다. 즉 F(x) = 0 이라는 목표를 두고 학습을 진행합니다. 이렇게 학습을 진행하면 F(x) = 0이라는 목표값이 주어지기 때문에 학습이 더 쉬워집니다. 결국 H(x) = F(x) + x 가 되는데 이때 입력값인 x를 사용하기 위해 쓰는 것이 Skip Connection입니다. 즉 Skip Connection은 입력 값이 일정 층들을 건너뛰어 출력에 더할 수 있게 하는 역할을 합니다. [^1]

<br>

- **비교 및 정리**

![image](https://user-images.githubusercontent.com/78655692/163760970-434e57eb-e164-42bd-817f-0d2dac9e2a0b.png)

- 기존 CNN 구조는 x -> weight layer -> relu -> weight layer -> relu -> H(x)
  - 그 후 최적의 H(x)를 찾기 위해 weight 파라미터 값을 수정해나간다.
- 하지만 오른쪽 그림처럼, F(x)를 최적화하려고 한다면 **최적의 H(x)(=출력)-x(=입력) 값의 최소화**를 찾도록 학습한다.
  - 즉, 다시 말해 H(x)=F(x)+x 라 할 때, F(x)가 거의 0에 도달하는 방향으로 학습시켜 입력(x)의 작은 움직임을 쉽게 검출할 수 있다. [^4]

<br>

### 3.2 Identity Mapping by Shortcuts

![image](https://user-images.githubusercontent.com/78655692/163199522-52779629-e29f-4ce6-ae4b-e8b112c1000a.png) <br>이미지출처 [^2]

<br>

- $y=\mathcal{F}(x,\{W_i\})+x$ : building block
  - $\mathcal{F}$와 $x$의 차원은 반드시 같을 때
- $y=\mathcal{F}(x,\{W_i\})$ : 학습할 잔차 매핑
- $\sigma$ : relu
- $\mathcal{F}+x$는 shortcut connection과 요소별 덧셈에 의해 수행된다.
- $y=\mathcal{F}(x,\{W_i\})+W_ix$
  - $\mathcal{F}$와 $x$의 차원이 같지 않을 때
  - $W_i$ : linear projection

<br>

### 3.3 Network Architectures

![resnet](https://user-images.githubusercontent.com/78655692/163201846-aadaff8d-a5c7-4728-8a6c-04a1bcadf218.jpg)

<br>

### 3.4 Implementation

- Dataset : ImageNet
- Resize : 224 x 224
- batch normalization (BN) 적용 (conv와 activation 사이)
- 256 미니 배치 크기의 SGD 사용
- learning rate : 0.1
- 모델은 $60 \times 10^4$ 까지 훈련한다.
- weight decay : 0.0001
- momentum : 0.9
- dropout X

<br>
<br>

## 4. Experiments

![image](https://user-images.githubusercontent.com/78655692/163204278-5d7dbb41-355f-42d3-8a93-8bcb28004a51.png)

<br>

### Deeper Bottleneck Architectures

- 할애할 수 있는 훈련 시간을 고려해야 되기 때문에, building block을 **bottleneck** 구조로 수정했다. (Fig. 5)
  - 1x1 conv 층 2개를 사용하기 때문에 연속 2번인 3x3 conv층보다 연산량 줄어듦.
  - bottleneck 구조를 이용하여 더 깊은 층을 쌓을 수 있게 되었음.
  - 코드로 알고 싶으신 분은 [ResNet_scratch_pytorch](https://ingu627.github.io/code/ResNet_scratch_pytorch/) 여기를 참조!!

<img src='https://user-images.githubusercontent.com/78655692/163205277-299e1ee5-49a0-444d-bde4-a8f08fd81153.png' width=550>

<br>
<br>
<br>
<br>

## References

[^1]: [한땀한땀 딥러닝 컴퓨터 비전 백과사전 - ResNet, ResNet의 확장](https://wikidocs.net/137252)
[^2]: [ndb796 - Deep-Learning-Paper-Review-and-Practice](https://github.com/ndb796/Deep-Learning-Paper-Review-and-Practice/blob/master/lecture_notes/ResNet.pdf)
[^3]: [[논문읽기] 02. Deep Residual Learning for Image Recognition : ResNet - 참신러닝 (Fresh-Learning)](https://leechamin.tistory.com/184)
[^4]: [Organize everything I know - ResNet](https://oi.readthedocs.io/en/latest/computer_vision/cnn/resnet.html)
[^5]: [딥러닝 피드 포워드 (feed forward) - NeoWizard](https://m.blog.naver.com/PostView.naver?isHttpsRedirect=true&blogId=beyondlegend&logNo=221373971859)