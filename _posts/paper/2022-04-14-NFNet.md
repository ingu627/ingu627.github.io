---
layout: single
title: "[논문 리뷰] High-Performance Large-Scale Image Recognition Without Normalization"
excerpt: "NFNet은 imagenet 데이터셋에서 배치 정규화없이 ResNet을 학습시켰습니다. 그 결과, imagenet에서 SOTA를 갱신한 모델이 되었습니다."
categories: paper
tag : [resnet, residual, batch normalization, 배치 정규화, large scale, image recognition, Adaptive Gradient Clipping, Normalizer-Free Network, AGC, 논문, 리뷰, 정리, 후기, design, 설명, layer, 딥러닝, cv, 컴퓨티 비전]
toc: true
toc_sticky: true
sidebar_main: true

last_modified_at: 2022-05-28
---

![image](https://user-images.githubusercontent.com/78655692/163315843-2cb7fce6-02ad-46f6-8492-64f7dda3a21e.png)

"High-Performance Large-Scale Image Recognition Without Normalization" 논문을 개인 공부 및 리뷰를 위해 쓴 글입니다. <br> 이 논문은 선행 연구들이 많이 포함되어 있어, 어색할 수 있습니다. 이 부분은 따로 정리하겠습니다. <br><br> **논문 순서** <br> 1. Batch Normalization Biases Residual Blocks towards the Identity Function in Deep Networks, De and Smith, NeurIPS 2020 <br> 2. Characterizing Signal Propagation to Close the Performance Gap in Unnormalized ResNets, Brock et al., ICLR 2021 <br> **3. High Performance Large-Scale Image Recognition without Normalization, Brock et al., ICMR 2021** <br><br> 논문 링크 : <https://arxiv.org/abs/2102.06171>
{: .notice--info}

<br>
<br>

## 1. Introduction

- 컴퓨터 비전의 최근 모델은 대부분 **배치 정규화(batch normalization)로 학습된 deep residual network**의 변형들이다.
- **Internal Covariate Shift** : 훈련 과정에서 심층망의 내부 노드 분포(distribution)의 변화
  - 좀 더 풀어쓰면, 아무런 규제없이 가중치 값들이 제멋대로 학습하다보면 값의 범위가 넓어지게 될 수 있다. 매개변수 값들의 변동이 심해지는 현상을 말한다. [^5]
  - **Covariate shift** : 학습 데이터셋과 테스트 데이터셋의 데이터 분포가 다른 현상을 의미
    - 레이어 간의 분포가 달라진다.
- **배치 정규화**은 loss landscape를 매끄럽게 하여 큰 학습률(learning rate)과 큰 배치 크기(batch size)로 안정적인 학습을 가능하게 해준다.

<img src='https://user-images.githubusercontent.com/78655692/163351299-56a9bc50-f741-4ead-a962-a0fcd7a9bf4a.png' width=500>
<br> batch normalization 이미지출처: [^3]

- 위의 과정들을 살펴보면,
  - **mini-batch mean**은 미니배치 별 평균을 구하는 부분이다.
  - **mini-batch variance**은 미니배치 별 분산을 구하는 부분이다.
  - **normalize**은 위에서 구한 평균과 분산을 이용해서 정규화를 하는 부분이다. 즉, 여기까지는 입력데이터에 대한 범위를 규제하는 것이다.
  - **scale and shift**은 정규화된 가중치값을 scale($\gamma$)과 shift($\beta$)을 이용해 가중치 $W$의 범위를 규제하여 과적합(overfitting)을 막는 것이다. [^5]

<br>
<br>

## 2. Pros (BN)

- 정규화없이 네트워크를 학습하기 위해, 배치 정규화가 학습 중에 가져온 이점을 이해하고 이러한 이점을 보완할 대안 전략을 알아야 한다. 다음은 4가지의 주요 **이점(Pros)**이다.

### 2.1 Batch normalization downscales the residual branch

- 스킵 연결선(skip connections)과 배치 정규화(batch normalization)의 조합은 수 천개의 레이어를 학습할 수 있게 해주었다.
- 어떻게 학습할 수 있었냐면, 배치 정규화가 (일반적으로) residual branch에 배치될 때, 초기화 시 residual branch에 대한 hidden activation의 규모(scale)를 감소시키기 때문이다.
- 이는 신호를 skip path로 편향시켜, 네트워크가 학습 초기에 잘 작동하는 미분을 가지도록 하여 효율적인 최적화를 가능하게 해준다.

<br>

-> **보완** : **SkipInit** 또는 **NF 전략** 사용하기

<br>

### 2.2 Batch normalization eliminates mean-shift

- ReLU와 GELU와 같은 anti-symmetric이지 않은 activation function은 0이 아닌 평균 활성화가 있다.
- 비선형 직후의 독립적인 training 샘플의 활성화 사이의 내적(inner product)은 입력 특징 사이의 내적이 0에 가깝더라고 일반적으로 크고 positive하다.
- 이 문제는 네트워크 깊이가 증가함에 따라 복잡해진다.
- 그리고 네트워크의 깊이에 비례하는 단일 채널(channel)에서 서로 다른 training 샘플의 활성화에 mean-shift를 도입하는 것은 deep network가 초기화 시 모든 training 샘플에 대해 동일한 레이블을 예측하게 하는 문제가 있다.
- 배치 정규화를 사용하면 현재 배치에서 각 채널의 평균 활성화(mean activation)가 0이 되어 평균 이동(mean-shift)이 제거된다.

<br>

-> **보완** : **Scaled Weight Standardization** 사용하기

<br>

### 2.3 Batch normalization has a regularizing effect

- 배치 정규화는 테스트 셋 정확도를 향상시키는 regularizer 역할을 한다.
  - 학습 데이터의 하위 집합(subset)에서 계산되는 batch statistics의 noise때문에
- 배치 정규화 네트워크(batch-normalized network)의 테스트 정확도는 배치 크기를 조정(tune)하거나, 분산 학습(distributed training)에서 ghost batch normalization를 사용하여 개선 가능하다.

<br>

-> **보완** : **Explicit regularization** 사용하기

<br>

### 2.4 Batch normalization allows efficient large-batch training

- 배치 정규화는 loss landscape를 평활화(smoothen)하며, 이는 가장 큰 안정적인 학습속도(learning rate)를 증가시킨다.
  - 이 특징은 배치 크기가 작을 때는 이점이 없다.
- 큰 배치 크기로 효율적으로 훈련하려면 더 큰 학습 속도가 필요하다.
  - 여러 장치에 걸쳐 병렬화되었을 때 훈련 속도를 크게 향상시킬 수 있다.

<br>

-> **보완** : **Adaptive Gradient Clipping** 사용하기

<br>
<br>

## 3. Cons (BN)

- 하지만 **배치 정규화(batch normalization)**는 3가지 단점이 존재한다.
  1. 메모리 오버헤드(overhead) 발생 -> 일부 네트워크에서 미분(gradient)을 평가하는 데 필요한 시간 증가 -> 높은 계산량 발생
  2. training과 (추론 시간 )inference time의 모델의 행동사이의 불일치 발생 -> 튜닝이 필요한 하이퍼 파라미터(hyper parameter) 조정 필요
  3. 미니 배치(mini batch)에서 training 사이의 독립성(independence)를 깨뜨린다.
     - 배치 정규화는 다른 장치들에서 정확하게 복제하지 못한다.
     - 즉, 배치 정규화는 분산 학습에서 미묘한 오류를 발생시킨다.

- 배치 정규화는 일부 contrastive learning 알고리즘에서 정보 유출을 방지하는 데 노력이 필요하다.
- 배치 정규화가 있는 네트워크는 batch statistics가 학습 중에 큰 분산(variance)를 갖는 경우에도 성능 저하를 일으킬 수 있다.
- 배치 정규화의 성능은 배치 크기(batch size)에도 민감하며, 너무 작을 경우에도 문제가 된다. 

<br>
<br>

### Other works without using BN

- 배치 정규화가 최근에 많이 사용되었지만, 이것을 사용하지 않은 채 deep residual network를 학습시키는 연구들이 나왔다.

1. 0으로 초기화된 각 residual branch의 끝에 학습 가능한 스칼라를 도입하는 방법.
2. 활성화 함수 ReLU에 mean shift를 도입하여, 네트워크의 깊이가 증가함에 따라 서로 다른 training sample의 hidden activation이 점점 correlate하게 하는 방법.

- 하지만 대부분 성능이 배치정규화에 비해 안 좋거나, 학습이 안정적이지 않는 등의 문제가 발생했다.

<br>

### Contribution of this paper (NFNet)

1. **Modified residual branches and convolutions with Scaled Weight Standardization**
   - 초기화 시 residual branch를 억제하고 **scaled weight standardization**를 적용하는 "Normalizer-Free" ResNet를 도입하는 방법. 
2. **Adaptive Gradient Clipping (AGC)** 제안
   - 미분 노름(norm)과 매개변수 노름(norm)의 단위별 비율 미분(gradient)을 clip
   - AGC가 더 큰 배치 크기와 더 강력한 데이터 증대(data augmentation)로 normalizer-free 네트워크를 학습할 수 있음을 보여준다.
3. **NFNet (Normalizer-Free ResNets)** 설계
   - NFNet-F1 모델은 EfficientNet-B7과 유사한 정확도를 달성하면서 약 8.7배 더 빠르게 학습 가능
   - NFNet이 배치 정규화된 네트워크보다 훨씬 높은 검증 정확도를 달성하는 것을 보여준다.
   - 3억 개의 레이블된 이미지의 대규모 개인 데이터 셋을 사전 학습(pre-trainined) 후 ImageNet에서 미세 조정(fine-tuning)
   - 미세 조정 후 89.2% top-1 달성

<img src='https://user-images.githubusercontent.com/78655692/163329752-eb699e27-4cc3-42b0-97d4-7b1d6d287a3f.png' width=450>

<br>
<br>

## 4. Towards Removing Batch Normalization

- $h_{i+1}=h_i+\alpha f_i(h_i/\beta_{i})$ : residual block
  - $h_i$ : $i^{th}$ residual block의 입력
  - $f_i$ : $i^{th}$ residual block 에서 계산된 함수
  - $Var(f_i(z))=Var(z)$
  - $\alpha$ : 활성화 변동의 비율
  - $\beta_{i}$ : $i^{th}$ residual block에 대한 입력의 표준 편차(standard deviation)를 예측하여 결정된다.
  - $\beta_i=\sqrt{Var(h_i)}$, where $Var(h_{i+1})=Var(h_i)+\alpha^2$

<br>

- 또는 **Scaled Weight Standardization**을 도입하여 hidden activation의 mean-shift 현상 방지하는 방법이 있다.

- $\hat W_{ij}=\frac{W_{ij}-u_i}{\sqrt{N}\sigma_i}$
  - $u_i=\frac{1}{N}\sum_iW_{ij}$
  - $\sigma_i^2=\frac{1}{N}\sum_j(W_{ij}-u_i)^2$
  - **N** : fan-in
  - **ReLU** : $\gamma=\sqrt{2/(1-\frac{1}{\pi})}$ (non-linearity scalar gain)
- 또는 **Dropout**
- 또는 **Stochastic Depth**

<br>

- 이런 방법들은 배치 크기가 1024이거나 아주 작은 크기일 때도 잘 작동했지만, 4096 이상일 때는 성능이 나빠졌다.

<br>
<br>

## 5. Adaptive Gradient Clipping for Efficient Large-Batch Training

- NF-ResNet이 큰 배치 크기를 갖기 위해 **gradient clipping** 도입
- **Gradient Clipping** : 대규모 배치 크기와 강력한 데이터 증강을 효율적으로 훈련할 수 있는 그레이디언트 클리핑 방법
  - 경사 하강(gradient descent)이 가파른 절벽에서 합리적으로 수행될 수 있도록 도와줌 [^2]
  - 미분이 일정 threshold를 넘어가면 clipping 해준다.

<img src='https://user-images.githubusercontent.com/78655692/163338516-b16309c6-7503-4105-8cbf-be584e435add.png' width=650> <br> 이미지출처: [^1]

<br>

- **Gradient clipping**

![image](https://user-images.githubusercontent.com/78655692/165501285-b9f69322-ba35-4efa-a952-2e78dd49f37d.png)

  - $G=\partial L/\partial \theta$
  - $L$ : loss
  - $\theta$ : vector
  - $\lambda$ : threshold

<br>


- **Adaptive Gradient Clipping**

![image](https://user-images.githubusercontent.com/78655692/163349245-cd5e92f0-f369-46ad-84d8-14aefe3fd731.png)

  - $G^l$ : $W^l$에 해당하는 미분값
  - $\Vert W^l\Vert_F=$ $\sqrt{\sum_i^N\sum_j^M(W_{i,j}^l)^2}$
  - $\Vert W_i\Vert_F^*=max(\Vert W_i\Vert_F, \epsilon)$
  - $\epsilon$ : $10^{-3}$

<br>

<img src='https://user-images.githubusercontent.com/78655692/163344013-5873da67-7313-4342-be07-8d92a3e02eae.png' width=500>

- AGC를 사용하면 4096 배치 크기까지 학습할 수 있다.

<br>
<br>

### 5.1 Ablations for Adaptive Gradient Clipping (AGC)

<img src='https://user-images.githubusercontent.com/78655692/163344583-ef3aa72d-b61f-42c5-ae5c-aa90b2b7c3d2.png' width=600>

- 위 그림은 NFNet bottlenectk block의 요약


<br>
<br>

## 6. Experiments


![image](https://user-images.githubusercontent.com/78655692/163350351-d10b82a6-bc87-488b-b6d2-e9f923abbbc1.png)

- Nfnet-F5 모델은 MaxUp으로 이전 SOTA EfficientNet-B8보다 약간 개선된 86%의 상위 1개 검증 정확도를 달성했다.
- 그리고 Nfnet-F1 모델은 EfficientNet-B7의 84.7%와 RA를 매칭하는 동시에 교육 속도가 8.7배 빠르다.


<br>

### Conclusion [^4]

- Downscales the residual branch -> SkipInit or NF Strategy
- Prevent mean-shift -> Scaled Weight Standardization
- Implicit regularization -> Explicit regularization
- Enable large-batch training -> Adaptive Gradient Clipping


<br>
<br>

## Appendix

- NFNet 모델은 수정된 SE-ResNeXt-D 버전이다.
- 입력 데이터 : $H\times W\times C(RGB)$
  - 전체 ImageNet 학습 데이터셋에서 채널당 평균/표준 편차로 정규화됨



<br>
<br>
<br>
<br>

## References

[^1]: [Understanding Gradient Clipping (and How It Can Fix Exploding Gradients Problem)](https://neptune.ai/blog/understanding-gradient-clipping-and-how-it-can-fix-exploding-gradients-problem)
[^2]: [[SUB] HighPerformance Large Scale ImageRecognition Without Normalization](https://www.youtube.com/watch?v=HP4evlugOIo)
[^3]: [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167)
[^4]: [Normalizer Free Networks (High-Performance Large-Scale Image Recognition Without Normalization)](https://velog.io/@isseebx/High-Performance-Large-Scale-Image-Recognition-Without-Normalization)
[^5]: [10. Batch Normalization (배치 정규화) - Time Traveler](https://89douner.tistory.com/44)