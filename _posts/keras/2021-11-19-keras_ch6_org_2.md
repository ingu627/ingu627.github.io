---
layout: single
title: "[케라스(keras) 이해] 6장. 텍스트와 시퀀스를 위한 딥러닝 (2)"
excerpt: "케라스 창시자에게 배우는 딥러닝 - 순환 신경망"
categories: keras
tag : [python, keras, DL, NLP]
toc: true
sidebar_main: true
classes: wide

last_modified_at: 2021-11-19
---

본 글은 [케라스 창시자에게 배우는 딥러닝] - (박해선 옮김) 책을 개인공부하기 위해 요약, 정리한 내용입니다. <br>전체 코드는 [https://github.com/ingu627/deep-learning-with-python-notebooks](https://github.com/ingu627/deep-learning-with-python-notebooks)에 기재했습니다.(원본 코드 fork) <br>저와 같이 공부하면 좋을 것 같습니다.
{: .notice--info}


## 6_2. 순환 신경망 이해하기

## RNN

- **순환 신경망**(Recurrent Neural Network, RNN)은 입력과 출력을 시퀀스 단위로 처리하는 모델이다.
- 시퀀스의 원소를 순회하면서 지금까지 처리한 정보를 **상태**(state)에 저장한다.
- RNN의 상태는 2개의 다른 시퀀스를 처리하는 사이에 재설정된다.
- 하나의 시퀀스가 여전히 하나의 데이터 포인트로 간주된다.
- 또 다른 정의로는 히든 노드가 방향을 가진 엣지로 연결돼 순환구조를 이루는(directed cycle) 인공신경망의 한 종류이다.
- 시퀀스 길이에 관계없이 인풋과 아웃풋을 받아들일 수 있는 네트워크 구조이기 때문에 필요에 따라 다양하고 유연하게 구조를 만들 수 있다는 점이 RNN의 가장 큰 장점이다.

> 용어는 비슷하지만 순환 신경망과 재귀 신경망(Recursive Neural Network)은 전혀 다른 개념이다.

![image](https://user-images.githubusercontent.com/78655692/142578699-9154e73c-9c09-4f1d-902a-6f69c4045699.png)

### 예시

![image](https://user-images.githubusercontent.com/78655692/142586146-148b0c31-5ea1-44b4-9af8-1243ad12f9f7.png)


### 의사코드로 표현한 RNN

<script src="https://gist.github.com/ingu627/b9678d05c8e5e41b3ecddfde2cbf9afe.js"></script>

> 해석
- 이 RNN은 크기가 (timesteps, input_features)인 2D 텐서로 인코딩된 벡터의 시퀀스를 입력받는다. 
- 이 시퀀스는 타임스텝을 따라서 반복한다. 
- 각 타임스텝 t에서 현재 상태와 (input_features,) 크기의 입력을 연결하여 출력을 계산한다.
- 그다음 이 출력을 다음 스텝의 상태로 설정한다. 
- f 함수는 입력과 상태를 출력으로 변환한다.

### 넘파이로 구현한 간단한 RNN 

<script src="https://gist.github.com/ingu627/4baf15b829c78253f4bc190f07e00271.js"></script>

## 6_2_1. 케라스의 순환 층

- 넘파이로 간단하게 구현한 과정이 실제 케라스의 `SimpleRNN` 층에 해당한다.
- `SimpleRNN`은 (batch_size, timesteps, input_features) 크기의 입력을 받는다.
- 케라스에 있는 모든 순환 층과 마찬가지로 `SimpleRNN`은 두 가지 모드로 실행할 수 있다.
  - *1.* 각 타임스텝의 출력을 모은 전체 시퀀스를 반환(크기가 (batch_size, timesteps, output_features)인 3D 텐서)
    - `return_sequences=True`
  - *2.* 입력 시쿤스에 대한 마지막 출력만 반환 (크기가 (batch_size, output_features))
- 이 모드는 객체를 생성할 때 `return_sequences` 매개변수로 선택할 수 있다.

### IMDB 데이터 전처리하기 

<script src="https://gist.github.com/ingu627/f33b7ee720beccb43c412846b409310b.js"></script>

### Embedding 층과 SimpleRNN 층을 사용한 모델 훈련하기

<script src="https://gist.github.com/ingu627/14cb9a366908c9c1e87770eb5178d58a.js"></script>

### 결과 그래프 그리기 

<script src="https://gist.github.com/ingu627/05793e4347c8047fb842c1340d944ed0.js"></script>


## 6_2_2. LSTM과 GRU 층 이해하기 

- `SimpleRNN`은 이론적으로 시간 t에서 이전의 모든 타임스텝의 정보를 유지할 수 있었다.
- 하지만 실제로는 긴 시간에 걸친 의존성은 학습할 수 없는 것이 문제이다.
  - 층이 많은 일반 네트워크에서 나타는 것과 비슷한 현상인 **그래디언트 소실 문제** 때문이다.
    - **그래디언트 소실 문제** : 관련 정보와 그 정보를 사용하는 지점 사이 거리가 멀 경우 역전파시 그래디언트가 점차 줄어 학습능력이 크게 저하되는 것
- `LSTM`은 RNN의 히든 state에 cell-state를 추가한 구조이다. (정보를 여러 타임스텝에 걸쳐 나르는 방법이 추가된다.)
- 나중을 위해 정보를 컨베이어 벨터 의에 정보를 저장함으로써 처리 과정에서 오래된 시그널이 점차 소실되는 것을 막아 준다.
- **LSTM 셀의 역할은 과거 정보를 나중에 다시 주입하여 그래디언트 소실 문제를 해결하는 것이다.**

<img src="https://user-images.githubusercontent.com/78655692/142591414-28cdc1b6-3e07-40d1-957b-3059eb4275f6.png" width="500" height="400" alt="LSTM 구조">

### 케라스에서 LSTM 층 사용하기

<script src="https://gist.github.com/ingu627/62d7a0cccf4059240915b60d87c8539e.js"></script>

![image](https://user-images.githubusercontent.com/78655692/142592468-f4d87535-cbaf-4eaf-b1b6-260b27c48518.png)


## References

- [케라스 창시자에게 배우는 딥러닝](https://www.aladin.co.kr/shop/wproduct.aspx?ItemId=173992478) 
- [RNN과 LSTM을 이해해보자!](https://ratsgo.github.io/natural%20language%20processing/2017/03/09/rnnlstm/)
- [딥 러닝을 이용한 자연어 처리 입문 - 순환 신경망](https://wikidocs.net/22886)