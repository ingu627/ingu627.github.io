---
layout: single
title: "[케라스(keras) 이해] 7장. 딥러닝을 위한 고급 도구 (2)"
excerpt: "케라스 창시자에게 배우는 딥러닝 - 케라스 콜백과 텐서보드"
categories: keras
tag : [python, keras, DL, API, 딥러닝, 케라스, 콜백, 텐서보드]
toc: true
sidebar_main: true
classes: wide

last_modified_at: 2021-12-29
---

<img align='right' width='250' height='250' src='https://user-images.githubusercontent.com/78655692/147629300-4d7acc5e-225a-454a-92cd-4da82f6828f6.png
'>
본 글은 [케라스 창시자에게 배우는 딥러닝] - (박해선 옮김) 책을 개인공부하기 위해 요약, 정리한 내용입니다. <br>전체 코드는 [https://github.com/ingu627/deep-learning-with-python-notebooks](https://github.com/ingu627/deep-learning-with-python-notebooks)에 기재했습니다.(원본 코드 fork) <br> 뿐만 아니라 책에서 설명이 부족하거나 이해가 안 되는 것들은 외부 자료들을 토대로 메꿔 놓았습니다. 즉, 딥러닝은 이걸로 끝을 내보는 겁니다. <br> 오타나 오류는 알려주시길 바라며, 도움이 되길 바랍니다.
{: .notice--info}

<br>
<br>
<br>
<br>

## 7_2. 케라스 콜백과 텐서보드를 사용한 딥러닝 모델 검사와 모니터링 

## 7_2_1. 콜백을 사용하여 모델의 훈련 과정 제어하기 

- 검증 손실이 더 이상 향상되지 않을 때 훈련을 멈추는 것이다. 케라스의 **콜백**을 사용하여 구현할 수 있다.
- **콜백**은 모델의 fit() 메서드가 호출될 때 전달되는 객체(특정 메서드를 구현한 클래스 객체)이다.
- 훈련되는 동안 모델은 여러 지점에서 콜백을 호출한다.
- **콜백**은 모델의 상태와 성능에 대한 모든 정보에 접근하고 훈련 중지, 모델 저장, 가중치 적재 또는 모델 상태 변경 등을 처리할 수 있다.

콜백을 사용하는 몇 가지 사례

1. **모델 체크포인트 저장** : 훈련하는 동안 어떤 지점에서 모델의 현재 가중치를 저장한다.
2. **조기 종료**(early stopping) : 검증 손실이 더 이상 향상되지 않을 때 훈련을 중지한다.
3. **훈련하는 동안 하이퍼파라미터 값을 동적으로 조정한다.** : 옵티마이저의 학습률 같은 경우이다.
4. **훈련과 검증 지표를 로그에 기록하거나 모델이 학습한 표현이 업데이트될 때마다 시각화한다** : 진행 표시줄이 하나의 콜백이다.

### ModelCheckpoint와 EarlyStopping 콜백 

- `EarlyStopping`콜백을 사용하면 정해진 에포크 동안 모니터링 지표가 향상되지 않을 때 훈련을 중지할 수 있다.
  - 일반적으로 이 콜백은 훈련하는 동안 모델을 계속 저장해주는 `ModelCheckpoint`와 함께 사용한다. (선택적으로 지금까지 가장 좋은 모델만 저장할 수 있다. 에포크 마지막에 다다랐을 때 최고 성능을 달성한 모델이다.)

<script src="https://gist.github.com/ingu627/7989e0b8314b61be701dd27c4440e3af.js"></script>

### ReduceLROnPlateau 콜백

- `ReduceLROnPlateau`을 사용하면 검증 손실이 향상되지 않을 때 학습률을 작게 할 수 있다.
  - 손실 곡선이 평탄할 때 학습률을 작게 하거나 크게 하면 훈련 도중 지역 최솟값에서 효과적으로 빠져나올 수 있다.

<script src="https://gist.github.com/ingu627/64c703e7fb295c622c162e5010c8ee9e.js"></script>

### 자신만의 콜백 만들기

- 내장 콜백에서 제공하지 않는 특수한 행동이 훈련 도중 필요하면 자신만의 콜백을 만들 수 있다.
- 콜백은 `keras.callbacks.Callback` 클래스를 상속받아 구현한다. 그 다음 훈련하는 동안 호출될 여러 지점을 나타내기 위해 약속된 다음 메서드를 구현한다.

|파라미터	|설명|
|---|---|
|on_epoch_begin|	각 에포크가 시작할 때 호출한다.|
|on_epoch_end	|각 에포크가 끝날 때 호출한다.|
|on_batch_begin	|각 배치 처리가 시작되기 전에 호출한다.|
|on_batch_end	|각 배치 처리가 끝난 후에 호출한다.|
|on_train_begin	|훈련이 시작될 때 호출한다.|
|on_train_end	|훈련이 끝날 때 호출한다.|

- 이 메서드들은 모두 logs 매개변수와 함께 호출된다.
- 이 매개변수에는 이전 배치, 에포크에 대한 훈련과 검증 측정값이 담겨 있는 딕셔너리가 전달된다.
- 또 콜백은 다음 속성을 참조할 수 있다. 

|파라미터	|설명|
|---|---|
|self.model|콜백을 호출하는 모델 객체|
|self.validation_data:fit()|메서드에 전달된 검증 데이터|

<script src="https://gist.github.com/ingu627/7a4779c9bfd1116544380758c85435f6.js"></script>

## 7_2_2. 텐서보드 소개: 텐서플로의 시각화 프레임워크 

- 좋은 연구를 하거나 좋은 모델을 개발하려면 실험하는 모델 내부에서 어떤 일이 일어나는지 자주 그리고 많은 피드백을 받아야 한다.
- **텐서보드**의 핵심 목적은 훈련 모델의 내부에서 일어나는 모든 것을 시각적으로 모니터링할 수 있도록 돕는 것이다.
  - 모델의 최종 손실 외에 더 많은 정보를 모니터링하면 모델 작동에 대한 명확한 그림을 그릴 수 있다.

1. 훈련하는 동안 측정 지표를 시각적으로 모니터링
2. 모델 구조를 시각화
3. 활성화 출력과 그래디언트의 히스토그램
4. 3D로 임베딩 표현

### 텐서보드를 사용한 텍스트 분류 모델 

<script src="https://gist.github.com/ingu627/977e5ca09730e0992cbc046accd1504b.js"></script>

### 텐서보드 로그 파일을 위한 디렉터리 생성하기

```bash
$ mkdir my_log_dir
```

### 텐서보드 콜백과 함께 모델 훈련하기 

<script src="https://gist.github.com/ingu627/b434f59e17f5d044d7bfc942651ba5c3.js"></script>

- 이제 명령행에서 콜백이 사용하는 로그 디렉터리를 지정하여 텐서보드 서버를 실행한다. 

```bash
$ tensorboard --logdir=my_log_dir
```

그다음 브라우저에서 [http://localhost:6006](http://localhost:6006) 주소에 접속하면 훈련 결과를 확인할 수 있다.

> 11.24 위 주소로 들어가지지만 해당 파일을 찾지 못하는 에러 발생 

## 7_3. 모델의 성능을 최대로 끌어올리기

## 7_3_1. 고급 구조 패턴 

### 배치 정규화 

- **정규화**는 머신 러닝 모델에 주입되는 샘플들을 균일하게 만드는 광범위한 방법이다.
- 이 방법은 모델이 학습하고 새로운 데이터에 잘 일반화되도록 돕는다.
- `normalized_data = (data - np.mean(data, axis=...)) / np.std(data, axis=...)`
- **배치 정규화**(batch normalization)는 훈련하는 동안 평균과 분산이 바귀더라도 이에 적응하여 데이터를 정규화한다. (케라스에서 `BatchNormalization`) 훈련 과정에 사용된 배치 데이터의 평균과 분산에 대한 지수 이동 평균을 내부에 유지한다.
- **배치 정규화**의 주요 효과는 잔차 연 결과 매우 흡사하게 그래디언트의 전파를 도와주는 것이다. 결국 더 깊은 네트워크를 구성할 수 있다.
- `BatchNormalizaion()` 층은 일반적으로 합성곱이나 완전 연결 층 다음에 사용한다.

### 깊이별 분리 합성곱 

- **깊이별 분리 합성곱**(depthwise separable convolution) 층은 입력 채널별로 따로따로 공간 방향의 합성곱을 수행한다. 그다음 점별 합성곱(1x1 합성곱)을 통해 출력 채널을 합친다.
- 이는 공간 특성의 학습과 채널 방향 특성의 학습을 분리하는 효과를 낸다.
- 입력에서 공간상 위치는 상관관계가 크지만 채널별로는 매우 독립적이라고 가정한다.
- 합성곱을 통해 더 효율적으로 표현을 학습하기 때문에 적은 데이터로도 더 좋은 표현을 학습하고, 결국 성능이 더 높은 모델을 만든다. 

![image](https://user-images.githubusercontent.com/78655692/143191947-bf0a02ba-45b5-443d-96d7-ce00b9315d86.png)

<script src="https://gist.github.com/ingu627/4c740f8b0300a6c663f2f1990d4905f9.js"></script>

## 7_3_2. 하이퍼파라미터 최적화

- 딥러닝 모델은 무작위 선택의 연속이다. 층의 수, 유닛과 필터 수, 활성화 함수의 종류 등 설계자의 이유있는 랜덤한 선택의 연속이다. 
- 이런 구조에 관련된 파라미터를 역전파로 훈련되는 모델 파라미터와 구분하여 **하이퍼파라미터**(hyperparameter) 라고 한다.

전형적인 하이퍼파라미터 최적화 과정

1. 일련의 하이퍼파라미터를 자동으로 선택합니다.
2. 선택된 하이퍼파라미터로 모델을 만듭니다.
3. 훈련 데이터에 학습하고 검증 데이터에서 최종 성능을 측정합니다.
4. 다음으로 시도할 하이퍼파라미터를 자동으로 선택합니다.
5. 이 과정을 반복합니다.
6. 마지막으로 테스트 데이터에서 성능을 측정합니다.

- 하이퍼파라미터 최적화는 어느 작업에서 최고의 모델을 얻거나 머신 러닝 경연 대회에서 우승하기 위한 강력한 도구이다.

## 모델 앙상블

- **모델 앙상블**(model ensemble)은 여러 개 다른 모델의 예측을 합쳐 더 좋은 예측을 만든다.
- 가장 기본적인 앙상블은 추론에 나온 예측을 평균내는 방법이다. 
- 가중치를 주고 평균을 내는 방법도 있다. 
- 좋은 앙상블 가중치는 랜덤 서치나 넬더-미드 방법 같은 간단한 최적화 방법이 있다. 이외에도 지수 값을 평균할 수 있다.



## References 

- [케라스 창시자에게 배우는 딥러닝](https://www.aladin.co.kr/shop/wproduct.aspx?ItemId=173992478) 
- [[Keras Study] 7장. 딥러닝을 위한 고급도구](https://subinium.github.io/Keras-7/)