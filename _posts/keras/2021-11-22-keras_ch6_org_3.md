---
layout: single
title: "[케라스(keras) 이해] 6장. 텍스트와 시퀀스를 위한 딥러닝 (3)"
excerpt: "케라스 창시자에게 배우는 딥러닝 - 순환 신경망의 고급 사용법"
categories: keras
tag : [python, keras, DL, ML, AI, 인고지능, 딥러닝, 케라스, NLP, dropout, 드롭아웃, 스태킹, 양방향]
toc: true
sidebar_main: true
classes: wide

last_modified_at: 2021-12-29
---

<img align='right' width='250' height='250' src='https://user-images.githubusercontent.com/78655692/147629300-4d7acc5e-225a-454a-92cd-4da82f6828f6.png
'>
본 글은 [케라스 창시자에게 배우는 딥러닝] - (박해선 옮김) 책을 개인공부하기 위해 요약, 정리한 내용입니다. <br>전체 코드는 [https://github.com/ingu627/deep-learning-with-python-notebooks](https://github.com/ingu627/deep-learning-with-python-notebooks)에 기재했습니다.(원본 코드 fork) <br> 뿐만 아니라 책에서 설명이 부족하거나 이해가 안 되는 것들은 외부 자료들을 토대로 메꿔 놓았습니다. 즉, 딥러닝은 이걸로 끝을 내보는 겁니다. <br> 오타나 오류는 알려주시길 바라며, 도움이 되길 바랍니다.
{: .notice--info}

<br>
<br>
<br>
<br>

## 6_3. 순환 신경망의 고급 사용법

- **순환 드롭아웃**(recurrent dropout): 순환 층에서 과대적합을 방지하기 위해 케라스에 내장되어 있는 드롭아웃을 사용한다. 
- **스태킹 순환 층**(stacking recurrent layer): 네트워크의 표현 능력을 증가시킨다. (그 대신 계산 비용이 많이 든다.)
- **양방향 순환 층**(bidirectional recurrent layer): 순환 네트워크에 같은 정보를 다른 방향으로 주입하여 정확도를 높이고 기억을 좀 더 오래 유지시킨다.

## 6_3_1. 기온 예측 문제 

### 데이터 다운 받기

<script src="https://gist.github.com/ingu627/84201460377a7bc0a8b9838f018fe47e.js"></script>

- unzip 안될 시 [https://ingu627.github.io/error/unzip/](https://ingu627.github.io/error/unzip/) 참고 
- 그래도 안될 시 [https://s3.amazonaws.com/keras-datasets/jena_climate_2009_2016.csv.zip](https://s3.amazonaws.com/keras-datasets/jena_climate_2009_2016.csv.zip) 수동 다운
- **+추가** `datasets` 폴더에 있다.. 다운받을 필요 없다.

### 예나의 날씨 데이터셋 조사하기 

<script src="https://gist.github.com/ingu627/110d937200b33f155516a7b91466110f.js"></script>

- 출력된 줄 수는 42만 551이다.

### 데이터 파싱하기 

<script src="https://gist.github.com/ingu627/b912f1938485bd0866e62c5e68ce21e7.js"></script>

![image](https://user-images.githubusercontent.com/78655692/142636345-875e45b5-c660-4cf9-9067-3a9527dd957b.png)

## 6_3_2. 데이터 준비

- **일자별 수준의 시계열 데이터를 예측 해본다.**
- lookback 타임스텝만큼 이전으로 돌아가서 매 steps 타입스탭마다 샘플링한다.
- `lookback = 1440` : 10일 전 데이터로 돌아간다.
- `steps = 6` : 1시간마다 데이터 포인트 하나를 샘플링한다.
- `delay = 144` : 24시간이 지난 데이터가 타깃이 된다. 

### 데이터 정규화하기

<script src="https://gist.github.com/ingu627/949e2eca763ce4988989074858f4d142.js"></script>

### 시계열 데이터와 타깃을 반환하는 제너레이터 함수 

<script src="https://gist.github.com/ingu627/356b0a798e9f68e72a786efc0fe89289.js"></script>

- 이 제너레이터 함수는 (samples, targets) 튜프을 반복적으로 반환한다.
- samples는 입력 데이터로 사용할 배치고 targets는 이에 대응되는 타깃 온도의 배열이다.
- **매개 변수**
- `data`: 정규화한 부동 소수 데이터로 이루어진 원본 배열
- `lookback`: 입력으로 사용하기 위해 거슬러 올라갈 타임스텝
- `delay`: 타깃으로 사용할 미래의 타임스텝
- `min_index`와 `max_index`: 추출할 타임스텝의 범위를 지정하기 위한 data 배열의 인덱스, 검증 데이터와 테스트 데이터를 분리하는 데 사용한다. 
- `shuffle`: 샘플을 섞을지, 시간 순서대로 추출할지 결정
- `batch_size`: 배치의 샘플 수 
- `step`: 데이터를 샘플링할 타임스텝 간격.

### 훈련, 검증, 테스트 제너레이터 준비하기

<script src="https://gist.github.com/ingu627/715cebc82fb6dd217db0c27a7ab09bff.js"></script>

## 6_3_4. 기본적인 머신 러닝 방법

- 복잡하고 연산 비용이 많이 드는 모델을 시도하기 전에 간단하고 손쉽게 만들 수 있는 머신 러닝 모델을 먼저 만드는 것이 좋다.
- 이를 바탕으로 더 복잡한 방법을 도입하는 근거가 마련되고 실제적인 이득도 얻게 된다.

### 완전 연결 모델을 훈련하고 평가하기

<script src="https://gist.github.com/ingu627/8f028dabba4f9e78729c707d24af1083.js"></script>

### 결과 그래프 그리기

<script src="https://gist.github.com/ingu627/bcbc2a6c61f2f6b6f62dbc6b0372291e.js"></script>

![image](https://user-images.githubusercontent.com/78655692/142822403-8fddba77-cd4d-4e0c-865e-12d1f50b4046.png)

## 6_3_5. 첫 번째 순환 신경망

- GRU(Gated Recurrent Unit) 층은 LSTM과 같은 원리로 작동하지만 조금 더 간결하고, 계산비용이 덜 든다.

### GRU를 사용한 모델을 훈련하고 평가하기

<script src="https://gist.github.com/ingu627/7f78101cca717415a8ae6c993092c41f.js"></script>

### 결과 그래프 그리기

![image](https://user-images.githubusercontent.com/78655692/142823987-bb805703-bce9-4d67-9557-6cd147060971.png)

## 6_3_6. 과대적합을 감소하기 위해 순환 드롭아웃 사용하기

- **드롭 아웃 기법**은 훈련 데이터를 층에 주입할 때 데이터에 있는 우연한 상관관계를 깨뜨리기 위해 입력 층의 유닛을 랜덤하게 끄는 기법이다.

<script src="https://gist.github.com/ingu627/22b26324a04745ffb3d7823c850fcc12.js"></script>

## 6_3_7. 스태킹 순환 층 

- 네트워크의 용량을 늘리려면 일반적으로 층에 있는 유닛의 수를 늘리거나 층을 더 많이 추가한다.
- 예를 들어 구글 번역 알고리즘의 현재 성능은 7개의 대규모 LSTM 층을 쌓은 대규모 모델에서 나온 것이다.
- `return_sequences=True` : 모든 중간층이 전체 시퀀스를 출력해야 할 때

<script src="https://gist.github.com/ingu627/8a23ce4e0c69423bab613cf1898ebbc6.js"></script>

## 6_3_8. 양방향 RNN 사용하기 

- RNN은 특히 순서 또는 시간에 민감하다. 양방향 RNN(bidirectional RNN)은 RNN이 순서에 민감하다는 성질을 사용한다. 
- GRU나 LSTM 같은 RNN 2개를 사용하는데 각 RNN은 입력 시퀀스를 한 방향(시간 순서나 반대 순서)으로 처리한 후 각 표현을 합친다.
- 시퀀스를 양쪽 방향으로 처리하기 때문에 양방향 RNN은 단방향 RNN이 놓치기 쉬운 패턴을 감지할 수 있다.

<img src="https://user-images.githubusercontent.com/78655692/142826896-d902f7a0-1497-4e91-9bb7-d3806e8f563e.png" width="500" height="400" alt="동작 원리" />

<script src="https://gist.github.com/ingu627/4b18c0331129eae4dbf537af7b302d61.js"></script>


순환 어텐션(attention)과 시퀀스 마스킹(sequence masking) 공부하기
{: .notice--danger}

## 그외 함수들 (순환신경망)

- tf.data.Dataset의 `from_tensor_slices()`는 주어진 텐서들을 첫번째 차원을 따라 슬라이스한다.

![image](https://user-images.githubusercontent.com/78655692/142894665-a34c7ebb-67d1-4226-927d-51efa9ab056b.png)


## References

- [케라스 창시자에게 배우는 딥러닝](https://www.aladin.co.kr/shop/wproduct.aspx?ItemId=173992478) 
- [시계열 예측 -tensorflow.org](tensorflow.org)