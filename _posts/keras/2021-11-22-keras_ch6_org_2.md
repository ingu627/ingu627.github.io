---
layout: single
title: "[케라스(keras) 이해] 6장. 텍스트와 시퀀스를 위한 딥러닝 (2)"
excerpt: "케라스 창시자에게 배우는 딥러닝 - 순환 신경망의 고급 사용법, 컨브넷을 사용한 시퀀스 처리"
categories: keras
tag : [python, keras, DL, ML, AI, 인고지능, 딥러닝, 케라스, NLP, dropout, 드롭아웃, 스태킹, 양방향, 시퀀스, CNN, RNN, 리뷰, 정리, 이해, 모델, 케라스 창시자에게 배우는 딥러닝]
toc: true
sidebar_main: false
classes: wide

last_modified_at: 2022-01-11
---

<img align='right' width='200' height='200' src='https://user-images.githubusercontent.com/78655692/147629300-4d7acc5e-225a-454a-92cd-4da82f6828f6.png
'>
본 글은 [케라스 창시자에게 배우는 딥러닝] - (박해선 옮김) 책을 개인공부하기 위해 요약, 정리한 내용입니다. <br>전체 코드는 [https://github.com/ingu627/deep-learning-with-python-notebooks](https://github.com/ingu627/deep-learning-with-python-notebooks)에 기재했습니다.(원본 코드 fork) <br> 뿐만 아니라 책에서 설명이 부족하거나 이해가 안 되는 것들은 외부 자료들을 토대로 메꿔 놓았습니다. 즉, 딥러닝은 이걸로 끝을 내보는 겁니다. <br> 오타나 오류는 알려주시길 바라며, 도움이 되길 바랍니다.
{: .notice--info}

<br>
<br>
<br>
<br>

## 6_3. 순환 신경망의 고급 사용법

- **순환 드롭아웃**(recurrent dropout): 순환 층에서 과대적합을 방지하기 위해 케라스에 내장되어 있는 드롭아웃을 사용한다. 
- **스태킹 순환 층**(stacking recurrent layer): 네트워크의 표현 능력을 증가시킨다. (그 대신 계산 비용이 많이 든다.)
- **양방향 순환 층**(bidirectional recurrent layer): 순환 네트워크에 같은 정보를 다른 방향으로 주입하여 정확도를 높이고 기억을 좀 더 오래 유지시킨다.

<br>
<br>

## 6_3_1. 기온 예측 문제 

### 데이터 다운 받기

<script src="https://gist.github.com/ingu627/84201460377a7bc0a8b9838f018fe47e.js"></script>

- unzip 안될 시 [https://ingu627.github.io/error/unzip/](https://ingu627.github.io/error/unzip/) 참고 
- 그래도 안될 시 [https://s3.amazonaws.com/keras-datasets/jena_climate_2009_2016.csv.zip](https://s3.amazonaws.com/keras-datasets/jena_climate_2009_2016.csv.zip) 수동 다운
- **+추가** `datasets` 폴더에 있다.. 다운받을 필요 없다.

<br>
<br>

### 예나의 날씨 데이터셋 조사하기 

<script src="https://gist.github.com/ingu627/110d937200b33f155516a7b91466110f.js"></script>

- 출력된 줄 수는 42만 551이다.

<br>
<br>

### 데이터 파싱하기 

<script src="https://gist.github.com/ingu627/b912f1938485bd0866e62c5e68ce21e7.js"></script>

![image](https://user-images.githubusercontent.com/78655692/142636345-875e45b5-c660-4cf9-9067-3a9527dd957b.png)

<br>
<br>

## 6_3_2. 데이터 준비

- **일자별 수준의 시계열 데이터를 예측 해본다.**
- lookback 타임스텝만큼 이전으로 돌아가서 매 steps 타입스탭마다 샘플링한다.
- `lookback = 1440` : 10일 전 데이터로 돌아간다.
- `steps = 6` : 1시간마다 데이터 포인트 하나를 샘플링한다.
- `delay = 144` : 24시간이 지난 데이터가 타깃이 된다. 

<br>
<br>

### 데이터 정규화하기

<script src="https://gist.github.com/ingu627/949e2eca763ce4988989074858f4d142.js"></script>

<br>
<br>

### 시계열 데이터와 타깃을 반환하는 제너레이터 함수 

<script src="https://gist.github.com/ingu627/356b0a798e9f68e72a786efc0fe89289.js"></script>

- 이 제너레이터 함수는 (samples, targets) 튜프을 반복적으로 반환한다.
- samples는 입력 데이터로 사용할 배치고 targets는 이에 대응되는 타깃 온도의 배열이다.
- **매개 변수**
- `data`: 정규화한 부동 소수 데이터로 이루어진 원본 배열
- `lookback`: 입력으로 사용하기 위해 거슬러 올라갈 타임스텝
- `delay`: 타깃으로 사용할 미래의 타임스텝
- `min_index`와 `max_index`: 추출할 타임스텝의 범위를 지정하기 위한 data 배열의 인덱스, 검증 데이터와 테스트 데이터를 분리하는 데 사용한다. 
- `shuffle`: 샘플을 섞을지, 시간 순서대로 추출할지 결정
- `batch_size`: 배치의 샘플 수 
- `step`: 데이터를 샘플링할 타임스텝 간격.

<br>
<br>

### 훈련, 검증, 테스트 제너레이터 준비하기

<script src="https://gist.github.com/ingu627/715cebc82fb6dd217db0c27a7ab09bff.js"></script>

<br>
<br>

## 6_3_4. 기본적인 머신 러닝 방법

- 복잡하고 연산 비용이 많이 드는 모델을 시도하기 전에 간단하고 손쉽게 만들 수 있는 머신 러닝 모델을 먼저 만드는 것이 좋다.
- 이를 바탕으로 더 복잡한 방법을 도입하는 근거가 마련되고 실제적인 이득도 얻게 된다.

<br>
<br>

### 완전 연결 모델을 훈련하고 평가하기

<script src="https://gist.github.com/ingu627/8f028dabba4f9e78729c707d24af1083.js"></script>

### 결과 그래프 그리기

<script src="https://gist.github.com/ingu627/bcbc2a6c61f2f6b6f62dbc6b0372291e.js"></script>

![image](https://user-images.githubusercontent.com/78655692/142822403-8fddba77-cd4d-4e0c-865e-12d1f50b4046.png)

<br>
<br>

## 6_3_5. 첫 번째 순환 신경망

- GRU(Gated Recurrent Unit) 층은 LSTM과 같은 원리로 작동하지만 조금 더 간결하고, 계산비용이 덜 든다.

<br>
<br>

### GRU를 사용한 모델을 훈련하고 평가하기

<script src="https://gist.github.com/ingu627/7f78101cca717415a8ae6c993092c41f.js"></script>

<br>
<br>

### 결과 그래프 그리기

![image](https://user-images.githubusercontent.com/78655692/142823987-bb805703-bce9-4d67-9557-6cd147060971.png)

<br>
<br>

## 6_3_6. 과대적합을 감소하기 위해 순환 드롭아웃 사용하기

- **드롭 아웃 기법**은 훈련 데이터를 층에 주입할 때 데이터에 있는 우연한 상관관계를 깨뜨리기 위해 입력 층의 유닛을 랜덤하게 끄는 기법이다.

<script src="https://gist.github.com/ingu627/22b26324a04745ffb3d7823c850fcc12.js"></script>

<br>
<br>

## 6_3_7. 스태킹 순환 층 

- 네트워크의 용량을 늘리려면 일반적으로 층에 있는 유닛의 수를 늘리거나 층을 더 많이 추가한다.
- 예를 들어 구글 번역 알고리즘의 현재 성능은 7개의 대규모 LSTM 층을 쌓은 대규모 모델에서 나온 것이다.
- `return_sequences=True` : 모든 중간층이 전체 시퀀스를 출력해야 할 때

<script src="https://gist.github.com/ingu627/8a23ce4e0c69423bab613cf1898ebbc6.js"></script>

<br>
<br>

## 6_3_8. 양방향 RNN 사용하기 

- RNN은 특히 순서 또는 시간에 민감하다. 양방향 RNN(bidirectional RNN)은 RNN이 순서에 민감하다는 성질을 사용한다. 
- GRU나 LSTM 같은 RNN 2개를 사용하는데 각 RNN은 입력 시퀀스를 한 방향(시간 순서나 반대 순서)으로 처리한 후 각 표현을 합친다.
- 시퀀스를 양쪽 방향으로 처리하기 때문에 양방향 RNN은 단방향 RNN이 놓치기 쉬운 패턴을 감지할 수 있다.

<img src="https://user-images.githubusercontent.com/78655692/142826896-d902f7a0-1497-4e91-9bb7-d3806e8f563e.png" width="500" height="400" alt="동작 원리" />

<script src="https://gist.github.com/ingu627/4b18c0331129eae4dbf537af7b302d61.js"></script>


순환 어텐션(attention)과 시퀀스 마스킹(sequence masking) 공부하기
{: .notice--danger}

<br>
<br>

### 그외 함수들 (순환신경망)

- tf.data.Dataset의 `from_tensor_slices()`는 주어진 텐서들을 첫번째 차원을 따라 슬라이스한다.

![image](https://user-images.githubusercontent.com/78655692/142894665-a34c7ebb-67d1-4226-927d-51efa9ab056b.png)

<br>
<br>

## 6_4. 컨브넷을 사용한 시퀀스 처리

- **1D 컨브넷**(1D Convnet)은 특정 시퀀스 처리 문제에서 RNN과 결줄 만한다.
- 일반적으로 계산 비용이 훨씬 싸다.
- 1D 컨브넷은 전형적으로 **팽창된 커널**(dilated kernel)과 함께 사용된다.
  - **팽창 합성곱**은 커널에 구멍을 추가하여 입력을 건너뛰면서 합성곱하는 것과 같다.
- **1D 합성곱** : 시퀀스에서 1D 패치(부분 시퀀스)를 추출하여 1D 합성곱을 적용한다.

<img src="https://user-images.githubusercontent.com/78655692/142854777-41f8c287-0473-48d4-8943-bd7be31bad0b.png" width="500" height="400" alt="동작 원리" />

- 이런 1D 합성곱 층은 시퀀스에 있는 지역 패턴을 인식할 수 있다.
- 동일한 변환이 시퀀스에 있는 모든 패치에 적용되기 때문에 특정 위치에서 학스한 패턴을 나중에 다른 위치에서 인식할 수 있다.
- 이는 **1D 컨브넷**(시간의 이동에 대한) 이동 불변성(translation invariant)을 제공한다. 
  - 예를 들어 크기 5인 윈도우를 사용하여 문자 시퀀스를 처리하는 1D 컨브넷은 5개 이하의 단어나 단어의 부분을 학습한다.
  - 이 컨브넷은 이 단어가 입력 시퀀스의 어느 문장에 있더라도 인식할 수 있다.
  - 따라서 문자 수준의 1D 컨브넷은 단어 형태학(word morphology)에 관해 학습할 수 있다.
- 1D 풀링 연산은 2D 풀링 연산과 동일하다. 입력에서 1D 패치(부분 시퀀스)를 추출하고 최댓값(최대 풀링)을 출력하거나 평균값(평균 풀링)을 출력한다.
- 2D 컨브넷과 마찬가지로 1D 입력의 길이를 줄이기 위해 사용한다. (서브샘플링)

<br>
<br>

## 6_4_3. 1D 컨브넷 구현 

- 케라스에서 1D 컨브넷은 Conv1D 층을 사용하여 구현한다.
- (samples, time, features) 크기의 3D 텐서를 입력받고 비슷한 형태의 3D 텐서를 반환한다. 
- **합성곱 윈도우**는 1D 윈도우이다. 즉, 입력 텐서의 두 번째 축이다.

<br>
<br>

### IMDB 데이터 전처리하기 

<script src="https://gist.github.com/ingu627/e7f2794e87d5860fc172ab2555c294be.js"></script>

<br>
<br>

### IMDB 데이터에 1D 컨브넷을 훈련하고 평가받기

<script src="https://gist.github.com/ingu627/fcb2770b9933c658413003a195554dd0.js"></script>

- Conv1D와 MaxPooling1D층을 쌓고 전역 풀링 층이나 Flatten 층으로 마친다.
  - **GlobalAveragePooling1D**, **GlobalMaxPooling1D**은 (sammples, timesteps, features) 크기의 텐서를 입력받고 (samples, features) 크기의 텐서를 출력한다. 즉 시간 축 전체에 풀링을 적용한다.
  - **GlobalAveragePooling2D**, **GlobalMaxPooling2D** 풀링은 (samples, height, width, channels) 크기의 텐서를 입력받고 (samples, channels) 크기의 텐서를 출력한다. 즉 특성 맵의 공간 차원 전체에 대한 풀링이다.
- 이 구조는 3D 입력을 2D 출력으로 바꾸므로 분류나 회귀를 위해 모델에 하나 이상의 Dense 층을 추가할 수 있다.

![image](https://user-images.githubusercontent.com/78655692/142859707-a26439ee-fe83-4577-b15b-c388e4ac41db.png)

<br>
<br>

## 6_4_4. CNN과 RNN을 연결하여 긴 시퀀스를 처리하기 

- 1D 컨브넷이 입력 패치를 독립적으로 처리하기 때문에 RNN과 달리 (합성곱 윈도우 크기의 범위를 넘어서는) 타임스탭의 순서에 민감하지 않다.
- 물론 장기간 패턴을 인식하기 위해 많은 합성곱 층과 풀링 층을 쌓을 수 있다.
- 컨브넷의 속도와 경량함을 RNN의 순서 감지 능력과 결합하는 한 가지 전략은 1D 컨브넷을 RNN 이전에 전처리 단계로 사용하는 것이다.
  - 수천 개의 스텝을 가진 시퀀스 같이 RNN으로 처리하기에는 현실적으로 너무 긴 시퀀스를 다룰 때 특별히 도움이 된다.
  - 컨브넷이 긴 입력 시퀀스를 더 짧은 고수준 특성의 (다운샘플된) 시퀀스로 변환한다.
  - 추출된 특성의 시퀀스는 RNN 파트의 입력이 된다.

<img src="https://user-images.githubusercontent.com/78655692/142860349-542446c7-1441-4e82-b439-854d9b3eb45f.png" width="500" height="400" alt="결합 동작 원리" />

<br>
<br>

## References

- [케라스 창시자에게 배우는 딥러닝](https://www.aladin.co.kr/shop/wproduct.aspx?ItemId=173992478) 
- [시계열 예측 -tensorflow.org](tensorflow.org)