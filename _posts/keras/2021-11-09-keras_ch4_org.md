---
layout: single
title: "[케라스(keras) 이해] 4장. 머신 러닝의 기본 요소"
excerpt: "케라스 창시자에게 배우는 딥러닝 - 머신 러닝의 네 가지 분류"
categories: keras
tag : [python, keras, DL, ML, AI, 인고지능, 딥러닝, 케라스, 머신러닝]
toc: true
sidebar_main: true
classes: wide

last_modified_at: 2022-01-07
---

<img align='right' width='200' height='200' src='https://user-images.githubusercontent.com/78655692/147629300-4d7acc5e-225a-454a-92cd-4da82f6828f6.png
'>
본 글은 [케라스 창시자에게 배우는 딥러닝] - (박해선 옮김) 책을 개인공부하기 위해 요약, 정리한 내용입니다. <br>전체 코드는 [https://github.com/ingu627/deep-learning-with-python-notebooks](https://github.com/ingu627/deep-learning-with-python-notebooks)에 기재했습니다.(원본 코드 fork) <br> 뿐만 아니라 책에서 설명이 부족하거나 이해가 안 되는 것들은 외부 자료들을 토대로 메꿔 놓았습니다. 즉, 딥러닝은 이걸로 끝을 내보는 겁니다. <br> 오타나 오류는 알려주시길 바라며, 도움이 되길 바랍니다.
{: .notice--info}

<br>
<br>
<br>
<br>

## 4_1. 머신 러닝의 네 가지 분류

- 지도 학습, 비지도 학습, 자기 지도 학습, 강화 학습

<br>
<br>

### 분류와 회귀에서 사용하는 용어

- **샘플** 또는 **입력** : 모델에 주입될 하나의 데이터 포인트
- **예측** 또는 **출력** : 모델로부터 나오는 값
- **타깃** : 정답. 외부 데이터 소스에 근거하여 모델이 완벽하게 예측해야 하는 값
- **예측 오차** 또는 **손실 값** : 모델의 예측과 타깃 사이의 거리를 측정한 값
- **클래스** : 분류 문제에서 선택할 수 있는 가능한 레이블의 집합.
  - 예를 들어 고양이와 강아지 사진을 분류할 때 클래스는 '고양이'와 '강아지' 2개이다.
- **레이블** : 분류 문제에서 선택할 수 있는 가능한 레이블의 집합.
  - 예를 들어 사진 #1234에 '강아지' 클래스가 들어 있다고 표시하면 '강아지'는 사진#1234의 레이블이 된다.
- **꼬리표**(annotation) : 데이터셋에 대한 모든 타깃. 일반적으로 사람에 의해 수집된다.
- **이진 분류** : 각 입력 샘플이 2개의 배타적인 범주로 구분되는 분류 작업
- **다중 분류** : 각 입력 샘플이 2개 이상의 범주로 구분되는 분류 작업. 예를 들어 손글씨 숫자 분류
- **다중 레이블 분류** : 각 입력 샘플이 여러 개의 레이블에 할당될 수 있는 분류 작업 
  - 예를 들어 하나의 이미지에 고양이와 강아지가 모두 들어 있을 때는 '고양이' 레이블과 '강아지' 레이블을 모두 할당해야 한다.
- **스칼라 회귀** : 타깃이 연속적인 스칼라 값인 작업. 주택 가격이 예측이 그 예 
- **벡터 회귀** : 타깃이 연속적인 값의 집합인 작업.
- **미니 배치** 또는 **배치** : 모델에 의해 동시에 처리되는 소량의 샘플 묶음(일반적으로 8개에서 128개 사이)
  - 샘플 개수는 GPU의 메모리 할당이 용이하도록 2의 거듭제곱으로 하는 경우가 많다.
  - 훈련할 때 미니 배치마다 한 번씩 모델의 가중치에 적용할 경사 하강법 업데이트 값을 계산한다.

<br>
<br>

## 4_3_1. 신경망을 위한 데이터 전처리

<br>

### 벡터화 

- 신경망에서 모든 입력과 타깃은 부동 소수 데이터로 이루어진 텐서여야 한다.
- 사운드, 이미지, 텍스트 등 처리해야 할 것이 무엇이든지 먼저 텐서로 변환해야 한다.

<br>

### 값 정규화

- 네트워크를 쉽게 학습하기 위한 데이터의 특징
  1. 데이터는 작은 값은 취한다. 일반적으로 대부분의 값이 0~1 사이여야 한다.
  2. 균일해야 한다. 모든 특성이 대체로 비슷한 범위를 가져야 한다.

<br>

### 누락된 값 다루기

- 훈련 샘플의 일부를 여러벌 복사해서 테스트 데이터에서 빠질 것 같은 특성을 제거한다. 

<br>

### 특성 공학

- 모델에 데이터를 주입하기 전에 하드코딩된 변환을 적용하여 알고리즘이 더 잘 수행되도록 만들어 준다.
- 특성을 더 간단한 방식으로 표현하여 문제를 쉽게 만든다.

<br>
<br>

## 4_4_2. 가중치 규제 증가 

- **오캄의 면도날**(Occam's razor) : 어떤 것에 대한 두 가지의 설명이 있다면 더 적은 가정이 필요한 간단한 설명이 옳을 것이라는 이론
  - 간단한 모델이 복잡한 모델보다 덜 과대적합될 가능성이 높다.
- 과대적합을 완화하기 위한 일반적인 방법은 네트워크의 **복잡도에 제한을 두어 가중치가 작은 값을 가지도록 강제하는 것**. 가중치 값의 분포가 더 균일하게 된다. (**가중치 규제**)
  - 네트워크의 손실 함수에 큰 가중치에 연관된 비용을 추가한다. 
  - 두 가지 형태의 비용이 있다.
1. **L1 규제** : 가중치의 절댓값에 비례하는 비용이 추가된다.
2. **L2 규제** : 가중치의 제곱에 비례하는 비용이 추가된다. (=가중치 감쇠(weight decay))
   - 가중치가 커지는 것을 억제

- 케라스에서 가중치 규제 객체를 층의 키워드 매개변수로 전달하여 가중치 규제를 추가할 수 있다.

<script src="https://gist.github.com/ingu627/c874596cc69151da84cad5932d66a632.js"></script>

- `l2(0.001)` : 가중치 행렬의 모든 원소를 제곱하고 0.001을 곱하여 네트워크의 전체 손실에 더해진다는 의미.
  - 이 페널티 항은 훈련할 때만 추가된다.

<br>
<br>

### 드롭아웃(dropout)

- 뉴런의 연결을 임의로 삭제하는 것
- 네트워크 층에 드롭아웃을 적용하면 훈련하는 동안 무작위로 층의 일부 출력 특성을 제외시킨다.
- **층의 출력 값에 노이즈를 추가하여 중요하지 않은 우연한 패턴을 깨뜨리는 것이다.**
  - 노이즈가 없다면 네트워크가 이 패턴을 기억하기 시작할 것이다.
- 케라스에서는 층의 출력 바로 뒤에 Dropout 층을 추가하여 네트워크에 드롭아웃을 적용할 수 있다.
- `model.add(layers.Dropout(0.5))

<br>
<br>

### 신경망에서 과대적합을 방지하는 방법 

1. 훈련 데이터를 더 모은다.
2. 네트워크의 용량을 감소시킨다.
3. 가중치 규제를 추가한다.
4. 드롭아웃을 추가한다.

<br>
<br>

### 모델에 맞는 마지막 층의 활성화 함수와 손실 함수 선택

|문제 유형 | 마지막 층의 활성화 함수 | 손실 함수 |
| --- | ---| ---|
|이진 분류 | sigmoid | binary_crossentropy|
|단일 레이블 다중 분류 | softmax | categorical_crossentropy |
|다중 레이블 다중 분류 | sigmoid | binary_crossentropy |
|임의 값에 대한 회귀 | X | mse |
|0과 1 사이 값에 대한 회귀 | sigmoid | mse 또는 binary_crossentropy |

<br>
<br>

## 4_5_7. 모델 규제와 하이퍼파라미터 튜닝 

- 드롭아웃을 추가한다.
- 층을 추가하거나 제거해서 다른 구조를 시도해 본다.
- L1이나 L2 도는 두 가지 모두 추가한다.
- 최적의 설정을 찾기 위해 하이퍼파라미터를 바꾸어 시도해 본다. (층의 유닛 수나 옵티마이저의 학습률 등)
- 선택적으로 특성 공학을 시도해 본다. 새로운 특성을 추가하거나 유용하지 않을 것 같은 특성을 제거한다.

<br>
<br>

## References

- [케라스 창시자에게 배우는 딥러닝](https://www.aladin.co.kr/shop/wproduct.aspx?ItemId=173992478)  