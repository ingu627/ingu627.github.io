---
layout: single
title: "[핸즈온 머신러닝] 10장. 케라스를 사용한 인공 신경망 소개"
excerpt: "part 2 신경망과 딥러닝 부분을 개인공부를 목적으로 내용 요약 및 정리한 글입니다. - 케라스를 사용한 인공 신경망 소개"
categories: hands_on
tag : [python, tensorflow, DL, 텐서플로, 딥러닝, 인공 신경망, 뉴런, 퍼셉트론, dense, 입력층, 다층 퍼셉트론, DNN, 역전파, 에포크, 연쇄법칙, 오차, 경사 하강법, 활성화 함수, 케라스, 컴파일,함수형 API, 콜백]
toc: true
sidebar_main: true
classes: wide

last_modified_at: 2022-03-21
---

<img align='right' width='200' height='200' src='https://user-images.githubusercontent.com/78655692/147628941-a1aeb296-324e-4a60-816b-e4cc6666d13e.png
'>
본 글은 [핸즈온 머신러닝 2판 (Hands-On Machine Learning with Scikit-Learn, keras & TensorFlow)] - (박해선 옮김) 책을 개인공부하기 위해 요약, 정리한 내용입니다. <br>전체 코드는 <https://github.com/ingu627/handson-ml2>에 기재했습니다.(원본 코드 fork) <br> 뿐만 아니라 책에서 설명이 부족하거나 이해가 안 되는 것들은 외부 자료들을 토대로 메꿔 놓았습니다. <br> 오타나 오류는 알려주시길 바라며, 도움이 되길 바랍니다.
{: .notice--info}

<br>
<br>
<br>
<br>

## 10장. 케라스를 사용한 인공 신경망 소개

- 지능적인 기계를 만드는 법에 대한 영감을 얻으려면 뇌 구조를 살펴보는 것이 합리적이다. 이는 **인공 신경망**(ANN; Artificial Neural Network)을 촉발시킨 근원이다. 
- **인공 신경망**은 뇌에 있는 생물학적 뉴런의 네트워크에서 영감을 받은 머신러닝 모델이다.
  - 요즈음에는 `뉴런`(neuron) 대신 `유닛`(unit)으로 부른다.
- **케라스**는 신경망 구축, 훈련, 평가, 실행을 목적으로 설계된 멋지고 간결한 고수준 API이다.

<br>

## 10.1 생물학적 뉴런에서 인공 뉴런까지 

![image](https://user-images.githubusercontent.com/78655692/146319097-9bb0cf08-ca8d-452f-8611-ab0cdb036409.png) <br> 이미지 출처[^1]<br>

- 예전과는 다르게 인공 신경망이 우리 생활에 훨씬 커다란 영향을 줄 것이라는 믿을 만한 근거가 몇가지 있다.

1. 신경망을 훈련하기 위한 데이터가 엄청나게 많아졌다. 인공 신경망은 종종 규모가 크고 복잡한 문제에서 다른 머신러닝 기법보다 좋은 성능을 낸다.
2. 1990년대 이후 컴퓨터 하드웨어가 크게 발전했다. 덕분에 납득할 만한 시간 안에 대규모 신경망을 훈련할 수 있다. 또한 수백만 개의 강력한 GPU 카드를 생산해내는 게임 산업 덕분이기도 하고 클라우드 플랫폼은 이런 강력한 도구를 손쉽게 사용할 수 있는 환경을 제공한다.

<br>

### 10.1.2 뉴런을 사용한 논리 연산

<img src="https://user-images.githubusercontent.com/78655692/146319960-39f6eaf4-618e-4c5e-b98d-9538d36133c8.png" width="650" alt="생물학적 뉴런"> <br> 이미지 출처[^2] <br>

- 이 세포는 핵을 포함하는 세포체와 복잡한 구성 요소로 이루어져 있다. 
- **수상돌기**라는 나뭇가지 모양의 돌기와 **축삭돌기**라는 아주 긴 돌기 하나가 있다. 축삭돌기의 끝은 **축삭끝가지**라는 여러 가지로 나뉘고, 이 가지 끝은 **시냅스 말단**이라는 미세한 구조이며 다른 뉴런의 수상돌기나 세포체에 연결된다.
- 다시 정리하면 각각의 뉴런은 수상돌기를 통해 다른 뉴런에서 입력 신호를 받아서 축색돌기를 통해 다른 뉴런으로 신호를 내보낸다. 시냅스는 뉴런과 뉴런을 연결하는 역할을 한다. 출력신호는 입력된 신호가 모여서 일정한 용량을 넘어설 때 일어난다.[^3]

- **Dendrite** : 수상돌기, 다른 뉴런으로부터 신호를 수용하는 부분
- **Axon** : 축삭돌기, 신호를 내보내는 부분
- **Synaptic terminals** : 시냅스(synapse) 뉴런의 접합부, 다른 뉴런으로 부터 짧은 전기 자극 신호(signal)를 받음

<br>

### 10.1.2 뉴런을 사용한 논리 연산

![image](https://user-images.githubusercontent.com/78655692/146321804-9fe68a2f-880e-4c72-a59b-bc9c5f6fb175.png)

- 인공 뉴런 모델은 하나 이상의 이진(on/off) 입력과 이진 출력 하나를 가진다.
- **인공 뉴런**은 단순히 입력이 일정 개수만큼 활성화되었을 때 출력을 내보낸다.
- ①은 항등함수를 의미하며, 뉴런 A가 활성화 되면 뉴런 C 또한 활성화된다.
- ②는 논리곱 연산을 의미하며, 뉴런 A와 B가 모두 활성화될 때만 뉴런 C가 활성화된다. 
- ③은 논리합 연산을 의미하며, 뉴런 A와 B 둘 중 하나라도 활성화되면 C가 활성화된다.
- ④는 어떤 입력이 뉴런의 활성화를 억제할 수 있다고 가정하면, 위의 그림에서 처럼 뉴런 A가 활성화 되고 B가 비활성화될 때 뉴런 C가 활성화된다.[^4]

<br>

### 10.1.3 퍼셉트론

- **퍼셉트론**은 가장 간단한 인공 신경망 구조 중 하나이다.
- 퍼셉트론은 **TLU**(Threshold Logic Unit)이라는 형태의 뉴런을 기반으로 하며, 아래의 그림과 같이 입력과 출력이 어떤 숫자고 각각의 입력에 각각 고유한 가중치($W$, weight)가 곱해진다.
- TLU는 입력의 가중치 합을 계산 ($z = w_1x_1 + w_2x_2 + ... + w_nx_n = x^tw$)한 뒤 계산된 합에 계단 함수(step function)를 적용하여 결과를 출력한다. 즉, $h_w(x) = step(z)$, 여기에서 $z=x^tw$이다.

![image](https://user-images.githubusercontent.com/78655692/146322983-e8b764e7-e63c-4a9b-9fc6-b12e2840ced3.png)

- 퍼셉트론에서 가장 널리 사용되는 계단 함수는 **헤비사이드 계단 함수**(Heaviside step function)이다. 부호 함수(sign function)를 대신 사용하기도 한다.

  - $heaviside(z) = (0 \quad z<0일때) \\ (1 \quad z\ge0일때)$
  - $sgn(z) = (-1 \quad z<0일때) \\ (0 \quad z=0일때) \\ (+1 \quad z>0일때)$

- 하나의 TLU는 간단한 선형 **이진 분류 문제**에 사용할 수 있다. 입력의 선형 조합을 계산해서 그 결과가 임곗값을 넘으면 양성 클래스를 출력한다. 그렇지 않으면 음성 클래스를 출력한다. (로지스틱 회귀나 선형 SVM 분류기처럼)
- TLU를 훈련한다는 것은 최적의 $w_0, w_1, w_2$를 찾는다는 뜻이다.
- 퍼셉트론은 층이 하나뿐인 TLU로 구성된다. 각 TLU은 모든 입력에 연결되어 있다. 한 층에 있는 모든 뉴런이 이전 층의 모든 뉴런과 연결되어 있을 때 이를 **완전 연결 층** 또는 **밀집 층**이라고 부른다. 

![image](https://user-images.githubusercontent.com/78655692/146323635-ef8fdc8e-d06e-408d-aa4e-ab51b70327aa.png)

- **입력층** (input layer)은 모두 입력 뉴런으로 구성된다. 보통 여기에 **편향** 특성이 더해진다. 
- 위 그림은 입력 두 개와 출력 세 개로 구성된 퍼셉트론이다. 이 퍼셉트론은 샘플을 세 개의 다른 이진 클래스로 동시에 분류할 수 있으므로 다중 레이블 분류기(multilabel classifier)이다.
- 완전 연결 층의 출력 계산 : $h_{W,b}(X) = \phi(XW + b)$
  - $X$ : 입력 특성의 행렬을 나타냄
    - 이 행렬의 행은 샘플, 열은 특성
  - $W$ : 가중치 행렬. 편향 뉴런을 제외한 모든 연결 가중치를 포함
    - 이 행렬의 행은 입력 뉴런에 해당하고 열은 출력층에 있는 인공 뉴런에 해당
  - $b$ : 편향 벡터. 편향 뉴런과 인공 뉴런 사이의 모든 연결 가중치를 포함
    - 인공 뉴런마다 하나의 편향 값이 있다.
  - $\phi$ : 활성화 함수 (activation function)

<br>

- 두 뉴런이 동일한 출력을 낼 때마다 이 둘 사이의 연결 가중치가 증가하며 이러한 규칙을 헤브의 규칙 또는 **헤브 학습**(Hebbian Learning)이라고 한다. (`퍼셉트론 훈련`)
- **퍼셉트론**은 네트워크가 예측할 때 만드는 오차를 반영하도록 조금 변형된 규칙을 사용하여 훈련된다. 퍼셉트론 학습 규칙은 오차가 감소되도록 연결을 강화시킨다.
- **퍼셉트론**은 네트워크가 만드는 에러를 반영하도록 학습되며 잘못된 출력을 만드는 연결은 올바른 출력을 만들 수 있도록 가중치를 조정한다.
- 퍼셉트론 학습 규칙(가중치 업데이트) 
  - $w_{\rm i,j}^{\rm next\   step} = w_{\rm i,j} + \eta(y_j - y_j)\hat x_i$
    - $w_{\rm i,j}$: $i$번째 입력 뉴런과 $j$번째 출력 뉴런 사이를 연결하는 가중치
    - $x_i$ : 현재 학습 데이터 샘플의 $i$번째 뉴런의 입력값
    - $\hat y$ : 현재 학습 데이터 샘플의 -번째 출력 뉴런의 출력값
    - $y_i$ : 현재 학습 데이터 샘플의 -번째 출력 뉴런의 실제값
    - $\eta$ : 학습률, learning rate
- 각 출력 뉴런의 결정 경계는 선형이므로 퍼셉트론도 복잡한 패턴을 학습하지 못한다.
- 하지만 **퍼셉트론 수렴 이론**으로 즉, 훈련 샘플이 선형적으로 구분될 수 있다면 이 알고리즘이 정답에 수렴한다는 것을 증명 

```python
import numpy as np
from sklearn.datasets import load_iris
from sklearn.linear_model import Perceptron

iris = load_iris()
X = iris.data[:, (2, 3)]  # 꽃잎 길이, 꽃잎 너비
y = (iris.target == 0).astype(np.int)

per_clf = Perceptron(max_iter=1000, tol=1e-3, random_state=42)
per_clf.fit(X, y)

y_pred = per_clf.predict([[2, 0.5]])

y_pred
# array([1])
```

```python
a = -per_clf.coef_[0][0] / per_clf.coef_[0][1]
b = -per_clf.intercept_ / per_clf.coef_[0][1]

axes = [0, 5, 0, 2]

x0, x1 = np.meshgrid(
        np.linspace(axes[0], axes[1], 500).reshape(-1, 1),
        np.linspace(axes[2], axes[3], 200).reshape(-1, 1),
    )
X_new = np.c_[x0.ravel(), x1.ravel()]
y_predict = per_clf.predict(X_new)
zz = y_predict.reshape(x0.shape)

plt.figure(figsize=(10, 4))
plt.plot(X[y==0, 0], X[y==0, 1], "bs", label="Not Iris-Setosa")
plt.plot(X[y==1, 0], X[y==1, 1], "yo", label="Iris-Setosa")

plt.plot([axes[0], axes[1]], [a * axes[0] + b, a * axes[1] + b], "k-", linewidth=3)
from matplotlib.colors import ListedColormap
custom_cmap = ListedColormap(['#9898ff', '#fafab0'])

plt.contourf(x0, x1, zz, cmap=custom_cmap)
plt.xlabel("Petal length", fontsize=14)
plt.ylabel("Petal width", fontsize=14)
plt.legend(loc="lower right", fontsize=14)
plt.axis(axes)

save_fig("perceptron_iris_plot")
plt.show()
```
![image](https://user-images.githubusercontent.com/78655692/146347505-015e8a0f-332f-4b2d-8f52-0875dad0b66a.png)

- Perceptron 클래스는 매개변수가 loss="perceptron", learning_rate="constant", eta0=1(학습률), penalty=None(규제 없음)인 SGDClassifier와 같다.
- 로지스틱 회귀 분류기와 달리 퍼셉트론은 클래스 확률을 제공하지 않으며 고정된 임곗값을 기준으로 예측을 만든다.
- 퍼셉트론을 여러 개 쌓아올리면 일부 제약을 줄일 수 있다. 이런 인공신경망을 **다층 퍼셉트론**(MLP)라 한다.
  - MLP는 XOR 문제를 풀 수 있다.

![image](https://user-images.githubusercontent.com/78655692/146353566-82f47bbc-4a4d-456d-b527-acf1a08bd50a.png)
이미지 출처[^5]

<br>

### 10.1.4 다층 퍼셉트론과 역전파

- 다층 퍼셉트론은 (통과) **입력층**(input layer) 하나와 **은닉층**(hidden layer)이라 불리는 하나 이상의 TLU 층과 마지막 **출력층**(output layer)으로 구성된다. 입력층과 가까운 층을 보통 **하위 층**(lower layer)이라 부르고 출력에 가까운 층을 **상위 층**(upper layer)라고 부른다. 출력층을 제외하고 모든 층은 편향 뉴런을 포함하며 다음 층과 완전히 연결되어 있다.

![image](https://user-images.githubusercontent.com/78655692/146354772-28308ce6-34e1-415f-be77-7b268cef2d04.png)

> 신호는 (입력에서 출력으로) 한 방향으로만 흐른다. (이런 구조를 피드포워드 신경망(FNN))에 속한다.

- 은닉층을 `여러 개` 쌓아 올린 인공 신경망을 **심층 신경망**(DNN; deep neural network)라고 한다.
- 딥러닝은 심층 신경망을 연구하는 분야이며 조금 더 일반적으로는 연산이 연속하여 길게 연결된 모델을 연구한다.
- 이렇게 여러층을 쌓은 MLP를 통해 XOR 문제를 해결 했지만, 층이 깊어질 수록 증가하는 가중치 매개변수의 수로 인해 다층 퍼셉트론을 학습시키기에는 오랜 시간이 걸리는 문제가 발생했다. 하지만, 1986년 역전파(backpropogation) 알고리즘이 등장하면서 계산량을 획기적으로 줄일 수 있게 되었다.[^6]
  - **역전파 알고리즘**(backpropagation)은 효율적인 기법으로 그레이디언트를 자동으로 계산(== **자동 미분**)하는 경사 하강법이다. 
  - 네트워크를 두 번(정방향 한번, 역방향 한 번) 통과하는 것만으로 이 역전파 알고리즘은 모든 모델 파라미터에 대한 네트워크 오차의 그레이디언트를 계산할 수 있다.
  - 다른 말로 하면 오차를 감소시키기 위해 각 연결 가중치와 편향값이 어떻게 바뀌어야 할지 알 수 있다. 

<br>

### 알고리즘

0. 한 번에 (예를 들어, 32개의 샘플이 포함된) 하나의 미니배치씩 진행하여 전체 훈련 세트를 처리한다. 이 과정을 여러 번 반복한다. 각 반복을 **에포크**(epoch)라 한다.
1. 각 미니배치는 네트워크의 입력층으로 전달되어 첫 번째 은닉층으로 보내진다. 
2. 해당 층에 있는 모든 뉴런의 출력을 계산한다. 이 결과는 다음 층으로 전달된다.
3. 다시 이 층의 출력을 계산하고 결과는 다음 층으로 전달된다. (**정방향 계산**)
4. 알고리즘이 네트워크의 출력 오차를 측정한다.(즉, 손실 함수를 사용하여 기대하는 출력과 네트워크의 실제 출력을 비교하고 오차 측정 값을 반환한다.)
5. 각 출력 연결이 이 오차에 기여하는 정도를 계산한다. (**연쇄법칙** 적용)
6. 연쇄 법칙을 계속 사용하여 이전 층의 연결 가중치가 이 오차의 기여 정도에 얼마나 기여했는지 측정한다. 이렇게 입력층에 도달할 때까지 역방향으로 계속된다.
7. 알고리즘은 경사 하강법을 수행하여 방금 계산한 오차 그레이디언트를 사용해 네트워크에 있는 모든 여결 가중치를 수정한다.

- **위를 다시 요약 정리**

1. 각 훈련 샘플에 대해 역전파 알고리즘이 먼저 예측을 만들고(정방향 계산) 오차를 측정한다.
2. 역방향으로 각 층을 거치면서 각 연결이 오차에 기여한 정도를 측정한다. (역방향 계산)
3. 이 오차가 감소하도록 가중치를 조정한다. (경사 하강법 단계)

<br>

### 왜 활성화 함수가 필요할까?

- 선형 변환을 여러 개 연결해도 얻을 수 있는 것은 선형 변환뿐이다.
- 층 사이에 비선형성을 추가하지 않으면 아무리 층을 많이 쌓아도 하나의 층과 동일해진다. 이런 층으로는 복잡한 문제를 풀 수 없다.
- 반대로 **비선형 활성화 함수가 있는 충분히 큰 심층 신경망**은 이론적으로 어떤 연속 함수도 근사할 수 있다.

![image](https://user-images.githubusercontent.com/78655692/146358398-658ecaf2-cd26-4cc9-8cbb-d97a4fdf68a3.png)

<br>

<img src='https://user-images.githubusercontent.com/78655692/147922323-c7cedd04-0c3c-4ba9-94ec-df4bbef3394b.png' width=350>

- **로지스틱(시그모이드) 함수** : $\sigma(z) = \frac {1}{1+exp(-z)}$
- **하이퍼볼릭 탄젠트 함수** (쌍곡 탄젠트 함수) : $\tanh(z) = 2\sigma(2z) - 1$
- **ReLU 함수** : $ReLU(z)=max(0, z)$
  - ReLU 함수의 값이 0보다 클 때 기울기는 항상 1이므로 오차 그레이디언트를 그대로 역전파시킨다.
  - ReLU 함수 값이 0보다 작거나 같을 때는 오차 그레이디언트를 역전파하지 않는다.

> 로지스틱 함수나 하이퍼볼릭 탄젠트 함수는 출력 범위가 0~1 또는 -1~1 사이로 한정되어 양극단에서 기울기가 급격히 감소하므로 오차 그레이디언트를 잘 역전파하지 못한다.

<br>

### 10.1.5 회귀를 위한 다층 퍼셉트론

- 회귀 MLP의 전형적인 구조

|하이퍼파라미터|일반적인 값|
|---|---|
|입력 뉴런수 | 특성마다 하나 (예를 들어 MNIST의 경우 28x28=784)
|은닉층 수 | 문제에 따라 다름. 일반적으로 1에서 5 사이
|은닉층의 뉴런 수 |문제에 따라 다름. 일반적으로 10에서 100사이
|출력 뉴런 수 |예측 차원마다 하나
|은닉층의 활성화 함수 |ReLU
|출력층의 활성화 함수 |1. 없음 <br>2. 출력이 양수일 때 : ReLU/sotfplus <br> 3. 출력을 특정 범위로 제한할 때 : logisitc(0~1)/tanh(-1~1)를 사용 
|손실 함수 |MSE나 (이상치가 있다면)MAE

<br>

- **출력 뉴런 수** : 예측해야 하는 값의 수에 따라 출력 뉴런 설정

<br>

### 10.1.6 분류를 위한 다층 퍼셉트론

- **다층 퍼셉트론**은 분류 작업에도 사용할 수 있다.
- 이진 분류 문제에서는 **로지스틱 활성화 함수**를 가진 하나의 출력 뉴런만 필요하다. 
  - 출력은 0과 1사이의 실수이다.
- **다층 퍼셉트론**은 다중 레이블 이진 분류 (multilabel binary classification) 문제를 쉽게 처리할 수 있다.
- 각 샘플이 3개 이상의 클래스 중 한 클래스에만 속할 수 있다면 (예를 들어 숫자 이미지 분류에서 클래스 0에서 9까지) 클래스마다 하나의 출력 뉴런이 필요하다. 출력층에는 **소프트맥스 활성화 함수**를 사용해야 한다. 
  - **소프트맥스 함수**는 모든 예측 확률을 0과 1 사이로 만들고 더했을 때 1이 되도록 만든다. 이를 **다중 분류** (multiclass classifcation)라고 부른다.
- 확률 분포를 예측해야 하므로 손실 함수에는 일반적으로 크로스 엔트로피 손실(cross-entropy loss)을 선택하는 것이 좋다.

![image](https://user-images.githubusercontent.com/78655692/146359986-4fc952b8-0e93-48cc-9cfb-932850b03633.png)

- 분류 MLP의 전형적인 구조

|하이퍼파라미터|이진 분류|다중 레이블 분류|다중 분류|
|---|---|---|---|
|입력층과 은닉층|회귀와 동일|회귀와 동일|회귀와 동일
|출력 뉴런 수|1개 |레이블마다 1개|클래스마다 1개
|출력층의 활성화 함수|로지스틱 함수|로지스틱 함수|소프트맥스 함수
|손실 함수|크로스 엔트로피|크로스 엔트로피|크로스 엔트로피


<br>
<br>

## 10.2 케라스로 다층 퍼셉트론 구현하기

- **케라스**는 모든 종류의 신경망을 손쉽게 만들고 훈련, 평가, 실행할 수 있는 고수준 딥러닝 API이다.
- 텐서플로와 케라스 다음으로 가장 인기 있는 딥러닝 라이브러리는 페이스북 **파이토치** (PyTorch)이다.

<br>

## 10.2.1 텐서플로 2 설치

- <https://ingu627.github.io/tips/install_cuda/>에서 자세히 다룬바 있다.
- UPDATE : <https://ingu627.github.io/tips/install_cuda2/> (Tensorflow 2.8 버전 설치)

<br>

## 10.2.2 시퀀셜 API를 사용하여 이미지 분류기 만들기

```python
# 케라스를 사용하여 데이터셋 적재하기
import keras
import tensorflow as tf

fashion_mnist = tf.keras.datasets.fashion_mnist
(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()

print(X_train_full.shape) # (60000, 28, 28)
print(X_train_full.dtype) # uint8

# 경사 하강법으로 신경망을 훈련하기 때문에 입력 특성의 스케일을 조정해야 한다. 255(최대 범위)로 나누어 0~1 사이로 조정
X_valid, X_train = X_train_full[:5000] / 255.0, X_train_full[5000:] / 255.0
y_valid, y_train = y_train_full[:5000], y_train_full[5000:]
X_test = X_test / 255.0

# 클래스 이름의 리스트 만들기
class_names = ["T-shirt/top", "Trouser", "Pullover", "Dress", "Coat", "Sandal", "Shirt", "Sneaker", "Bag", "Ankle boot"]
```

<br>

### 시퀀셜 API를 사용하여 모델 만들기

```python
from tensorflow.keras.layers import Flatten, Dense
from tensorflow.keras.models import Sequential

model = Sequential() # 순서대로 연결된 층을 일렬로 쌓아서 구성한다.
model.add(Flatten(input_shape=[28,28])) # 모델의 첫 번째 층이므로 input_shpae를 지정해야 한다. 이 층은 어떤 모델 파라미터도 가지지 않고 간단한 전처리를 수행할 뿐이다. 여기에는 배치 크기를 제외하고 샘플의 크기만 써야 한다. 
model.add(Dense(300, activation='relu')) # 뉴런 300개를 가진 Dense 은닉층을 추가한다. 이 층은 ReLU 활성화 함수를 사용한다. Dense 층마다 각자 가중치 행렬을 관리한다. 이 행렬에는 층의 뉴런과 입력 사이의 모든 연결 가중치가 포함된다. 또한 (뉴런마다 하나씩 있는) 편향도 벡터로 관리한다. 
model.add(Dense(300, activation='relu'))
model.add(Dense(10, activation='softmax')) # (클래스마다 하나씩) 뉴런 10개를 가진 Dense 출력층을 추가한다. 배타적인 클래스이므로 소프트맥스 활성화 함수를 사용한다.
```

- `Flatten()` : 입력 이미지를 1D 배열로 변환

<br>

- `summary()` : 모델에 있는 모든 층을 출력한다. (각 층의 이름 + 출력 크기 + 파라미터 개수)

```python
model.summary()

Model: "sequential"
# _________________________________________________________________
# Layer (type)                 Output Shape              Param #   
# =================================================================
# flatten (Flatten)            (None, 784)               0         
# _________________________________________________________________
# dense (Dense)                (None, 300)               235500    
# _________________________________________________________________
# dense_1 (Dense)              (None, 300)               90300     
# _________________________________________________________________
# dense_2 (Dense)              (None, 10)                3010      
# =================================================================
# Total params: 328,810
# Trainable params: 328,810
# Non-trainable params: 0
# _________________________________________________________________
```

- 첫 번째 은닉층은 784 x 300 개의 연결 가중치와 300개의 편향을 가진다. 이를 더하면 235,500개의 파라미터를 가진다.
  - 이런 모델은 훈련 데이터를 학습하기 충분한 유연성을 가진다.
- `Dense` 층은 대칭성을 깨뜨리기 위해 연결 가중치를 무작위로 초기화한다. 편향은 0으로 초기화한다. 
  - 가중치 행렬의 크기는 입력의 크기에 달려 있다. 이 때문에 Sequential 모델에 첫 번째 층을 추가할 때 input_shape 매개변수를 지정한 것이다. 

<br>

### 모델 컴파일

- 모델을 만들고 나서 compile() 메서드를 호출하여 사용할 손실 함수와 옵티마이저를 지정해야 한다.

```python
model.compile(loss="sparse_categorical_crossentropy",
optimizer='sgd',
metrics=['accuracy']) 
```

- 클래스가 배타적이므로 `sparsed_categorical_crossentropy` 사용.
  - 만약 샘플마다 클래스별 타깃 확률을 가지고 있다면 대신 `categorical_crossentropy`손실을 사용해야 한다.
  - 이진 분류나 다중 레이블 이진 분류를 수행한다면 "sigmoid" 함수를 사용하고 `binary_Crossentropy` 손실을 사용한다.
- `sgd` = stochastic gradient descent (default = lr=0.01)
  - 옵티마이저에 sgd를 지정하면 역전파 알고리즘을 수행하는 것이다.

<br>

### 모델 훈련과 평가

```python
history = model.fit(X_train, y_train, epochs=30,
                    validation_data = (X_valid, y_valid))

# Epoch 1/30
# 1719/1719 [==============================] - 12s 6ms/step - loss: 1.0080 - accuracy: 0.6764 - val_loss: 0.5268 - val_accuracy: 0.8168
# Epoch 2/30
# 1719/1719 [==============================] - 10s 6ms/step - loss: 0.5108 - accuracy: 0.8239 - val_loss: 0.4558 - val_accuracy: 0.8476
# Epoch 3/30
# 1719/1719 [==============================] - 11s 6ms/step - loss: 0.4533 - accuracy: 0.8430 - val_loss: 0.4098 - val_accuracy: 0.8634
# Epoch 4/30
# 1719/1719 [==============================] - 10s 6ms/step - loss: 0.4222 - accuracy: 0.8512 - val_loss: 0.4230 - val_accuracy: 0.8574
# Epoch 5/30
# 1719/1719 [==============================] - 10s 6ms/step - loss: 0.4025 - accuracy: 0.8583 - val_loss: 0.4019 - val_accuracy: 0.8598
# Epoch 6/30
# 1719/1719 [==============================] - 10s 6ms/step - loss: 0.3807 - accuracy: 0.8658 - val_loss: 0.3893 - val_accuracy: 0.8618
# Epoch 7/30
# 1719/1719 [==============================] - 11s 6ms/step - loss: 0.3699 - accuracy: 0.8711 - val_loss: 0.3613 - val_accuracy: 0.8748
# ...
# Epoch 29/30
# 1719/1719 [==============================] - 10s 6ms/step - loss: 0.2322 - accuracy: 0.9151 - val_loss: 0.3025 - val_accuracy: 0.8916
# Epoch 30/30
# 1719/1719 [==============================] - 10s 6ms/step - loss: 0.2277 - accuracy: 0.9185 - val_loss: 0.3012 - val_accuracy: 0.8912
```

- 모델을 훈련하려면 `fit()` 메서드를 호출한다.
- 입력 특성과 타깃 클래스, 훈련할 에포크 횟수를 전달한다. 검증 세트도 전달한다. (optional) 
  - `X_train` : 입력 특성
  - `y_train` : 타깃 클래스
  - `epochs` : 훈련할 에포크 횟수
- 케라스는 에포크가 끝날 때마다 검증 세트를 사용해 손실과 추가적인 측정 지표를 계산한다.
  - 이 지표는 모델이 얼마나 잘 수행되는지 확인하는 데 유용하다. 
- 케라스는 (진행 표시줄과 함께) 처리한 샘플 개수와 샘플마다 걸린 평균 훈련 시간, 훈련 세트와 검증 세트에 대한 손실과 정확도를 출력한다.

<br>

- **모델 성능 테스트**
  - 훈련된 모델의 `evaluate` 메서드 활용
  - 손실과 정확도 계산해줌

```python
model.evalute(X_test, y_test)
```

<br>

- 어떤 클래스는 많이 등장하고 다른 클래스는 조금 등장하여 훈련 세트가 편중되어 있다면 fit() 메서드를 호출할 때 **class_weight** 매개변수를 지정하는 것이 좋다.
  - 적게 등장하는 클래스는 높은 가중치를 부여하고 많이 등장하는 클래스는 낮은 가중치를 부여한다.
  - 케라스가 손실을 계산할 때 이 가중치를 사용한다.
- 샘플별로 가중치를 부여하고 싶다면 `sample_weight` 매개변수를 지정한다.
  - 각 샘플의 손실에 가중치를 곱한 후 전체 샘플 개수로 나누어 손실을 계산한다.
- fit() 메서드가 반환하는 History 객체에는 에포크가 끝날 때마다 훈련 세트와 (있다면) 검증 세트에 대한 손실과 측정한 지표를 담은 **딕셔너리 (history.history)**가 있다.

```python
# 학습 곡선
import pandas as pd
import matplotlib.pyplot as plt

pd.DataFrame(history.history).plot(figsize=(8,5))
plt.grid(True)
plt.gca().set_ylim(0,1)
plt.show()
```

![output](https://user-images.githubusercontent.com/78655692/146779584-7d03fd28-b752-4826-bfb4-b6a26c2bb9e2.png)

- 훈련하는 동안 훈련 정확도와 검증 정확도가 꾸준히 상승하고 훈련 손실과 검증 손실은 감소한다.
- 검증 손실은 에포크가 끝난 후에 계산되고 훈련 손실은 에포크가 진행되는 동안 계산된다.
- 케라스에서는 `fit()` 메서드를 다시 호출하면 중지되었던 곳에서부터 훈련을 이어갈 수 있다.
- 모델 성능에 만족스럽지 않으면 처음부터 되돌아가서 `하이퍼파라미터`를 튜닝해야 한다.
  - 여전히 성능이 높지 않으면 층 개수, 층에 있는 뉴런 개수, 은닉층이 사용하는 활성화 함수와 같은 모델의 하이퍼파리미터를 튜닝해 본다.

<br>

### 모델을 사용해 예측 만들기

- `predict()` 메서드를 사용해 새로운 샘플에 대해 예측을 만들 수 있다.
- **predict()** : 각 클래스에 속할 확률 계산
- **predict_class()** : 가장 높은 확률의 클래스 지정

<br>

## 10.2.3 시퀀셜 API를 사용하여 회귀용 다층 퍼셉트론 만들기

```python
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

housing = fetch_california_housing()

X_train_full, X_test, y_train_full, y_test = train_test_split(
    housing.data, housing.target)
X_train, X_valid, y_train, y_valid = train_test_split(
    X_train_full, y_train_full
)

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_valid = scaler.transform(X_valid)
X_test = scaler.transform(X_test)
```

- 이 데이터셋에는 잡음이 많기 때문에 과대적합을 막는 용도로 뉴런 수가 적은 은닉층 하나만 사용한다.

```python
from tensorflow.keras.layers import Dense
from tensorflow.keras.models import Sequential

model = Sequential([
    Dense(30, activation='relu', input_shape=X_train.shape[1:]),
    Dense(1) # 하나의 값을 예측하기 때문에 1 (활성화 함수 X)
])
model.compile(loss='mse', optimizer='sgd')
history = model.fit(X_train, y_train, epochs=2,
                    validation_data=(X_valid, y_valid))
mse_test = model.evaluate(X_test, y_test)
X_new = X_test[:3]
y_pred = model.predict(X_new)
```

- 시퀀셜 API를 사용해 회귀용 MLP를 구축, 훈련, 평가, 예측하는 방법은 분류에서 했던 것과 비슷하지만 
  1. 출력층이 활성화 함수가 없는 하나의 뉴런(하나의 값을 예측하기 때문)을 가진다는 것과
  2. 손실 함수로 평균 제곱 오차를 사용한다는 점이 주된 차이점이다.

<br>

### 케라스 Sequential 클래스 활용의 장단점

- 사용하기 매우 쉬우며 성능 우수하다.
- 입출력이 여러 개이거나 더 복잡한 네트워크를 구성하기 어려움
- Sequential 클래스 대신에 함수형 API, 하위클래스(subclassing) API 등을 사용하여 보다 복잡하며, 보다 강력한 딥러닝 모델을 구축 가능하다.

## 10.2.4 함수형 API를 사용해 복잡한 모델 만들기

- **와이드 & 딥** (Wide & Deep) 신경망
  - 신경망이 (깊게 쌓은 층을 사용한) 복잡한 패턴과 (짧은 경로를 사용한) 간단한 규칙을 모두 학습할 수 있다.

<img src='https://user-images.githubusercontent.com/78655692/147923150-82adc84b-c493-4c66-99c8-902a8250b41c.png' width=400> 이미지 출처 [^7] <br>

```python
from tensorflow.keras.layers import Input, Dense, Concatenate
from tensorflow.keras.models import Model

input_ = Input(shape=X_train.shape[1:]) # 이 객체는 shape과 dtype을 포함하여 모델의 입력을 정의한다.
hidden1 = Dense(30, activation='relu')(input_) # 30개의 뉴런과 ReLU 활성화 함수를 가진 Dense 층을 만든다. 이 층은 만들어지자마자 입력과 함께 함수처럼 호출된다.
# 이 메서드에는 build() 메서드를 호출하여 층의 가중치를 생성한다.
hidden2 = Dense(30, activation='relu')(hidden1) 
concat = Concatenate()([input_, hidden2]) # Concatenate 층을 만들고 또 다시 함수처럼 호출하여 두 번째 은닉층의 출력과 입력을 연결한다.
output = Dense(1)(concat) # 하나의 뉴런과 활성화 함수가 없는 출력층을 만들고 Concatenate 층이 만든 결과를 사용해 호출한다.
model = Model(inputs=[input_], outputs=[output]) # 사용할 입력과 출력을 지정하여 케라스 Model을 만든다. 
```
<br>

<img src='https://user-images.githubusercontent.com/78655692/147924063-21fd1cc2-0e97-4ec2-aad4-dfe0f2fb27d3.png' width='350'> 이미지 출처 [^8] <br>

```python
from tensorflow.keras.layers import Input, Dense, concatenate
from tensorflow.keras.models import Model

input_A = Input(shape=[5], name="wide_input") # 5개의 특성을 입력받을 수 있다.
input_B = Input(shape=[6], name="deep_input") # 6개의 특성을 입력받을 수 있다.
hidden1 = Dense(30, activation="relu")(input_B)
hidden2 = Dense(30, activation="relu")(hidden1)
concat = concatenate([input_A, hidden2])
output = Dense(1, name='output')(concat)
model = Model(inputs=[input_A, input_B], outputs=[output])
```

<br>

- 모델 컴파일은 이전과 동일하지만 fit() 메서드를 호출할 때 하나의 입력 행렬 X_train을 전달하는 것이 아니라 입력마다 하나씩 행렬의 튜플 (X_train_A, X_train_B)을 전달해야 한다.

```python
from tensorflow.keras.optimizers import SGD

model.compile(loss='mse', optimizer=SGD(lr=1e-3))

X_train_A, X_train_B = X_train[:, :5], X_train[:, 2:] # input_A 입력값 : 0~4번 인덱스 특성, input_B 입력값 : 2~7번 인덱스 특성
X_valid_A, X_valid_B = X_valid[:, :5], X_valid[:, 2:]
X_test_A, X_test_B = X_test_A[:3], X_test_B[:3]
X_new_A, X_new_B = X_test_A[:3], X_test_B[:3]

history = model.fit((X_train_A, X_train_B), y_train, epochs=20,
                    validation_data=((X_valid_A, X_valid_B), y_valid))
mse_test = model.evaluate((X_test_A, X_test_B), y_test)
y_pred = model.predict((X_new_A, X_new_B))
```

<br>

- 다중 출력이 필요할 때
- **보조 출력**을 사용해 하위 네트워크가 나머지 네트워크에 의존하지 않고 그 자체로 유용한 것을 학습하는지 확인할 수 있다.

<img src='https://user-images.githubusercontent.com/78655692/147926849-7e148d99-37b7-4642-b49a-1d288def3ee9.png' width='350'> 이미지 출처 [^9] <br> 

```python
from tensorflow.keras.layers import Input, Dense, concatenate
from tensorflow.keras.models import Model

input_A = Input(shape=[5], name="wide_input")
input_B = Input(shape=[6], name="deep_input")
hidden1 = Dense(30, activation="relu")(input_B)
hidden2 = Dense(30, activation="relu")(hidden1)
concat = concatenate([input_A, hidden2])
output = Dense(1, name="main_output")(concat) # 주 출력
aux_output = Dense(1, name="aux_output")(hidden2) # 보조 출력
model = Model(inputs=[input_A, input_B],
              outputs=[output, aux_output])
```

<br>

- 각 출력은 자신만의 손실 함수가 필요하다.
- 모델을 컴파일할 때 손실의 리스트를 전달해야 한다.
- 케라스는 나열된 손실을 모두 더하여 최종 손실을 구해 훈련에 사용한다.
- 출력 별로 **손실가중치**를 지정하여 출력별 중요도 지정 가능

```python
model.compile(loss=['mse', 'mse'], loss_weights=[0.9, 0.1], optimizer='sgd')
```

<br>

- 모델을 훈련할 때 각 출력에 대한 레이블을 제공해야 한다.
- 여기에서는 주 출력과 보조 출력이 같은 것을 예측해야 하므로 동일한 레이블을 사용한다.

```python
history = model.fit(
    [X_train_A, X_train_B], [y_train, y_train], epochs=20,
    validation_data=([X_valid_A, X_valid_B], [y_valid, y_valid])
)
```

<br>

- 모델을 평가하면 케라스는 개별 손실과 함께 총 손실을 반환한다.

```python
total_loss, main_loss, aux_loss = model.evaluate(
    [X_test_A, X_test_B], [y_test, y_test]
)
```

<br>

- `predict()` 메서드는 각 출력에 대한 예측을 반환한다.

```python
y_pred_main, y_pred_aux = model.predict([X_new_A, X_new_B])
```

<br>

## 10.2.5 서브클래싱 API로 동적 모델 만들기

- 시퀀셜 API와 함수형 API는 모두 선언적 (declarative)이다. (=정적)
  - **사용할 층과 연결 방식을 먼저 정의하고 모델에 데이터를 주입하여 훈련이나 추론을 시작할 수 있다.**
  - 장점
    1. 모델을 저장하거나 복사, 공유하기 쉽다.
    2. 모델의 구조를 출력하거나 분석하기 좋다.
    3. 프레임워크가 크기를 짐작하고 타입을 학인하여 에러를 일찍 발견할 수 있다.
    4. 전체 모델이 층으로 구성된 정적 그래프이므로 디버깅하기 쉽다.
- 하지만 어떤 모델은 반복문을 포함하고 다양한 크기를 다루어야 하며 조건문을 가지는 등 여러 가지 동적인 구조를 필요로 한다.
  - **서브클래싱 API**를 쓴다!!

<br>

- `Model` 클래스를 상속한 다음 다음 생성자 안에서 필요한 층을 만든다.
- 초기설정 메서드(`__init__()`)를 이용하여 은닉층과 출력층 설정
- `call()` 메서드 안에 수행하려는 연산을 기술한다.

```python
import tensorflow as tf

class WideAndDeepModel(tf.keras.Model):
    def __init__(self, units=30, activation='relu', **kwargs):
        super().__init__(**kwargs) # 표준 매개변수 처리
        self.hidden1 = tf.keras.layers.Dense(units, activation=activation)
        self.hidden2 = tf.keras.layers.Dense(units, activation=activation)
        self.main_output = tf.keras.layers.Dense(1)
        self.aux_output = tf.keras.layers.Dense(1)
    
    def call(self, inputs):
        input_A, input_B = inputs
        hidden1 = self.hidden1(input_B)
        hidden2 = self.hidden2(hidden1)
        concat = tf.keras.layers.concatenate([input_A, hidden2])
        main_output = self.main_output(concat)
        aux_output = self.aux_output(hidden2)
        return main_output, aux_output

model = WideAndDeepModel()
```

<br>

- 위 코드는 Input 클래스의 객체를 만들 필요가 없다. 대신 `call()` 메서드의 input 매개변수를 사용한다.
- **단점**
  1. 모델 구조가 `call()` 메서드 안에 숨겨져 있기 때문에 케라스가 쉽게 이를 분석할 수 없다. (모델을 저장하거나 복사 X)
  2. `summary()` 메서드를 호출하면 층의 목록만 나열되고 층 간의 연결 정보를 얻을 수 없다.
  3. 케라스가 타입과 크기를 미리 확인할 수 없다.

<br>

## 10.2.6 모델 저장과 복원

- 모델 저장 형태 & 로드 

```python
model = Sequential([...])
model.compile([...])
model.fit([...])
model.save('my_keras_model.h5') # 저장

model = load_model('my_keras_model.h5') # 로드
```

> save_weights()와 load_weights() 메서드를 사용하여 모델 파라미터를 저장하고 복원할 수 있다. 그 외에는 모두 수동으로 저장하고 복원해야 한다.

<br>

## 10.2.7. 콜백 사용하기

- fit() 메서드의 `callbakcs` 매개변수를 사용하여 케라스가 훈련의 시작이나 끝에 호출할 객체 리스트를 지정할 수 있다. 또는 에포크의 시작이나 끝, 각 배치 처리 전후에 호출할 수도 있다.
- `ModelCheckpoint`는 훈련하는 동안 일정한 간격으로 모델의 체크포인트를 저장한다.

```python
import tensorflow as tf
from tensorflow.keras.callbacks import ModelCheckpoint

checkpoint_cb = ModelCheckpoint('my_keras_model.h5')
history = model.fit(X_train, y_train, epochs=10, callbacks=[checkpoint_cb])
```

- `ModelCheckpoint`에서 `save_best_only=True`로 지정하면 최상의 검증 세트 점수에서만 모델을 저장한다.

```python
import tensorflow as tf
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.models import load_model

checkpoint_cb = ModelCheckpoint('my_keras_model.h5',
                                save_best_only=True)
history = model.fit(X_train, y_train, epochs=10, 
                    validation_data=(X_valid, y_valid),
                    callbacks=[checkpoint_cb])
model = load_model("my_keras_model.h5")
```

- `EarlyStopping` 콜백은 일정 에포크 동안 검증 세트에 대한 점수가 향상되지 않으면 훈련을 멈춘다.
- 모델이 향상되지 않으면 훈련이 **자동으로 중지되기 때문에** 에포크의 숫자를 크게 지정해도 된다.
- `restore_best_weights=True` : 최상의 모델 복원 기능 설정
  - 훈련이 끝난 후 최적 가중치 바로 복원

```python
from tensorflow.keras.callbacks import EarlyStopping

early_stopping_cb = EarlyStopping(patience=10,
                                  restore_best_weights=True)
history = model.fit(X_train, y_train, epochs=100,
                    validation_data=(X_valid, y_valid),
                    callbacks=[checkpoint_cb, early_stopping_cb])
```

<br>

## 10.3.1 은닉층 개수

- **심층신경망**은 복잡한 함수를 모델링하는 데 얕은 신경망보다 훨씬 적은 수의 뉴런을 사용하므로 동일한 양의 훈련 데이터에서 더 높은 성능을 낼 수 있다.
- 하나 또는 두 개의 은닉층만으로도 많은 문제를 꽤 잘 해결할 수 있다.

<br>

## 10.3.2 은닉층의 뉴런 개수

- 네트워크가 과대적합이 시작되기 전까지 점진적으로 뉴런 수를 늘릴 수 있다.
- 과대적합되지 않도록 조기 종료나 규제 기법을 사용


<br>
<br>

## References

[^1]: 조선비즈,2016.03.09 <https://www.crinity.net/Newsletter/2016/03/IT_contents1.html>
[^2]: [[인공신경망과 서포트벡터머신] 강민형 교수 (신경망과 서포트벡터머신의 이해)](https://www.youtube.com/watch?v=cSdbs3x4YBw)
[^3]: <https://brunch.co.kr/@gdhan/6>
[^4]: <https://excelsior-cjh.tistory.com/172>
[^5]: [kyu9341.github.io](https://kyu9341.github.io/MachineLearning/2019/11/23/marchinelearning3/)
[^6]: <https://excelsior-cjh.tistory.com/172>
[^7]: <https://formal.hknu.ac.kr/handson-ml2/slides/handson-ml2-10.slides.html#/79>
[^8]: <https://formal.hknu.ac.kr/handson-ml2/slides/handson-ml2-10.slides.html#/82>
[^9]: <https://formal.hknu.ac.kr/handson-ml2/slides/handson-ml2-10.slides.html#/86>