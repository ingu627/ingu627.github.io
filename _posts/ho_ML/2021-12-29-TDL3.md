---
layout: single
title: "[핸즈온 머신러닝] 12장. 텐서플로를 사용한 사용자 정의 모델과 훈련"
excerpt: "part 2 신경망과 딥러닝 부분을 개인공부를 목적으로 내용 요약 및 정리한 글입니다. - 텐서플로를 사용한 사용자 정의 모델과 훈련"
categories: hands_on
tag : [python, tensorflow, DL, 텐서플로, 딥러닝, 심층 신경망]
toc: true
sidebar_main: true
classes: wide

last_modified_at: 2021-12-29
---

<img align='right' width='200' height='200' src='https://user-images.githubusercontent.com/78655692/147628941-a1aeb296-324e-4a60-816b-e4cc6666d13e.png
'>
본 글은 [핸즈온 머신러닝 2판 (Hands-On Machine Learning with Scikit-Learn, keras & TensorFlow)] - (박해선 옮김) 책을 개인공부하기 위해 요약, 정리한 내용입니다. <br>전체 코드는 <https://github.com/ingu627/handson-ml2>에 기재했습니다.(원본 코드 fork) <br> 뿐만 아니라 책에서 설명이 부족하거나 이해가 안 되는 것들은 외부 자료들을 토대로 메꿔 놓았습니다. <br> 오타나 오류는 알려주시길 바라며, 도움이 되길 바랍니다.
{: .notice--info}

<br>
<br>
<br>
<br>

## 12.1 텐서플로 훑어보기

- **텐서플로**는 강력한 수치 계산용 라이브러리이다. 특히 대규모 머신러닝에 잘 맞도록 튜닝되어 있다.
  - 핵심 구조는 넘파이와 매우 비슷하지만 GPU를 지원한다.
  - (여러 장치와 서버에 대해서) 분산 컴퓨팅을 지원한다. 
  - 일종의 JIT 컴파일러를 포함한다. 속도를 높이고 메모리 사용량을 줄이기 위해 계산을 최적화한다. 이를 위해 파이썬 함수에서 계산 그래프를 추출한 다음 최적화하고 효율적으로 실행한다. 
  - 계산 그래프는 플랫폼에 중립적인 포맷으로 내보낼 수 있다.
  - 텐서플로는 자동 미분 기능과 RMSProp, Nadam 같은 고성능 옵티마이저를 제공하므로 모든 종류의 손실 함수를 최소화할 수 있다.

![image](https://user-images.githubusercontent.com/78655692/147628860-2c554246-f350-4f40-aa42-6ae149cebb0c.png) [^1]

- 가장 저수준의 텐서플로 연산은 매우 효율적인 C++ 코드로 구현되어 있다. 
- 많은 연산은 **커널**이라 부르는 여러 구현을 가진다.
- 각 커널은 CPU, GPU 또는 TPU와 같은 특정 장치에 맞추어 만들어졌다.

<br>

## 12.2 넘파이처럼 텐서플로 사용하기

- 텐서는 일반적으로 다차원 배열

<br>

### 12.2.1 텐서와 연산

- `tf.constant()` 함수로 텐서를 만들 수 있다.

```python
import tensorflow as tf

tf.constant([[1.,2.,3.], [4.,5.,6.]]) # 행렬
# <tf.Tensor: shape=(2, 3), dtype=float32, numpy=
# array([[1., 2., 3.],
#        [4., 5., 6.]], dtype=float32)>
tf.constant(42) # 스칼라
# <tf.Tensor: shape=(), dtype=int32, numpy=42>
```

- `tf.Tensor`는 크기와 데이터 타입을 가진다.

```python
t = tf.constant([[1.,2.,3.], [4.,5.,6.]])
t.shape # TensorShape([2, 3])
t.dtype # tf.float32
```

- 인덱스 참조도 넘파이와 매우 비슷하게 작동

```python
t[:, 1:]
# <tf.Tensor: shape=(2, 2), dtype=float32, numpy=
# array([[2., 3.],
#        [5., 6.]], dtype=float32)>

t[..., 1, tf.newaxis]
# <tf.Tensor: shape=(2, 1), dtype=float32, numpy=
# array([[2.],
#        [5.]], dtype=float32)>
```

- 모든 종류의 텐서 연산이 가능하다.
- `t+10`이라고 쓰는 것은 `tf.add(t,10)`을 호출하는 것과 같다.

```python
t + 10
# <tf.Tensor: shape=(2, 3), dtype=float32, numpy=
# array([[11., 12., 13.],
#        [14., 15., 16.]], dtype=float32)>

tf.square(t) # 제곱
# <tf.Tensor: shape=(2, 3), dtype=float32, numpy=
# array([[ 1.,  4.,  9.],
#        [16., 25., 36.]], dtype=float32)>

t @ tf.transpose(t) # transpose는 행렬 변환
# <tf.Tensor: shape=(2, 2), dtype=float32, numpy=
# array([[14., 32.],
#        [32., 77.]], dtype=float32)>
```

<br>

### 12.2. 텐서와 넘파이

- 넘파이 배열에 텐서플로 연산을 적용할 수 있고 텐서에 넘파이 연산을 적용할 수도 있다.
- 넘파이는 기본으로 64비트 정밀도를 사용하지만 텐서플로는 32비트 정밀도를 사용한다.
  - 넘파이 배열로 텐서를 만들 때 `dtype=tf.float32`로 지정해야 한다.

```python
import numpy as np

a = np.array([2., 4., 5.])
tf.constant(a)
# <tf.Tensor: shape=(3,), dtype=float64, numpy=array([2., 4., 5.])>
np.array(t)
# array([[1., 2., 3.],
#        [4., 5., 6.]], dtype=float32)
tf.square(a)
# <tf.Tensor: shape=(3,), dtype=float64, numpy=array([ 4., 16., 25.])>
np.square(t)
# array([[ 1.,  4.,  9.],
#        [16., 25., 36.]], dtype=float32)
```

<br>

### 12.2.3 타입 변환

- 텐서플로는 어떤 타입 변환도 자동으로 수행하지 않는다. 
- 호한되지 않는 타입의 텐서로 연산을 실행하면 예외가 발생한다.
- 타입 변환이 필요할 때는 `tf.cast()` 함수를 사용한다.

```python
t2 = tf.constant(40., dtype=tf.float64)
tf.constant(2.0) + tf.cast(t2, tf.float32)
# <tf.Tensor: shape=(), dtype=float32, numpy=42.0>
```

<br>

### 12.2.4 변수

- `tf.Variable`는 텐서의 내용을 바꿀 수 있다. 변수의 값을 증가시키거나 원소의 값을 바꾸면 새로운 텐서가 만들어진다.

```python
v = tf.Variable([[1.,2.,3.], [4.,5.,6.]])
v
# <tf.Variable 'Variable:0' shape=(2, 3) dtype=float32, numpy=
# array([[1., 2., 3.],
#        [4., 5., 6.]], dtype=float32)>
```

- `assign()` 메서드를 사용하여 변숫값을 바꿀 수 있다.
  - `assign_add()`나 `assign_sub() 메서드를 사용하면 주어진 값만큼 변수를 증가시키거나 감소시킬 수 있다.
- 원소의 `assign()` 메서드나 `scatter_update()`, `scatter_nd_update()` 메서드를 사용하여 개별 원소를 수정할 수 있다.

```python
v.assign(2 * v)
# <tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=
# array([[ 2.,  4.,  6.],
#        [ 8., 10., 12.]], dtype=float32)>

v[0,1].assign(42)
# <tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=
# array([[ 2., 42.,  6.],
#        [ 8., 10., 12.]], dtype=float32)>

v[:,2].assign([0., 1.])
# <tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=
# array([[ 2., 42.,  0.],
#        [ 8., 10.,  1.]], dtype=float32)>

v.scatter_nd_update(indices=[[0,0], [1,2]], updates=[100., 200.])
# <tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=
# array([[100.,  42.,   0.],
#        [  8.,  10., 200.]], dtype=float32)>
```

<br>

## 12.3 사용자 정의 모델과 훈련 알고리즘

<br>

### 12.3.1 사용자 정의 손실 함수

- 후버 손실 정의하기

```python
def huber_fn(y_true, y_pred):
    error = y_true - y_pred
    is_small_error = tf.abs(error) < 1
    squared_loss = tf.square(error) / 2
    linear_loss = tf.abs(error) - 0.5
    return tf.where(is_small_error, squared_loss, linear_loss)
```

- 이 손실을 사용해 케라스 모델의 컴파일 메서드를 호출하고 모델을 훈련할 수 있다.

```python
model.compile(loss=huber_fn, optimizer='nadam')
model.fit(X_train, y_train, [...])
```

- 훈련하는 동안 배치마다 케라스는 `huber_fn()` 함수를 호출하여 손실을 계산하고 이를 사용해 경사 하강법을 수행한다. 또한 에포크 시작부터 전체 손실을 기록하여 평균 손실을 출력한다.

<br>

### 12.3.2 사용자 정의 요소를 가진 모델을 저장하고 로드하기

- 모델을 로드할 때는 함수 이름과 실제 함수를 매핑한 딕셔너리를 전달해야 한다.
- 사용자 정의 객체를 포함한 모델을 로드할 때는 그 이름과 객체를 매핑해야 한다.

```python
from tensorflow.keras.models import load_model

model = load_model("my_model_with_a_custom_loss.h5",
                   custom_objects={"huber_fn": huber_fn})
```

- 매개변수를 받을 수 있는 함수 만들기

```python
def create_huber(threshold=1.0):
    def huber_fn(y_true, y_pred):
        error = y_true - y_pred
        is_small_error = tf.abs(error) < threshold
        squared_loss = tf.square(error) / 2
        linear_loss = threshold * tf.abs(error) - threshold**2 / 2
        return tf.where(is_small_error, squared_loss, linear_loss)
    return huber_fn

model.compile(loss=create_huber(2.0), optimizer="nadam")
```

- 모델을 로드할 때 threshold 값을 지정해야 한다.

```python
model = load_model("my_model_with_a_custom_loss_threshold_2.h5",
                   custom_objects={"huber_fn": create_huber(2.0)})
```

- 이 문제는 `keras.losses.Loss` 클래스를 상속하고 `get_config()` 메서드를 구현하여 해결할 수 있다.

```python
from tensorflow.keras.losses import Loss

class HuberLoss(Loss):
    def __init__(self, threshold=1.0, **kwargs):
        self.threshold = threshold
        super().__init__(**kwargs)
    def call(self, y_true, y_pred):
        error = y_true - y_pred
        is_small_error = tf.abs(error) < self.threshold
        squared_loss = tf.square(error) / 2
        linear_loss = self.threshold * tf.abs(error) - self.threshold**2 / 2
        return tf.where(is_small_error, squared_loss, linear_loss)
    def get_config(self):
        base_config = super().get_config()
        return {**base_config, "threshold": self.threshold}
```

- 모델을 컴파일 할 때 이 클래스의 인스턴스를 사용할 수 있다.

```python
model.compile(loss=HuberLoss(2.), optimizer="nadam")
```

- 이 모델을 저장할 때 임곗값도 함께 저장된다. 모델을 로드할 때 클래스 이름과 클래스 자체를 매핑해주어야 한다.

```python
model = load_model("my_model_with_a_custom_loss_class.h5",
                   custom_objects={"HuberLoss": HuberLoss})
```

<br>

### 12.3.3 활성화 함수, 초기화, 규제, 제한을 커스터마이징하기

- `사용자 정의 활성화 함수 (keras.activations.softplus())`

```python
def my_softplus(z):
    return tf.math.log(tf.exp(z) + 1.0)
```

- `사용자 정의 글로럿 초기화 (keras.initializers.glorot_normal())`

```python
def my_glorot_initializer(shape, dtype=tf.float32):
    stddev = tf.sqrt(2. / (shape[0] + shape[1]))
    return tf.random.normal(shape, stddev=stddev, dtype=dtype)
```

- `사용자 정의 $l_1$ 규제 (keras.regularizers.l1(0.01))`

```python
def my_l1_regularizer(weights):
    return tf.reduce_sum(tf.abs(0.01 * weights))
```

- `양수인 가중치만 남기는 사용자 정의 제한 (keras.constraints.nonneg())`

```python
def my_positive_weights(weights):
    return tf.where(weights < 0., tf.zeros_like(weights), weights)
```



<br>
<br>

## References

[^1]: <https://dschloe.github.io/python/python_edu/07_deeplearning/chapter_7_3_1_tensorflow_basic/>