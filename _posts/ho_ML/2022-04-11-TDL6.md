---
layout: single
title: "[핸즈온 머신러닝] 14장. 합성곱 신경망을 사용한 컴퓨터 비전 (2)"
excerpt: "part 2 신경망과 딥러닝 부분을 개인공부를 목적으로 내용 요약 및 정리한 글입니다. - 합성곱 신경망을 사용한 컴퓨터 비전"
categories: hands_on
tag : [파이썬, 텐서플로, 딥러닝, 합성곱, 필터, 풀링, 스트라이드, cnn, 케라스, ResNet-34, 사전훈련된 모델, 구현]
toc: true
sidebar_main: true
classes: wide

last_modified_at: 2022-04-11
---

<img align='right' width='200' height='200' src='https://user-images.githubusercontent.com/78655692/147628941-a1aeb296-324e-4a60-816b-e4cc6666d13e.png
'>
본 글은 [핸즈온 머신러닝 2판 (Hands-On Machine Learning with Scikit-Learn, keras & TensorFlow)] - (박해선 옮김) 책을 개인공부하기 위해 요약, 정리한 내용입니다. <br>전체 코드는 <https://github.com/ingu627/handson-ml2>에 기재했습니다.(원본 코드 fork) <br> 뿐만 아니라 책에서 설명이 부족하거나 이해가 안 되는 것들은 외부 자료들을 토대로 메꿔 놓았습니다. <br> 오타나 오류는 알려주시길 바라며, 도움이 되길 바랍니다. 
{: .notice--info}

**more info** <br><br> 1. 이미지 자료 : [formal.hknu.ac.kr](https://formal.hknu.ac.kr/handson-ml2/slides/handson-ml2-14-1.slides.html#/) <br> 2. [딥러닝을 위한 콘볼루션 계산 가이드](https://goo.gl/qvNTyu)
{: .notice--warning}


<br>
<br>
<br>
<br>

## 14.4 케라스를 사용해 ResNet-34 CNN 구현하기

- 먼저 **ResidualUnit** 층 만들기 (그림 참고)

![image](https://user-images.githubusercontent.com/78655692/160429318-66098142-d3c6-4078-8610-1c686dc2e42e.png)

```python
import tensorflow as tf

class ResidualUnit(tf.keras.layers.Layer):
    def __init__(self, filters, strides=1, activation='relu', **kwargs):
        super().__init__(**kwargs)
        self.activation = tf.keras.activations.get(activation)
        self.main_layers = [
            tf.keras.layers.Conv2D(filters, 3, strides=strides,
                                   padding='same', use_bias=False),
            tf.keras.layers.BatchNormalization(),
            self.activation,
            tf.keras.layers.Conv2D(filters, 3, strides=1,
                                   padding='same', use_bias=False),
            tf.keras.layers.BatchNormalization()
            ]
        self.skip_layers = []
        if strides > 1:
            self.skip_layers = [
                tf.keras.layers.Conv2D(filters, 1, strides=strides,
                                       padding='same', use_bias=False),
                tf.keras.layers.BatchNormalization()
            ]
    
    def call(self, inputs):
        Z = inputs
        for layer in self.main_layers:
            Z = layer(Z)
        skip_Z = inputs
        for layer in self.skip_layers:
            skip_Z = layer(skip_Z)
        return self.activation(Z + skip_Z)
```

<br>

- 이 네트워크는 연속되어 길게 연결된 층이기 때문에 Sequential 클래스를 사용해 ResNet-34 모델을 만들 수 있다.

<br>

```python
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Conv2D(64, 7, strides=2, input_shape=[224, 224, 3],
                                 padding='same', use_bias=False))
model.add(tf.keras.layers.BatchNormalization())
model.add(tf.keras.layers.Activation('relu'))
model.add(tf.keras.layers.MaxPool2D(pool_size=3, strides=2, padding='same'))
prev_filters = 64
for filters in [64] * 3 + [128] * 4 + [256] * 6 +[512] * 3:
    strides = 1 if filters == prev_filters else 2
    model.add(ResidualUnit(filters, strides=strides))
    prev_filters = filters
model.add(tf.keras.layers.GlobalAvgPool2D())
model.add(tf.keras.layers.Flatten())
model.add(tf.keras.layers.Dense(10, activation='softmax'))
```

- `model.summary()`

<img src='https://user-images.githubusercontent.com/78655692/162778564-01de3a8d-1ed2-486b-a564-88ee08614ab1.png' width=450>



<br>
<br>

## 14.6 케라스에서 제공하는 사전훈련된 모델 사용하기

- `keras.applications` 패키지에서 사전훈련된 모델을 가져올 수 있다.
- ex)

```python
model= tf.keras.applications.resnet50.ResNet50(weights='imagenet')
# ResNet-50 모델을 만들고 이미지넷 데이터셋에서 사전훈련된 가중치를 다운로드
```

<br>

```python
images_resized = tf.image.resize(images, [224, 224])
# 적재한 이미지의 크기를 바꿈

inputs = tf.keras.applications.resnet50.preprocess_input(images_resized * 255)
# preprocess_input() : 모델마다 이미지를 전처리 해줌

Y_proba = model.predict(inputs)
# 예측
```

<br>
<br>

## 14.7 사전훈련된 모델을 사용한 전이 학습

```python
import tensorflow_datasets as tfds

dataset, info = tfds.load('tf_flowers', as_supervised=True, with_info=True)
# with_info=True : 데이터셋에 대한 정보 얻기
dataset_size = info.splits['train'].num_examples
class_names = info.features['label'].names
n_classes = info.features['label'].num_classes
```

<br>

```python
test_set_raw, valid_set_raw, train_set_raw = tfds.load(
    "tf_flowers",
    split=["train[:10%]", "train[10%:25%]", "train[25%:]"],
    as_supervised=True)
# 모델 나누기
```

<br>

```python
def preprocess(image, label):
    resized_image = tf.image.resize(image, [224, 224]) # 크기 조정
    final_image = tf.keras.applications.xception.preprocess_input(resized_image) # 이미지 전처리
    return final_image, label
```

<br>

```python
batch_size = 32
train_set_raw = train_set_raw.shuffle(1000)

# 배치 크기 정함 & 프리패츠 적용
train_set = train_set_raw.map(preprocess).batch(batch_size).prefetch(1)
valid_set = valid_set_raw.map(preprocess).batch(batch_size).prefetch(1)
test_set = test_set_raw.map(preprocess).batch(batch_size).prefetch(1)
```

<br>

```python
base_model = tf.keras.applications.xception.Xception(weights='imagenet',
                                                     include_top=False)
# include_top=False : 네트워크의 최상층에 해당하는 전역 평균 풀링 층과 밀집 출력 층을 제외                                                    
avg = tf.keras.layers.GlobalAveragePooling2D()(base_model.output)
output = tf.keras.layers.Dense(n_classes, activation='softmax')(avg)
model = tf.keras.Model(inputs=base_model.input, outputs=output)
```

<br>


```python
# 사전훈련된 층의 가중치 동결
for layer in base_model.layers:
    layer.trainable = False
```

<br>

```python
# 모델을 컴파일하고 훈련 시작
optimizer = tf.keras.optimizers.SGD(lr=0.2, momentum=0.9, decay=0.01)
model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer,
              metrics=['accuracy'])
history = model.fit(train_set, epochs=5, validation_data=valid_set)
```

<br>

```python
# 모델을 몇 번의 에포크 동안 훈련하는 것은 새로 추가한 최상위 층을 훈련했다는 의미.add()# 다시 모든 층의 동결을 해제하고 훈련을 계속한다.

for layer in base_model.layers:
    layer.trainable = True

optimizer = tf.keras.optimizers.SGD(lr=0.01, momentum=0.9, decay=0.001)
model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer,
              metrics=['accuracy'])
history = model.fit(train_set, epochs=10, validation_data=valid_set)
```



<br>
<br>
<br>
<br>





















