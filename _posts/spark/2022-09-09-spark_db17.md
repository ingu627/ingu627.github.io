---
layout: single
title: "[Spark] Dataset와 관련 기능 정리"
excerpt: "Spark The Definitive Guide 책을 중심으로 스파크를 요약 및 정리해보았습니다. spark 예제를 통해 dataset에 대해 알아봅니다."
categories: spark
tag : [스파크, spark, sql, 스칼라, scala, 정리, 의미, 란, 이란, dataset, action, transformation, filter, map, case class, joinWith]
toc: true
toc_sticky: true
sidebar_main: true

last_modified_at: 2022-09-16
---

<img align='right' width='150' height='200' src='https://user-images.githubusercontent.com/78655692/186088421-fe905f9e-d40f-43b2-ac6e-094e9c342473.png'>
[Spark The Definitive Guide - BIG DATA PROCESSING MADE SIMPLE] 책을 중심으로 스파크를 개인 공부를 위해 요약 및 정리해보았습니다. <br> 다소 복잡한 설치 과정은 도커에 미리 이미지를 업로해 놓았습니다. 즉, 도커 이미지를 pull하면 바로 스파크를 사용할 수 있습니다. <br><br> 도커 설치 및 활용하는 방법 : [[Spark] 빅데이터와 아파치 스파크란 - 1.2 스파크 실행하기](https://ingu627.github.io/spark/spark_db1/#12-%EC%8A%A4%ED%8C%8C%ED%81%AC-%EC%8B%A4%ED%96%89%ED%95%98%EA%B8%B0) <br> 도커 이미지 링크 : [https://hub.docker.com/r/ingu627/hadoop](https://hub.docker.com/r/ingu627/hadoop) <br> 예제를 위한 데이터 링크 : [FVBros/Spark-The-Definitive-Guide](https://github.com/ingu627/BigData/tree/master/spark/data) <br> 예제에 대한 실행 언어는 SQL과 스칼라(scala)로 했습니다.
{: .notice--info}
**기본 실행 방법** <br> 1. 예제에 사용 될 데이터들은 도커 이미지 생성 후 `spark-3.3.0` 안의 하위 폴더 `data`를 생성 후, 이 폴더에 추가합니다. <br> 1.1 데이터와 도커 설치 및 활용하는 방법은 위에 링크를 남겼습니다. <br> 2. 프로그램 시작은 `cd spark-3.3.0` 후, `./bin/spark-shell` 명령어를 실행하시면 됩니다.

<br>
<br>

## 1. Dataset

- Dataset은 구조적 API의 기본 데이터 타입이다.
  - DataFrame은 Row 타입의 Dataset이다.
  - Dataset은 스칼라와 자바에서만 사용할 수 있다.
- Dataset을 사용해 데이터셋의 각 로우를 구성하는 객체를 정의한다.
- 스칼라에서는 스키마가 정의된 케이스 클래스 객체를 사용해 Dataset을 정의한다.

<br>

- 스파크는 `StringType`, `BigIntType`, `StructType`과 같은 다양한 데이터 타입을 제공한다.
- 또한 스파크가 지원하는 다양한 언어의 `String`, `Integer`, 그리고 `Double`과 같은 데이터 타입을 스파크의 특정 데이터 타입으로 매핑할 수 있다.
- 스칼라나 자바를 사용할 때 모든 DataFrame은 Row 타입의 Dataset을 의미하는데, 도메인별 특정 객체를 효과적으로 지원하기 위해 **인코더(encoder)**라 부르는 특수 개념이 필요하다.
  - **인코더(encoder)**는 도베인별 특정 객체 T를 스파크의 내부 데이터 타입으로 매핑하는 시스템을 의미한다.

<br>

- Dataset API를 사용한다면 스파크는 데이터셋에 접근할 때마다 Row 포맷이 아닌 사용자 정의 데이터 타입으로 변환한다.
  - 하지만 사용자 정의 데이터 타입을 사용하면 성능이 나빠지게 된다.

<br>
<br>

## 2. Dataset을 사용할 시기

- 하지만 그럼에도 불구하고 Dataset을 사용해야 하는 이유가 있다.

  1. DataFrame 기능만으로는 수행할 연산을 표현할 수 없는 경우
  2. 성능 저하를 감수하더라도 타입 안정성(type-safe)을 가진 데이터 타입을 사용하고 싶은 경우

- 단일 노드의 워크로드와 스파크 워크로드에서 전체 로우에 대한 다양한 트랜스포메이션을 **재사용**하려면 Dataset을 사용하는 것이 적합하다.
- 케이스 클래스로 구현된 데이터 타입을 사용해 모든 데이터와 트랜스포메이션을 정의하면 재사용할 수 있다.

<br>
<br>

## 3. Dataset 생성

- Dataset을 생성하는 것은 수동 작업이므로 정의할 스키마를 미리 알고 있어야 한다.
- 스칼라에서 Dataset을 생성하려면 스칼라 case class 구문을 사용해 데이터 타입을 정의해야 한다.
- 케이스 클래스(case class)는 다음과 같은 특징을 가진 정규 클래스(regular class)이다.
  - 불변성(Immutable)
    - 객체들이 언제 어디서 변경되었는지 추적할 필요가 없다.
  - 패턴 매칭으로 분해 가능
    - 패턴 매칭은 로직 분기를 단순화해 버그를 줄이고 가독성을 좋게 만든다.
  - 참조값 대신 클래스 구조를 기반으로 비교
  - 사용하기 쉽고 다루기 편함

```scala
case class Flight(DEST_COUNTRY_NAME: String,
                  ORIGIN_COUNTRY_NAME: String,
                  count: BigInt)

val flightsDF = spark.read.parquet("./data/flight-data/parquet/2010-summary.parquet/")
val flights = flightsDF.as[Flight]
```

<br>
<br>

## 4. 액션

- Dataset과 DataFrame에 collect, take, count 같은 액션을 적용할 수 있다.

```scala
flights.show(2)
```

- **실행결과**

  ![image](https://user-images.githubusercontent.com/78655692/190842215-dbbbec20-6a88-453f-a468-e765d6107f41.png)

<br>

## 5. 트랜스포메이션

- DataFrame의 모든 트랜스포메이션은 Dataset에서 사용할 수 있다.

### 5.1 필터링

- Flight 클래스를 파라미터로 사용해 불리언값을 반환하는 함수를 만들어본다.
  - 스칼라에서 함수는 다음과 같이 정의한다. [^1]

    ```scala
    def 함수명:타입 = 표현식

    def 함수명(매개변수:타입) = 표현식
    ```

```scala
def originIsDestination(flight_row: Flight): Boolean = {
  return flight_row.ORIGIN_COUNTRY_NAME == flight_row.DEST_COUNTRY_NAME
}
```

<br>

- 위에서 정의한 함수를 filter 메서드에 적용해 각 행이 true를 반환하는지 평가하고 데이터셋을 필터링할 수 있다.
  - `=>`을 기준으로 왼쪽에는 매개변수 목록이고 오른쪽은 매개변수를 포함한 표현식이다. [^2]

```scala
flights.filter(flight_row => originIsDestination(flight_row)).first()
```

- **실행결과**

  ![image](https://user-images.githubusercontent.com/78655692/190842706-db117bf3-1071-4115-8191-679ec26dca58.png)

<br>

### 5.2 매핑

- 필터링은 단순한 트랜스포메이션이지만 때로는 특정 값을 다른 값으로 **매핑(mapping)**해야 한다. 
- 다음은 목적지 컬럼을 추출하는 예제이다.

```scala
val destinations = flights.map(f => f.DEST_COUNTRY_NAME)
```

<br>

- 드라이버는 결괏값을 모아 문자열 타입의 배열로 반환한다.

```scala
val localDestinations = destinations.take(5)
```

- **실행결과**

  ![image](https://user-images.githubusercontent.com/78655692/190842837-4a9bb147-d77c-4a4d-a271-c3c5f5094042.png)

<br>
<br>

## 6. 조인

- 조인은 DataFrame에서와 마찬가지로 Dataset에도 동일하게 적용된다.
- 하지만 Dataset은 `joinWith`처럼 정교한 메서드를 제공한다.
- **joinWith** 메서드는 co-group과 거의 유사하며 Dataset 안쪽에 다른 2개의 중첩된 Dataset으로 구성된다.
- 각 컬럼은 단일 Dataset이므로 Dataset 객체를 컬럼처럼 다룰 수 있다.
- 그러므로 조인 수행 시 더 많은 정보를 유지할 수 있으며 고급 맵이나 필터처럼 정교하게 데이터를 다룰 수 있다.

```scala
case class FlightMetadata(count: BigInt, randomData: BigInt)

// 스칼라에서 _ 의미는 모든 것을 포함하는 의미이다. 
val flightsMeta = spark.range(500
  ).map(x => (x, scala.util.Random.nextLong)
  ).withColumnRenamed("_1", "count"
  ).withColumnRenamed("_2", "randomData"
  ).as[FlightMetadata]

val flights2 = flights.joinWith(
  flightsMeta, flights.col("count") === flightsMeta.col("count")
)
```

- **실행결과**

  ![image](https://user-images.githubusercontent.com/78655692/190843430-6a90e421-2a92-4fe1-91cb-b214afe49532.png)


<br>

- 최종적으로 로우는 Fight와 FlightMetadata로 이루어진 키값 형태의 Dataset을 반환한다.
- Dataset이나 복합 데이터 타입의 DataFrame으로 데이터를 조회할 수 있다.

```scala
flights2.selectExpr("_1.DEST_COUNTRY_NAME")
```

<br>

- 드라이버로 데이터를 모은 다음 결과를 반환한다.

```scala
flights2.take(2)
```

- **실행결과**

  ![image](https://user-images.githubusercontent.com/78655692/190843461-bbf224df-9258-45c5-bed0-da5cd022f00d.png)

<br>

- 일반 조인 역시 잘 동작한다.

```scala
val flights2 = flights.join(flightsMeta, Seq("count"))
```

<br>
<br>

## 7. 그룹화와 집계

- 그룹화와 집계는 동일한 기본 표준을 따른다.
- 하지만 Dataset 대신 DataFrame을 반환하기 때문에 데이터 타입 정보를 잃게 된다.

```scala
flights.groupBy("DEST_COUNTRY_NAME").count()
```

<br>

- 데이터 타입 정보를 유지할 수 있는 그룹화와 집계 방법이 있다.
- 한 가지 예로 groupByKey 메서드는 Dataset의 특정 키를 기준으로 그룹화하고 형식화된 Dataset을 반환한다.
  - 하지만 이 함수는 컬럼명 대신 함수를 파라미터로 사용해야 한다.

```scala
flights.groupByKey(x => x.DEST_COUNTRY_NAME).count()
```

- **실행결과(이와 비교)**

  ![image](https://user-images.githubusercontent.com/78655692/190843635-0c375b8b-f205-4bd4-a6c7-62fcb57d1a04.png)

<br>

- Dataset의 키를 이용해 그룹화를 수행한 다음 결과를 키-값 형태로 함수에 전달해 원시 객체 형태로 그룹화된 데이터를 다룰 수 있다.

```scala
def grpSum(countryName:String, values: Iterator[Flight]) = {
  values.dropWhile(_.count < 5).map(x => (countryName, x))
}
flights.groupByKey(x => x.DEST_COUNTRY_NAME).flatMapGroups(grpSum).show(5)
```

- **실행결과**

  ![image](https://user-images.githubusercontent.com/78655692/190844112-2f1564ca-176b-43f0-ac84-395c89952978.png)

<br>

```scala
def grpSum2(f:Flight):Integer = {
  1
}
flights.groupByKey(x => x.DEST_COUNTRY_NAME).mapValues(grpSum2).count().take(5)
```

- **실행결과**

  ![image](https://user-images.githubusercontent.com/78655692/190844014-a2f8a710-fcbc-4cbe-8d07-b030c9f21789.png)

<br>

- 다음 예제처럼 새로운 처리 방법을 생성해 그룹을 축소(reduce)하는 방법을 정의할 수 있다.

```scala
def sum2(left:Flight, right:Flight) = {
  Flight(left.DEST_COUNTRY_NAME, null, left.count + right.count)
}
flights.groupByKey(x => x.DEST_COUNTRY_NAME).reduceGroups((l, r) => sum2(l, r)).take(5)
```

- **실행결과**

  ![image](https://user-images.githubusercontent.com/78655692/190844222-ab28fab1-bd3f-47a1-b40d-bdb09436e321.png)


<br>
<br>
<br>
<br>

## References

[^1]: [[스칼라] 함수 - 기초 - Sean Ma](https://sean-ma.tistory.com/35)
[^2]: [[Scala] 스칼라 시작하기 스칼라 구조 및 특징 / 표현식, 메소드, 함수, 클래스, 트레이스, 싱글턴 오브젝트](https://wonyong-jang.github.io/scala/2021/02/24/Scala.html)