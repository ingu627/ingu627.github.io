---
layout: single
title: '검색 증강 생성(RAG)의 진화: 아키텍처와 기술에 대한 포괄적 분석'
excerpt: "검색 증강 생성(Retrieval-Augmented Generation, RAG)은 생성형 언어 모델의 능력을 외부의 검증 가능한 지식 소스와 연결하여 강화하는 아키텍처 패턴이다. 이 접근법은 전적으로 사전 훈련된 데이터에만 의존하는 모델의 내재적 한계를 해결하기 위해 고안되었다."
categories: llm
tags : [llm, rag, 의미 검색, self-rag, graphrag, nlp, advanced-rag, modular-rag, 아키텍처, 정리, agentic, 청킹, 파싱]
toc: true
toc_sticky: true
sidebar_main: true

date: 2025-09-20
last_modified_at: 2025-09-20
---

<br>
<br>

검색 증강 생성(Retrieval-Augmented Generation, RAG)은 생성형 언어 모델의 능력을 외부의 검증 가능한 지식 소스와 연결하여 강화하는 아키텍처 패턴이다. <br> 
이 접근법은 전적으로 사전 훈련된 데이터에만 의존하는 모델의 내재적 한계를 해결하기 위해 고안되었다.
{: .notice--info}

<br>

## RAG의 정의: 매개변수적 지식과 비매개변수적 지식의 결합

- RAG의 핵심 아이디어는 두 가지 서로 다른 형태의 지식을 결합하는 데 있다.  [^1]
  - LLM의 매개변수적 메모리는 수십억 개의 신경망 가중치에 암묵적으로 인코딩된 패턴과 지식을 의미한다. 이는 마치 인간의 직관적 지식처럼 즉각적이고 유연하지만, 업데이트가 어렵고 때로는 부정확할 수 있다. 
  - 반면 비매개변수적 메모리는 데이터베이스나 문서 컬렉션처럼 명시적으로 저장되고 검색 가능한 지식을 나타낸다. 이는 정확하고 추적 가능하지만 단독으로는 유연한 추론이나 창의적 생성이 어렵다.
- RAG의 혁신은 이 두 가지를 상호 보완적으로 결합한 점에 있다.
  - 외부 검색을 통해 최신성과 정확성을 확보하면서도, LLM의 언어 이해와 추론 능력을 활용해 자연스럽고 맥락적인 응답을 생성한다. 
  - 이는 단순한 미세 조정보다 비용 효율적이며, 실시간 정보 업데이트가 가능하고, 출처 추적으로 투명성을 제공한다.
  - 특히 의료, 법률, 금융처럼 정확성이 중요한 도메인에서 RAG의 가치가 극대화된다.

<br>

## RAG 이전 시대: 정보 검색의 진화 궤적

- RAG의 개념적 뿌리를 이해하려면 정보 검색의 역사를 살펴봐야 한다. 
- 1970년대부터 시작된 정보 검색 연구는 TF-IDF, 벡터 공간 모델, 그리고 후에 등장한 BM25 같은 희소 검색 방법론을 발전시켰다. 
- 이런 전통적 방법들은 키워드 기반으로 작동해 정확한 용어 매칭에는 강했지만, 의미적 유사성이나 동의어 처리에는 한계가 있었다.

<br>

- 2000년대에 들어서면서 검색과 생성을 통합하려는 초기 시도들이 나타났다.
  - IBM Watson의 DeepQA 시스템은 Jeopardy! 게임에서 인간을 능가하며 주목받았지만, 이는 여전히 검색과 추론이 분리된 파이프라인 구조였다. 
  - 진정한 변화는 2018년 BERT와 트랜스포머의 등장으로 시작됐다. 
  - 이를 통해 밀집 검색(dense retrieval) 방법론이 발달하기 시작했고, 텍스트의 의미적 표현을 벡터 공간에서 효과적으로 포착할 수 있게 됐다.

<br>

- DrQA나 REALM 같은 초기 신경망 기반 QA 시스템들은 RAG의 직접적 전조였다.
  - 특히 REALM(Retrieval-Augmented Language Model Pre-training)은 사전 학습 단계부터 검색 기능을 통합해, 마스크된 언어 모델링을 학습 신호로 활용하며 검색기를 비지도 방식으로 훈련했다. 
  - 이는 검색과 생성이 완전히 통합된 종단간(end-to-end) 학습의 가능성을 보여줬다.

<br>

## 전환점: 2020년 Lewis 등의 RAG 논문 심층 분석

- 2020년 NeurIPS에 발표된 Patrick Lewis와 Facebook AI Research 팀의 논문은 RAG를 공식적으로 정립한 결정적 순간이었다. [^2]
- 이 논문의 제목 "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks"에서 알 수 있듯이, RAG를 특정 작업에 국한된 기술이 아닌 "범용 미세 조정 레시피"로 제시했다. [^3]
- 핵심 아키텍처는 DPR(Dense Passage Retriever)과 BART(Bidirectional and Auto-Regressive Transformers)의 결합이었다. [^3]
- **DPR의 혁신적 접근** 
  - DPR은 BERT 기반의 이중 인코더(bi-encoder) 구조를 사용해 쿼리와 문서를 별도로 인코딩한다. 
  - 쿼리 인코더와 문서 인코더가 공유된 밀집 벡터 공간에 각각의 표현을 매핑하고, 최대 내적 검색(MIPS)으로 가장 관련성 높은 문서를 찾는다. 
  - 이 방식은 기존 희소 검색의 어휘적 갭(lexical gap) 문제를 해결하면서도 대규모 코퍼스에서 효율적인 검색을 가능하게 했다.
- **BART의 생성 능력**
  - 4억 개 매개변수를 가진 BART-large가 생성기 역할을 담당했다. 
  - 검색된 문서와 원본 쿼리를 단순 연결(concatenation)하는 방식으로 입력을 구성하고, 이를 바탕으로 최종 출력 시퀀스를 생성한다. 
  - 이때 중요한 것은 검색된 문서를 단순히 참고 자료로 사용하는 것이 아니라, 생성 과정에 직접적으로 통합한다는 점이다.

<br>

- **종단간 학습의 핵심 혁신**
  - 가장 중요한 기여는 검색기와 생성기를 함께 최적화할 수 있는 종단간 학습 프레임워크였다. 
  - 검색된 문서를 잠재 변수(latent variable)로 취급함으로써, 생성기의 손실에서 검색기의 쿼리 인코더로 그래디언트를 역전파할 수 있었다. 
  - 이는 검색기가 단순히 의미적으로 유사한 문서를 찾는 것을 넘어, 실제로 생성 작업에 유용한 문서를 찾도록 최적화되게 만들었다.
- 즉, '관련성'의 정의가 정적인 유사도가 아니라 다운스트림 작업의 성능을 기준으로 동적으로 학습된다는 혁신적 개념을 제시했다.

<br>

## RAG-Sequence vs RAG-Token: 생성 전략의 분기점

- 이 논문에서 제시한 두 가지 변형 모델은 이후 RAG 발전의 중요한 설계 선택지가 됐다.
- **RAG-Sequence 모델**  
  - 초기 쿼리를 바탕으로 상위 K개 문서를 검색한 후, 동일한 단일 문서 집합을 사용해 전체 출력 시퀀스를 생성한다.  
  - 최종 출력 확률은 상위 K개 문서에 대해 주변화(marginalization)하여 계산한다.  
  - 구현이 단순하고 안정적이지만, 하나의 답변에서 여러 출처의 정보를 활용하기 어렵다는 한계가 있다.[^1]
- **RAG-Token 모델**  
  - 더 유연한 접근으로, 생성하는 각 토큰마다 다른 검색 문서를 참조할 수 있다.  
  - 이를 통해 단일 답변 내에서 여러 출처의 정보를 종합하여 더 구체적이고 사실에 기반한 응답을 생성할 수 있다.  
  - 이는 훗날 Advanced RAG에서 볼 수 있는 복잡한 융합(fusion) 기술과 다중 홉(multi-hop) 추론의 직접적 전조가 되었다.[^1]

- 이 두 모델의 제시는 RAG 초기부터 "얼마나 많은 문서를 사용할 것인가?"와 "어떻게 다중 문서 정보를 통합할 것인가?"가 핵심 설계 결정임을 보여준다.  

<br>

## 표준 RAG 파이프라인: Naive RAG의 구조와 한계

### 3단계 워크플로우의 상세 분석

- 학술적 기원에서 벗어나 실제 구현에서는 RAG가 보다 직선적이고 단순화된 "Naive RAG" 아키텍처로 발전했다.  
- 이는 고급 최적화나 피드백 루프 없이 색인(Indexing) → 검색(Retrieval) → 생성(Generation)의 3단계 파이프라인을 따른다.

<img width="800" height="260" alt="image" src="https://gist.github.com/user-attachments/assets/a49253fb-e3d9-47a0-b196-bcce183ae29a" />

- **색인 단계: 지식 베이스 구축**  
  - 외부 지식 소스(PDF, 웹 문서, 데이터베이스 등)를 수집해 전처리한다.  
  - 가장 중요한 과정 중 하나인 청킹(chunking)을 통해 긴 문서를 관리 가능한 작은 조각으로 분할한다.  
  - 임베딩 모델(Sentence-BERT, OpenAI ada-002 등)을 사용해 각 청크를 고차원 벡터로 변환한다.  
  - 이 벡터들을 전문 벡터 데이터베이스(Milvus, FAISS, Chroma, Pinecone 등)에 저장하고 색인화한다.[^5]

<br>

- **검색 단계: 관련 정보 탐색**  
  - 사용자 쿼리를 동일한 임베딩 모델로 벡터화한다.  
  - 벡터 데이터베이스에서 근사 최근접 이웃(ANN) 알고리즘을 사용해 쿼리와 가장 유사한 상위 K개 청크를 찾는다.  
  - 일반적으로 코사인 유사도나 유클리드 거리를 사용해 유사성을 측정한다.

<br>

- **생성 단계: 최종 응답 합성**  
  - 검색된 청크들과 원본 쿼리를 결합해 증강된 프롬프트를 구성한다.  
  - 현대적 구현에서는 GPT-4, Claude, Llama 같은 강력한 LLM이 이 증강된 컨텍스트를 바탕으로 최종 응답을 생성한다.  
  - LLM은 검색된 정보를 종합하고 해석해 일관성 있고 자연스러운 답변을 만들어낸다.

<br>

### 현대적 구현: 모듈화의 장점과 한계

- 원본 RAG 논문의 종단간 미세 조정과 달리, 현대 RAG 시스템은 높은 모듈성을 특징으로 한다.  
- 2022년 이후 GPT-3.5 같은 거대 사전 훈련 모델이 API를 통해 널리 접근 가능해지면서, 미세 조정 없이도 강력한 인컨텍스트 학습 능력을 활용할 수 있게 됐다.  
- 동시에 벡터 데이터베이스 생태계가 폭발적으로 성장해 기성품으로 고도로 최적화된 검색 솔루션을 제공하게 됐다.  
- 이로 인해 개발자들은 "동급 최고" 구성 요소들을 플러그 앤 플레이 방식으로 조합할 수 있게 됐다.[^1]

<br>

- **모듈화의 장점**  
  - 구축 비용과 복잡성이 대폭 줄어들었다  
  - 각 구성 요소를 독립적으로 업그레이드하거나 교체할 수 있다  
  - 다양한 도메인에 빠르게 적용할 수 있다.
- **모듈화의 한계**  
  - 구성 요소 간 인터페이스에서 새로운 과제가 발생한다  
  - 전체 시스템 최적화가 어려워진다  
  - 각 단계의 오류가 누적되는 문제가 생긴다.[^1]

<br>

### Naive RAG의 핵심 한계들

- Naive RAG의 선형적 구조는 여러 근본적 한계를 드러낸다.
- **검색 품질 문제**  
  - 낮은 정밀도: 관련 없는 노이즈 청크들이 검색되어 LLM을 혼란시킨다  
  - 낮은 재현율: 실제 답변에 필요한 모든 정보를 검색하지 못한다  
  - 차선의 청킹: 고정 크기 분할로 의미적 경계가 무시되어 문맥이 분리된다  
  - 쿼리-문서 불일치: 짧은 사용자 쿼리와 긴 문서 청크 간의 의미적 갭이 존재한다.[^5]
- **생성 품질 문제**  
  - 환각과 부정확성: 노이즈 많은 컨텍스트에서 LLM이 잘못된 정보를 생성한다  
  - 컨텍스트 통합 실패: 여러 청크의 정보를 효과적으로 종합하지 못해 불완전한 답변을 만든다  
  - 형식 오류: 요구되는 응답 형식이나 상세 수준을 맞추지 못한다.
- **시스템 수준 한계**  
  - 경직성: 모든 쿼리에 동일한 처리 방식을 적용해 유연성이 부족하다  
  - 조용한 실패: 시스템은 항상 그럴듯한 답변을 생성하지만 미묘하게 틀릴 수 있어 오류 감지가 어렵다  
  - 병목 효과: 각 단계의 품질이 전체 성능의 상한선을 결정한다.

<br>

## RAG의 진화: 3세대 아키텍처 발전사

### Advanced RAG: 품질 정제 중심의 진화

<img width="800" height="353" alt="image" src="https://gist.github.com/user-attachments/assets/c90433ff-b94b-4e61-ab9b-a2bb2a2c79fb" />

- Naive RAG의 한계를 인식한 연구자들과 실무자들은 파이프라인 각 단계에 정교한 처리 계층을 추가하는 Advanced RAG를 발전시켰다.  
- 핵심 아이디어는 검색된 원시 문서를 LLM에 그대로 전달하는 대신, 여러 단계의 정제와 개선을 통해 정보 품질을 높이는 것이다.  
- 이는 전통적인 계산 아키텍처의 "정밀도 vs 재현율" 트레이드오프를 다단계 깔때기로 해결하는 접근이다.[^1]
- Advanced RAG는 검색 전(Pre-retrieval), 검색(Retrieval), 검색 후(Post-retrieval) 3단계로 나뉜다.

<br>

## Advanced RAG 기술 심층 분석

### 검색 전(Pre-retrieval) 향상: 입력 최적화의 고도화

- 검색 전 단계의 개선은 "쓰레기 입력, 쓰레기 출력" 원칙을 방지하는 핵심이다.  

<br>

- **의미론적 청킹(Semantic Chunking)**  
  - 기존 고정 크기 청킹의 한계를 극복하기 위해 의미 기반 분할을 수행한다.  
  - 구체적 과정: 문서를 문장 단위로 분할 → 각 문장의 임베딩 생성 → 연속된 문장들의 코사인 유사도 계산 → 유사도가 임계값 이하로 떨어지는 지점에서 청크 분리.  
  - 이를 통해 주제적으로 일관되고 독립적인 청크를 생성해 검색 정확도를 향상시킨다.  
  - 단점은 계산 비용이 높고 임베딩 모델의 품질에 의존한다는 점이다.[^6][^5]

<br>

- **다중 쿼리 검색(Multi-Query Retrieval)**  
  - 사용자의 단일 쿼리로는 모든 관련 정보를 포착하기 어렵다는 한계를 해결한다.  
  - LLM을 사용해 원본 쿼리의 여러 변형을 다양한 관점에서 생성한다.  
  - 예시: "AI의 발전"이라는 쿼리를 "인공지능 기술 혁신", "머신러닝 최신 동향", "딥러닝 응용 분야"로 확장.  
  - 모든 변형 쿼리에 대해 검색을 수행하고 결과의 고유 합집합을 취해 더 풍부한 문서 집합을 확보한다.[^5]

<br>

- **HyDE (Hypothetical Document Embeddings)**  
  - 역발상적이지만 효과적인 접근 방식이다.  
  - 사용자 쿼리를 직접 임베딩하는 대신, LLM이 해당 쿼리에 대한 상세한 가상 답변을 먼저 생성한다.  
  - 이 가상 문서를 임베딩해서 실제 문서를 검색한다.  
  - 근거: "답변끼리는 질문보다 서로 더 유사하다"는 직관에 기반한다.  
  - 특히 짧고 모호한 쿼리에서 효과가 크다.[^5]

<br>

- **쿼리 분해(Query Decomposition)**  
  - 복잡한 다면적 질문을 여러 단순 질문으로 분해한다.  
  - 예시: "녹차와 홍차의 건강상 이점 비교"를 "녹차의 건강상 이점"과 "홍차의 건강상 이점"으로 분해.  
  - 각 하위 질문에 대해 별도 검색을 수행한 후 결과를 통합한다.  
  - 이는 환각을 줄이고 더 종합적인 답변을 가능하게 한다.

<br>

### 검색(Retrieval) 향상: 하이브리드 접근의 정교함

- **밀집 vs 희소 검색의 철학적 대립과 통합**  
  - 밀집 검색(Dense Retrieval): 의미적 유사성을 포착하고 동의어나 의역도 잘 처리하지만, 특정 키워드나 고유명사에서 약하다.  
  - 희소 검색(Sparse Retrieval, BM25): 정확한 키워드 매칭에 강하고 전문 용어 처리에 효과적이지만, 어휘적 변형을 못 다룬다.  
  - 하이브리드 검색은 이 두 방식의 강점을 결합한다.

<br>

- **RRF(Reciprocal Rank Fusion)의 수학적 우아함**  
  - 여러 검색 결과를 통합하는 효과적이고 간단한 알고리즘이다.  
  - 각 문서의 점수를 계산: score = 1/(k  rank), 여기서 k는 상수(보통 60).  
  - 모든 검색 시스템에서 얻은 점수를 합산해 최종 순위를 결정한다.  
  - 다른 시스템의 점수를 정규화할 필요 없어 구현이 간단하면서도 효과적이다.

<br>

### 검색 후(Post-retrieval) 향상: 정밀도 극대화 전략

- **교차 인코더 재정렬의 아키텍처적 우월성**  
  - 초기 검색은 재현율 중심으로 광범위하게 수행한다 (상위 20-50개).  
  - 교차 인코더는 쿼리와 문서를 함께 단일 입력으로 처리해 완전한 어텐션을 수행한다.  
  - 이중 인코더가 놓치는 미세한 상호작용과 관련성 신호를 포착한다.  
  - 0과 1 사이의 정밀한 관련성 점수를 출력해 훨씬 정확한 순위를 제공한다.  
  - 계산 비용이 높지만 소수 후보에 대해서는 충분히 실용적이다.

<br>

- **컨텍스트 압축과 품질 관리**  
  - 검색된 청크에서 중복되거나 관련 없는 정보를 제거한다.  
  - 메타데이터 기반 필터링으로 신뢰할 수 있는 출처만 선별한다.  
  - 시간 기반 필터링으로 최신 정보를 우선시한다.  
  - 이는 LLM의 "Lost in the Middle" 문제를 완화하는 데 중요하다.

<br>

### 정리하면 다음과 같다.

| 파이프라인 단계 | 기술                | 설명                                             | 해결하는 문제                                     |
| -------------- | ------------------- | ------------------------------------------------ | -------------------------------------------------- |
| 검색 전        | 의미론적 청킹       | 길이에 상관없이 의미에 따라 텍스트 분할          | 부실한 청킹으로 인한 비일관적이거나 관련 없는 컨텍스트 |
| 검색 전        | 다중 쿼리 검색기     | LLM을 사용하여 쿼리 변형 생성                    | 쿼리-문서 불일치, 단일 관점의 실패                   |
| 검색 전        | HyDE                | 쿼리 대신 가상 답변을 임베딩                     | 컨텍스트가 부족한 짧거나 모호한 쿼리                 |
| 검색           | 하이브리드 검색      | 키워드(BM25)와 의미론적(벡터) 검색 결합           | 한 가지 검색 유형의 실패(예: 전문 용어에 대한 키워드 누락) |
| 검색 후        | 교차 인코더 재정렬   | 더 강력한 모델로 초기 결과 재정렬                 | 상위 K개 결과의 낮은 정밀도, 노이즈가 많은 컨텍스트   |
| 검색 후        | 컨텍스트 압축        | 최종 컨텍스트에서 중복 정보 제거                  | LLM 컨텍스트 창 초과, "중간 분실" 문제                |

<br>

### Modular RAG: 아키텍처의 근본적 재설계

<img width="800" height="437" alt="image" src="https://gist.github.com/user-attachments/assets/b392e5c1-6c34-48b8-99c1-772534af161e" />

- Modular RAG는 한 걸음 더 나아가 RAG의 선형적 구조 자체를 해체한다.  
- 전통적인 파이프라인을 독립적이고 상호 교환 가능한 모듈들로 분해한다  
- 라우팅 로직을 통해 쿼리의 성격에 따라 동적으로 워크플로우를 조정한다  
- 이는 소프트웨어 아키텍처에서 모놀리식에서 마이크로서비스로의 전환과 유사하다.[^3][^1]
- **핵심 특징**  
  - 구성 요소의 독립성: 각 모듈이 별도로 개발, 테스트, 배포될 수 있다  
  - 동적 오케스트레이션: 쿼리 복잡성에 따라 필요한 모듈만 선택적으로 실행한다  
  - 확장성: 새로운 모듈 추가나 기존 모듈 교체가 용이하다.[^3]

<br>

## RAG 최전선: Agentic 아키텍처의 혁신

- Agentic RAG는 정적 파이프라인을 넘어 자율적 의사결정 능력을 가진 동적 시스템을 구현한다.[^7][^3]

### Self-RAG: 자기 성찰하는 지능형 시스템

<img width="800" height="332" alt="image" src="https://gist.github.com/user-attachments/assets/87d2a7ae-f277-42b3-b67f-e36f87d57abe" />

- Self-RAG는 단일 LLM에 메타인지 능력을 부여한 혁신적 접근이다.  
  - 모델이 특수한 "반성 토큰(reflection tokens)"을 생성해 자신의 검색과 생성 과정을 제어한다.  
  - 핵심 토큰들: `<Retrieve>` (검색 필요성), `<IsRel>` (문서 관련성), `<IsSup>` (답변 근거성), `<IsUse>` (응답 유용성).  
  - 각 생성 단계에서 검색 필요성을 자가 평가하고, 필요할 때만 외부 정보를 가져온다.  
  - 생성된 내용이 검색 문서에 의해 지원되는지 지속적으로 검증한다.[^7][^3]
- **Self-RAG의 학습 메커니즘**  
  - 반성 토큰을 포함한 훈련 데이터로 모델을 미세 조정한다.  
  - 추론 시에는 이 토큰들이 전체 워크플로우를 조율하는 제어 신호가 된다.  
  - 결과적으로 적응적이고 효율적인 검색-생성 프로세스가 가능해진다.[^3]

<br>

### Corrective-RAG (CRAG): 자가 교정 메커니즘

<img width="785" height="279" alt="image" src="https://gist.github.com/user-attachments/assets/60f28568-a421-48ab-8d3d-3655ae19e73e" />

- CRAG는 검색 품질을 명시적으로 평가하고 필요시 수정 조치를 취하는 플러그 앤 플레이 시스템이다.  
  - 경량 검색 평가기가 검색 결과를 '정확함', '부정확함', '모호함'으로 분류한다.  
  - 각 분류에 따른 차별화된 처리 전략을 적용한다.[^7][^3]

<br>

- **CRAG의 처리 전략**  
  - 정확함: 노이즈 제거를 위한 "분해-재구성" 알고리즘 적용.  
  - 부정확함: 검색 문서 폐기 후 웹 검색 등 대안 소스 활용.  
  - 모호함: 정제된 내부 문서와 외부 검색 결과를 결합.  
  - 이런 다층적 접근으로 검색 실패에 대한 견고성을 크게 높인다.[^3]

<br>

### GraphRAG: 구조화된 지식의 힘

- GraphRAG는 비정형 텍스트를 지식 그래프로 변환해 더 정교한 추론을 가능하게 한다.[^8][^9]
- **GraphRAG의 핵심 아키텍처**  
  - 문서에서 엔티티와 관계를 추출해 그래프 구조를 구축한다.  
  - 커뮤니티 감지 알고리즘으로 관련 엔티티들을 클러스터링한다.  
  - 각 커뮤니티에 대한 요약을 생성해 계층적 정보 구조를 만든다.  
  - 검색 시에는 단순한 텍스트 청크가 아닌 의미적으로 연결된 서브그래프를 반환한다.[^9][^8]

<br>

- **GraphRAG의 차별화된 강점**  
  - 전역 질의: "이 문서 집합의 주요 테마는 무엇인가?" 같은 광범위한 질문에 탁월하다.  
  - 다단계 추론: 여러 엔티티 간의 관계를 통한 복잡한 추론이 가능하다.  
  - 설명 가능성: 추론 경로가 그래프 구조로 명시적으로 드러난다.  
  - Microsoft의 대규모 실험에서 전통적 RAG 대비 현저한 성능 향상을 입증했다.[^8]

<br>

### FLARE: 생성 중 능동적 정보 탐색

- **FLARE(Forward-Looking Active REtrieval)**는 인간이 글을 쓸 때 정보를 찾는 방식을 모방한다.  
  - 장문 생성 과정에서 모델이 확신하지 못하는 내용을 감지하면 생성을 일시 중단한다.  
  - 다음에 올 내용을 예측하고, 이를 바탕으로 쿼리를 구성해 필요한 정보를 검색한다.  
  - 검색된 정보로 컨텍스트를 보강한 후 더 확신 있는 생성을 재개한다.  
  - 이는 특히 긴 보고서나 분석 글 작성에서 인간의 연구 과정과 유사한 패턴을 보인다.[^3]

<br>

## RAG의 주요 도전과제와 해결 전략

### 근본적 기술 한계들

- **지식 베이스 품질의 한계**  
  - 불완전성: 필요한 정보가 코퍼스에 존재하지 않으면 시스템이 환각을 일으키거나 부정확한 답변을 생성한다.  
  - 해결책: 프롬프트 엔지니어링을 통해 모델이 "모르겠습니다"라고 답변하도록 유도하고, 다중 소스 통합으로 정보 공백을 메운다.  
  - 시의성 문제: 정적 데이터베이스는 빠르게 변하는 정보를 반영하지 못한다.  
  - 해결책: 실시간 웹 검색 통합, 주기적 데이터 업데이트, 시간 기반 가중치 적용.[^1]

<br>

- **검색의 정밀도-재현율 딜레마**  
  - 정밀도 문제: 의미적으로 유사하지만 문맥적으로 관련 없는 정보가 검색되어 노이즈가 증가한다.  
  - 재현율 문제: 복잡한 질문에 필요한 모든 정보 조각을 찾지 못한다.  
  - 해결책: 다단계 검색 전략, 교차 인코더 재정렬, 쿼리 분해와 다중 검색의 조합 활용.[^5]

<br>

### RAG 진화의 핵심 패턴 종합

- RAG의 발전 과정을 되돌아보면 명확한 진화 패턴을 볼 수 있다.  
- **1세대 (Naive RAG)**: 기본 연결성 입증 - "LLM과 외부 지식을 연결할 수 있는가?"  
- **2세대 (Advanced RAG)**: 품질 관리 - "어떻게 더 정확하고 관련성 있는 정보를 제공할 것인가?"  
- **3세대 (Modular/Agentic RAG)**: 지능적 워크플로우 - "시스템이 스스로 판단하고 적응할 수 있는가?".[^3][^1]

<br>

- 각 세대는 이전 세대의 한계를 해결하면서도 새로운 과제를 제시했다.  
- Naive RAG는 구현의 단순성을 보여줬지만 품질 한계를 드러냈다.  
- Advanced RAG는 품질을 높였지만 복잡성과 계산 비용을 증가시켰다.  
- Agentic RAG는 자율성을 제공하지만 예측가능성과 디버깅의 어려움을 가져왔다.[^7][^3]

<br>
<br>

## References

[^1]: [A Comprehensive Survey of Retrieval-Augmented Generation (RAG): Evolution, Current Landscape and Future Directions](https://arxiv.org/abs/2410.12837)
[^2]: [Weights & Biases: RAG Techniques](https://wandb.ai/site/articles/rag-techniques/)
[^3]: [Agentic Retrieval-Augmented Generation: A Survey on Agentic RAG](https://arxiv.org/html/2501.09136v1)
[^4]: [Moonlight: Comprehensive Survey of RAG](https://www.themoonlight.io/en/review/a-comprehensive-survey-of-retrieval-augmented-generation-rag-evolution-current-landscape-and-future-directions)
[^5]: [Meilisearch Blog: RAG Techniques](https://www.meilisearch.com/blog/rag-techniques)
[^6]: [Weaviate Blog: Advanced RAG](https://weaviate.io/blog/advanced-rag)
[^7]: [AIMultiple: Agentic RAG](https://research.aimultiple.com/agentic-rag/)
[^8]: [Neo4j Blog: GraphRAG and Agentic Architecture](https://neo4j.com/blog/developer/graphrag-and-agentic-architecture-with-neoconverse/)
[^9]: [arXiv: 2501.13958v1 (HTML)](https://arxiv.org/html/2501.13958v1)

