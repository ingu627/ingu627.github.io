---
layout: single
title: 'RAG 성능을 좌우하는 핵심, 청킹(Chunking) 완벽 가이드: 기초부터 고급 전략까지'
excerpt: "청킹(Chunking)이란 AI가 방대한 문서의 내용을 정확하고 효율적으로 검색할 수 있도록 의미 있는 작은 단위로 분할하는 핵심 과정이다."
categories: [llm]
tags: [llm, rag, 청킹, chunking, embedding, 검색 증강 생성, 텍스트 분할, LangChain, 벡터 검색]
toc: true
toc_sticky: true
sidebar_main: true

date: 2025-09-21
last_modified_at: 2025-09-21
---

## 서론: RAG 시스템의 성패는 청킹에 달려있다

- RAG(Retrieval-Augmented Generation, 검색 증강 생성)를 비유하자면, LLM(거대 언어 모델)이라는 똑똑한 학생이 시험을 치르는 것과 같다. RAG가 없다면 이 시험은 '암기 과목 시험(closed-book exam)'이다. LLM은 오직 훈련 데이터로 외운 지식에만 의존해야 한다. 하지만 RAG를 도입하면 시험은 '오픈북 시험(open-book exam)'으로 바뀐다. LLM은 우리가 제공한 외부 문서라는 '참고서'를 실시간으로 참조하며 답변을 생성할 수 있게 된다. [^2]
- 여기서 **청킹(Chunking)**은 바로 그 참고서를 얼마나 효율적으로 정리하느냐의 문제다. 방대한 참고서의 내용을 의미 있는 단락, 문장, 아이디어 단위로 잘게 나누어 깔끔한 '요약 노트'로 만드는 과정이다. 만약 이 요약 노트가 뒤죽박죽 섞여 있거나, 문장 중간에 잘려 있다면 아무리 똑똑한 학생이라도 좋은 성적을 내기 어렵다.
- 많은 개발자들이 청킹을 단순 전처리로 보지만, 실제로는 RAG 성능을 결정짓는 핵심 기반이다. 잘못된 청킹은 검색 단계에서 엉뚱한 정보를 가져오게 만들고, 최종적으로 LLM이 잘못된 답변이나 환각(Hallucination)을 생성하게 만든다. 따라서 청킹은 검색 알고리즘의 논리를 정의하는 첫 번째 단계이며, 문서를 어떻게 나누느냐가 검색이 찾을 수 있는 '정답의 단위'를 결정한다. 
- 이 글에서는 청킹의 기본 원리부터 고급 전략까지, 실제 코드 예제와 함께 상세히 다룬다. [^6]

<br>

## 파트 1: 청킹의 기본 원리 — 왜 나누고, 무엇을 조절해야 하는가

- 청킹 전략을 제대로 이해하려면, 먼저 문서를 잘게 나눠야 하는 근본 제약과 우리가 제어할 수 있는 핵심 변수들을 알아야 한다.

### 1.1 청킹을 강제하는 세 가지 핵심 제약

- **LLM의 컨텍스트 창 (Context Window)**
  - LLM의 컨텍스트 창은 모델이 한 번에 처리할 수 있는 정보의 양, 즉 '작업 기억(working memory)' 또는 '주의력의 한계'와 같다. 최근 대형 모델들이 거대한 컨텍스트 창을 지원하더라도, 긴 컨텍스트를 무작정 넣으면 'lost in the middle' 현상으로 중간 정보가 소실될 수 있다. 청킹은 질문과 관련성이 높은 정보만 밀도 있게 제공하기 위한 필수 과정이다. [^10]

- **검색의 정확성: 방대한 자료에서 바늘 찾기**
  - RAG 검색은 질문을 임베딩 벡터로 바꾸고, 미리 임베딩된 청크 중 가장 유사한 것을 찾는 방식이다. 청크가 너무 크면 서로 다른 주제가 섞여 의미가 희석되고, 유사도 매칭이 흐려진다. 반대로 작고 주제에 집중된 청크는 명확한 의미 벡터를 만들어 검색 정확도를 높인다. [^2][^4]

<br>

### 1.2 개발자가 제어하는 두 가지 변수: chunk_size와 chunk_overlap

- **chunk_size**(청크 크기): 각 청크의 최대 글자 수 또는 토큰 수. 작게 잡으면 사실 검색 정확도가 높고, 크게 잡으면 요약/종합 질문에 유리하다. 보통 256~512 토큰에서 시작해 실험을 권한다. [^16][^17]
- **chunk_overlap**(청크 중첩): 인접 청크 간 겹치는 길이. 경계에서 의미 단절을 줄인다. 통상 청크 길이의 10~20%를 출발점으로 삼는다. [^20]

- 또한 chunk_size는 사용하는 임베딩 모델의 입력 한계와 최적 길이에 영향을 받는다. 예를 들어 일부 BERT 계열 모델은 512 토큰 한계를 가진다. 따라서 임베딩 모델 특성과 함께 최적화해야 한다.

<br>

## 파트 2: 청킹 툴킷 — 주요 전략별 실용 가이드

- 이제 곧바로 적용 가능한 대표 청킹 전략들을 개념, 동작 방식, 장단점, 추천 사례, 코드와 함께 살펴보자.

### 2.1 고정 크기 청킹 (Fixed-Size Chunking) — 가장 단순한 기준선

- **개념**: 내용/구조와 무관하게 정해진 길이로 자른다. [^7]
- **동작**: 슬라이딩 윈도우로 chunk_size만큼 자르고, chunk_overlap만큼 겹친다.
- **장점**: 구현 간단, 비용 저렴, 크기 일정.
- **단점**: 문장/단어 중간에서 잘릴 위험이 커 의미가 훼손될 수 있다.
- **추천**: 구조가 균일하고 의미 단위 중요성이 낮은 로그 등에서 기준선으로만 권장.

<script src="https://gist.github.com/ingu627/8b862d7b90e10651b1f1c30d24af5f40.js"></script>

> Note: chunks는 리스트이므로 요소 접근 시 인덱싱이 필요하다.

### 2.2 재귀적 문자 분할 (Recursive Character Splitting) — 현명한 기준선

- **개념**: 자연스러운 경계(단락, 문장 등)를 우선 활용해 분할한다. [^24]
- **동작**: 구분자 우선순위(["\n\n", "\n", " ", ""])를 적용하며, 조각이 너무 크면 더 작은 구분자로 재귀 분할한다.
- **장점**: 의미 단위를 최대한 보존. 다양한 텍스트에 안정적. 현실적인 기본값으로 추천.
- **단점**: 고정 크기 대비 계산량이 약간 증가. 구분자가 빈약한 텍스트엔 효과 제한.
- **추천**: 일반 텍스트 문서의 기본 전략.

<script src="https://gist.github.com/ingu627/0b30f33a58291a59541edeb51d402c44.js"></script>

<br>

### 2.3 문서 구조 기반 청킹 (Document-Specific Chunking) — 구조를 아는 자의 특권

- **개념**: 마크다운 헤더, HTML 태그, 소스 코드의 함수/클래스 등 문서의 고유 구조를 활용해 논리적 청크를 만든다. [^24]
- **동작**: 문서 유형별 분할기를 사용. 예: 마크다운은 헤더 레벨을 기준으로 섹션 단위 청크 생성, 상위 헤더 정보를 메타데이터로 부여.
- **장점**: 저자의 의도와 문맥을 잘 보존, 청크 자체의 완결성이 높다.
- **단점**: 구조가 명확한 문서에 한정.
- **추천**: 기술 문서, API 명세서, 지식 베이스, 법률 문서, 소스 코드 등.

<script src="https://gist.github.com/ingu627/46ee0d5ecd1df489a26b8f02a669e1b4.js"></script>

<br>

### 2.4 시맨틱 청킹 (Semantic Chunking) — 의미를 이해하는 접근법

- **개념**: '길이/구조'가 아닌 '의미' 기반으로 문장을 묶는다. [^19][^25]
- **동작**: 문장 분할 → 임베딩 → 인접 문장 유사도 계산 → 유사도가 급락하는 지점을 분할점으로 삼는다.
- **장점**: 문맥 일관성과 의미 보존이 가장 뛰어남. 복잡한 전문 문서에 강함.
- **단점**: 전 문장 임베딩으로 계산 비용이 큼. 임계값 튜닝 필요.
- **추천**: 연구 논문, 재무 보고서, 법률 계약서 등.

<script src="https://gist.github.com/ingu627/4b7a73d6bcb29ce2cbbc3b1a9e3883aa.js"></script>

- 이러한 전략의 흐름을 보면, 초기의 고정 크기 방식은 전처리를 단순화하고 모델이 모든 문맥 부담을 지도록 했다. 재귀적/구조 기반은 '구조적 지능'을 전처리에 도입하여 모델 부담을 줄였고, 시맨틱 청킹은 임베딩을 활용해 '의미적 지능'을 데이터에 내장한다. 상류 단계에서 데이터를 정제하면 검색은 단순해지고, 생성 모델의 환각 가능성도 줄어든다. [^6]

<br>

## 파트 3: 전략적 플레이북과 미래 기술

### 3.1 최신 기술 (고급/실험적)

- **에이전틱 청킹 (Agentic Chunking)**
- **개념**: LLM이 스스로 문서를 읽고 최적의 분할을 설계.
- **동작**: "문서를 읽고 주제별로 묶어 각 핵심을 요약" 같은 프롬프트로 분할 수행.
- **현황**: 비용이 크고 실험적이지만, 잘못된 추론을 크게 줄였다는 초기 결과가 보고됨. [^6]

- **문맥 강화 청킹 (Context-Enriched Chunking)**
- **슬라이딩 윈도우/중첩**: 기본적 맥락 보강.
- **요약 메타데이터**: 주변/전체 요약문을 메타데이터로 저장해 검색 유사도를 향상.
- **계층적 청킹**: 상위 요약 청크 → 하위 상세 청크로 2단계 검색(zoom-in) 전략. [^2]

<br>

### 3.2 전략 가이드 — 상황에 맞는 최적의 선택

- "최고의" 청킹 전략은 없다. 과제 특성에 가장 맞는 전략을 고르는 것이 정답이다. 다음 질문으로 선택을 돕자.

1) 내 문서의 구조는 어떠한가?
- 마크다운/소스 코드처럼 구조가 명확함 → 문서 구조 기반 청킹
- 일반 텍스트 문서 → 재귀적 문자 분할
- 구조는 없지만 내용이 복잡/전문적 → 시맨틱 청킹

2) 사용자는 어떤 질문을 하는가?
- 구체 사실 확인(예: 가격) → 작은 청크(문장 기반)
- 광범위 요약/종합 → 큰 청크(시맨틱 또는 재귀 분할)

3) 성능·비용 제약은 무엇인가?
- 빠른 속도/저비용 우선 → 재귀적 문자 분할
- 비용보다 정확도 우선 → 시맨틱/에이전틱 청킹

### 한눈에 보는 청킹 전략 비교 표

| 전략 | 핵심 동작 방식 | 문맥 보존 능력 | 계산 비용 | 구현 복잡도 | 추천 사용 사례 |
|---|---|---|---|---|---|
| 고정 크기 | 정해진 글자 수로 텍스트 분할 | 낮음 | 매우 낮음 | 매우 낮음 | 로그 등 구조 균일·의미 단위 중요도 낮은 경우 |
| 재귀적 문자 분할 | 단락/문장 등 우선순위 구분자로 재귀 분할 | 중간 | 낮음 | 낮음 | 대부분의 일반 텍스트(기본값으로 추천) |
| 문장 기반 | 문장 단위로 텍스트 분할 | 중간(문장 내) | 낮음 | 낮음 | 챗봇·QA 등 짧고 명확한 답변 |
| 문서 구조 기반 | 마크다운 헤더·HTML 태그 등 구조 활용 | 높음 | 낮음 | 중간 | 기술 문서·지식 베이스·소스 코드 |
| 시맨틱 청킹 | 문장 임베딩 유사도로 의미 경계에서 분할 | 매우 높음 | 높음 | 높음 | 연구 논문·법률/금융 문서 |
| 에이전틱 청킹 | LLM이 직접 이해하고 최적 분할 수행 | 매우 높음 | 매우 높음 | 매우 높음 | (실험적) 비용 제약 적고 최고 성능 필요 |

### 실험의 중요성

- 가이드는 출발점일 뿐이다. 먼저 재귀적 문자 분할을 chunk_size ≈ 500 토큰, chunk_overlap ≈ 50 토큰으로 설정해 견고한 기준선을 만들고, 문서/사용 사례 특성에 맞춰 다른 전략을 실험적으로 조정하자. [^16]

<br>

## 결론: 청킹은 고성능 RAG의 초석

- 지능적인 청킹은 시스템 성능·비용·신뢰도를 최적화하는 가장 강력한 레버다. 기본 분할기만 쓰지 말고, 데이터와 목표에 맞춰 파이프라인을 설계하자. 시맨틱/구조 기반 청킹은 초기 비용이 들지만, 최종 품질과 정확성의 큰 개선으로 돌아온다.

<br>
<br>

## References

[^1]: [Chunking Strategies in RAG You Need To Know — F22 Labs:]( https://www.f22labs.com/blogs/7-chunking-strategies-in-rag-you-need-to-know/)
[^2]: [Chunking Strategies to Improve Your RAG Performance — Weaviate:]( https://weaviate.io/blog/chunking-strategies-for-rag)
[^3]: [Chunking in RAG: A Beginner's Guide — Praveen Cs:]( https://medium.com/@praveencs87/chunking-in-rag-retrieval-augmented-generation-a-beginners-guide-28b5a81a8877)
[^4]: [Chunking Strategies for RAG — Simplified & Visualized — Mastering LLM:]( https://masteringllm.medium.com/11-chunking-strategies-for-rag-simplified-visualized-df0dbec8e373)
[^5]: [Mastering RAG: Advanced Chunking Techniques — Galileo AI:]( https://galileo.ai/blog/mastering-rag-advanced-chunking-techniques-for-llm-applications)
[^6]: [The Hidden Performance Bottleneck in RAG — Utkarsh Patel:]( https://medium.com/@utkarshhpatel13/the-hidden-performance-bottleneck-in-rag-why-your-chunking-strategy-makes-or-breaks-your-ai-system-6ec5e886e5bc)
[^7]: [RAG Chunking Strategies: Complete Guide — Latenode:]( https://latenode.com/blog/rag-chunking-strategies-complete-guide-to-better-retrieval)
[^8]: [How to Optimize Document Chunking — Ksolves:]( https://www.ksolves.com/blog/artificial-intelligence/optimize-document-chunking-for-better-rag-performance)
[^9]: [Enhancing RAG performance with smart chunking strategies — IBM Developer:]( https://developer.ibm.com/articles/awb-enhancing-rag-performance-chunking-strategies/)
[^10]: [Chunking for RAG: best practices — Unstructured:]( https://unstructured.io/blog/chunking-for-rag-best-practices)
[^11]: [A Deep-Dive into Chunking Strategy — Superteams:]( https://www.superteams.ai/blog/a-deep-dive-into-chunking-strategy-chunking-methods-and-precision-in-rag-applications)
[^12]: [The Ultimate Guide to Chunking Strategies for RAG with Databricks:]( https://medium.com/@debusinha2009/the-ultimate-guide-to-chunking-strategies-for-rag-applications-with-databricks-e495be6c0788)
[^13]: [Chunking and Embedding Strategies in RAG — Tahir Saeed:]( https://medium.com/@tahir.saeed_46137/chunking-and-embedding-strategies-in-rag-a-guide-to-optimizing-retrieval-augmented-generation-7c95432423b1)
[^14]: [A Guide to Chunking Strategies for RAG — Sagacify:]( https://www.sagacify.com/news/a-guide-to-chunking-strategies-for-retrieval-augmented-generation-rag)
[^15]: [Chunking strategies for RAG tutorial using Granite — IBM:]( https://www.ibm.com/think/tutorials/chunking-strategies-for-rag-with-langchain-watsonx-ai)
[^16]: [What Chunk Size and Chunk Overlap Should You Use? — DEV:]( https://dev.to/peterabel/what-chunk-size-and-chunk-overlap-should-you-use-4338)
[^17]: [What is the optimal chunk size for RAG applications? — Milvus:]( https://milvus.io/ai-quick-reference/what-is-the-optimal-chunk-size-for-rag-applications)
[^18]: [Chunking Strategies for RAG in Generative AI — ADaSci:]( https://adasci.org/chunking-strategies-for-rag-in-generative-ai/)
[^19]: [Semantic Chunking for RAG — Multimodal:]( https://www.multimodal.dev/post/semantic-chunking-for-rag)
[^20]: [Chunk size and overlap — Unstract Docs:]( https://docs.unstract.com/unstract/unstract_platform/user_guides/chunking/)
[^21]: [Microsoft Learn Q&A — 문서 청크 길이와 중첩 설정:]( https://learn.microsoft.com/en-us/answers/questions/1551865/how-do-you-set-document-chunk-length-and-overlap-w)
[^22]: [Simple Chunking Strategies for RAG Applications (Part 1) — Ayoub:]( https://medium.com/@ayoubkirouane3/simple-chunking-strategies-for-rag-applications-part-1-d56903b167c5)
[^23]: [Chunking Strategies in RAG Systems — Prem AI Blog:]( https://blog.premai.io/chunking-strategies-in-retrieval-augmented-generation-rag-systems/)
[^24]: [How to split Markdown by Headers — LangChain Docs:]( https://python.langchain.com/docs/how_to/markdown_header_metadata_splitter/)
[^25]: [llama-index-packs-node-parser-semantic-chunking — PyPI:]( https://pypi.org/project/llama-index-packs-node-parser-semantic-chunking/)
[^26]: [How Intelligent Chunking Transforms RAG Performance — Momen:]( https://momen.app/blogs/intelligent-chunking-boosts-rag-performance/)
[^27]: [SemDB's Approach to Chunking — Intelligence Factory AI:]( https://www.intelligencefactory.ai/blog/chunking-strategies-for-retrieval-augmented-generation-rag-a-deep-dive-into-semdbs-approach)
[^28]: [Chunking Strategies in RAG: Optimising Data for Advanced AI Responses — YouTube:]( https://www.youtube.com/watch?v=pIGRwMjhMaQ)
