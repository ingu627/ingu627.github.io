---
layout: single
title: "[케라스(keras) 이해] 6장. 텍스트와 시퀀스를 위한 딥러닝 (1)"
excerpt: "케라스 창시자에게 배우는 딥러닝 요약"
categories: python
tag : [python, keras, DL, NLP]
toc: true
toc_sticky: true
author_profile: false

last_modified_at: 2021-11-15
---

> 본 글은 [케라스 창시자에게 배우는 딥러닝] 책을 개인공부하기 위해 요약, 정리한 내용입니다.

## 6_1. 텍스트 데이터 다루기

- 시퀀스 데이터를 처리하는 기본적인 딥러닝 모델은 **순환 신경망**(recurrent neural network)과 **1D**(1D convnet) 두 가지이다.
- 텍스트는 가장 흔한 시퀀스 형태의 데이터이다. 단어의 시퀀스나 문자의 시퀀스로 이해할 수 있다. 
- 자연어 처리를 위한 딥러닝은 단어, 문장, 문단에 적용한 패턴 인식이다.
- 딥러닝 모델은 수치형 텐서만 다룰 수 있다.
- **텍스트 벡터화**(vectorrizing text) : 텍스트를 수치형 텐서로 변환하는 과정
  - 텍스트를 단어로 나누고 각 단어를 하나의 벡터로 변환한다.
  - 텍스트를 문자로 나누고 각 문자를 하나의 벡터로 변환한다.
  - 텍스트에서 단어나 문자의 **n-그램**을 추출하여 각 n-그램을 하나의 벡터로 변환한다. 
    - **n-gram** : 연속된 단어나 문자의 그룹으로 텍스트에서 단어나 문자를 하나씩 이동하면서 추출
- **토큰**(token) : 텍스트를 나누는 단위(단어, 문자, n-그램)
- **토큰화**(tokenization) : 텍스트를 토큰으로 나누는 작업
- 모든 텍스트 벡터화 과정은 어떤 종류의 토관화를 적용하고 생성된 토큰에 수치형 벡터를 연결하는 것으로 이루어진다.
- 이런 벡터는 시퀀스 텐서로 묶여져서 심층 신경망에 주입된다.

### n-그램과 BoW

- **단어 n-그램** : 문장에서 추출한 N개의 연속된 단어 그룹.
- **가방**(bag) : 다루고자 하는 것이 리스트나 시퀀스가 아니라 토큰의 집합을 의미. 특정한 순서가 없는데 이런 종류의 토큰화 방법을 **BoW**

## 6_1_1. 단어와 문자의 원-핫 인코딩 

- **원-핫 인코딩** : 모든 단어에 고유한 정수 인덱스를 부여하고 이 정수 인덱스 i를 크기가 N(어휘 사전의 크기)인 이진 벡터로 변환한다.
  - 이 벡터는 i번째 원소만 1이고 나머지는 모두 0이다. 

### 단어 수준의 원-핫 인코딩하기

<script src="https://gist.github.com/ingu627/9718fa6fae18c5c7c56f543f24040c73.js"></script>

### 문자 수준 원-핫 인코딩하기 

<script src="https://gist.github.com/ingu627/55c68a0220b767a51a32cfce7b879d4b.js"></script>

### 케라스를 사용한 단어 수준의 원-핫 인코딩하기

<script src="https://gist.github.com/ingu627/146f1ccee1a1fe3d29377ba348e84c72.js"></script>

![image](https://user-images.githubusercontent.com/78655692/141731087-21a7bc72-ed72-4227-9d4c-24e9c18e7031.png)

- `Tokenizer` : 문장으로부터 단어를 토큰화하고 숫자에 대응시키는 딕셔너리를 사용할 수 있도록 한다.
-`fit_on_texts` : 문자 데이터를 입력받아서 리스트의 형태로 변환한다.
- `word_index ` : tokenizer의 word_index 속성은 단어와 숫자의 키-값 쌍을 포함하는 딕셔너리를 반환한다.
- `texts_to_sequences() ` : 텍스트 안의 단어들을 숫자의 시퀀스의 형태로 변환
- `texts_to_matrix()` : 텍스트를 시퀀스 리스트로 바꾸어 주는 texts_to_sequences() 메서드와 시퀀스 리스트를 넘파이 배열로 바꾸어 주는 sequences_to_matrix() 메서드를 차례대로 호출한다.
  - `mode` 매개변수에서 지원하는 값은 기본값 `binary`외에 `count`(단어의 출현 횟수), `freq`(출현 횟수를 전체 시퀀스의 길이로 나누어 정규화), `tfidf`(TF-IDF)가 있다.

### 해싱 기법을 사용한 단어 수준의 원-핫 인코딩하기

- 이 방식은 어휘 사전에 있는 고유한 토큰의 수가 너무 커서 모두 다루기 어려울 때 사용한다.
- 각 단어에 명시적으로 인덱스를 할당하고 이 인덱스를 딕셔너리에 저장하는 대신에 단어를 해싱하여 고정된 크기의 벡터로 변환한다.

<script src="https://gist.github.com/ingu627/ae076fe88617a895987d5fd7e92c87f0.js"></script>

## 6_1_2. 단어 임베딩 사용하기 

- 단어 임베딩 == 밀집 단어 벡터
- 원-핫 인코딩으로 만든 벡터는 희소(sparse)하고(대부분 0으로 채워진다) 고차원이다.(어휘 사전에 있는 단어의 수와 차원이 같다.)
- 반면에 단어 임베딩은 저차원의 실수형 벡터이다.(희소 벡터의 반대인 밀집 벡터이다.)

## Embedding 층을 사용하여 단어 임베딩 학습하기

- **단어 임베딩**은 언어를 기하학적 공간에 매핑하는 것이다.

![image](https://user-images.githubusercontent.com/78655692/141734235-8f1e039b-eb4f-48ac-be66-6c962f74f447.png)

- 단어 임베딩 공간은 전형적으로 이런 해석 가능하고 잠재적으로 유용한 수천 개의 벡터를 특성으로 가진다. 

### Embedding 층의 객체 생성하기

<script src="https://gist.github.com/ingu627/52b53893a7334fba7e351bd12e33e634.js"></script>

- Embedding 층은 적어도 2개의 매개변수를 받는다. 가능한 토큰의 개수(여기서는 1000으로 단어 인덱스 최댓값 +1)와 임베딩 차원이다.
- **Embedding 층을 (특정 단어를 나타내는) 정수 인덱스를 밀집 벡터로 매핑하는 딕셔너리로 이해하면 된다.**
  - 정수를 입력으로 받아 내부 딕셔너리에서 이 정수에 연관된 벡터를 찾아 반환한다.
- 단어 인덱스 -> Embedding 층 -> 연관된 단어 벡터 

### 원리

- Embedding 층은 크기가 `(samples, sequence_length)`인 2D 정수 텐서를 입력으로 받는다. 각 샘플은 정수의 시퀀스이다. 가변 길이의 시퀀스를 임베딩할 수 있다.
  - 예를 들어 Embedding 층에 (32, 10) 크기의 배치(=길이가 10인 시퀀스 32개로 이루어진 배치)나 (64, 15) 크기의 배치(=길이가 15인 시퀀스 64개로 이루어진 배치)를 주입할 수 있다. 
  - 배치에 있는 모든 시퀀스는 길이가 같아야 하므로(하나의 텐서에 담아야 하기 때문에) 작은 길이의 시퀀스는 0으로 패딩되고 길이가 더 긴 시퀀스는 잘린다.
- Embedding 층은 크기가 `(samples, sequence_length, embedding_dimensionality)`인 3D 실수형 텐서를 반환한다.
  - 이런 텐서는 RNN 층이나 1D 합성곱 층에서 처리된다.
- Embedding 층의 객체를 생성할 때 가중치(토큰 벡터를 위한 내부 딕셔너리)는 다른 층과 마찬가지로 랜덤하게 초기화된다.
  - 훈련하면서 이 단어 벡터는 역전파를 통해 점차 조정되어 이어지는 모델이 사용할 수 있도록 임베딩 공간을 구성한다.
  - 훈련이 끝나면 임베딩 공간은 특정 문제에 특화된 구조를 많이 가지게 된다.

### IMDB 영화 리뷰 감성 예측 문제 적용해보기 

<script src="https://gist.github.com/ingu627/c20e02b4aa1c54c4345a6280c15df65b.js"></script>

- 영화 리뷰에서 가장 빈도가 높은 1만 개의 단어를 추출하고 리뷰에서 20개가 넘는 단어는 버린다.
- 이 네트워크는 1만 개의 단어에 대해 8차원의 임베딩을 학습하여 정수 시퀀스 입력(2D 정수 텐서)을 임베딩 시퀀스(3D 실수형 텐서)로 바꾼다.
- 그 다음 이 텐서를 2D로 펼쳐서 분류를 위한 Dense 층을 훈련한다.
- `pad_sequences` : 서로 다른 개수의 단어로 이루어진 문장을 같은 길이로 만들어주기 위해 패딩을 사용한다. 이 함수에 이 시퀀스를 입력하면 숫자 0을 이용해서 같은 길이의 시퀀스로 변환한다.

![image](https://user-images.githubusercontent.com/78655692/141738923-e285c39e-a909-46d0-b85c-0e2d4010f557.png)

## 6_1_3. 모든 내용을 적용하기: 원본 텍스트에서 단어 임베딩까지

ㄴ



## References

- [케라스 창시자에게 배우는 딥러닝](https://www.aladin.co.kr/shop/wproduct.aspx?ItemId=173992478) 
- [Word2Vec 으로 단어 임베딩하기 (Word2Vec word embeddings)](https://medium.com/@jennyamy2531/word2vec-%EC%9C%BC%EB%A1%9C-%EB%8B%A8%EC%96%B4-%EC%9E%84%EB%B2%A0%EB%94%A9%ED%95%98%EA%B8%B0-word2vec-word-embeddings-96725bfb2580)