---
layout: single
title: "[CS231n] 강의9. Object Detection and Image Segmentation (2) 리뷰 "
excerpt: "본 글은 2022년 4월에 강의한 스탠포드 대학의 Convolutional Neural Networks for Visual Recognition 2022년 강의를 듣고 정리한 Object Detection에 대한 내용입니다."
categories: cs231n
tag : [이미지, cnn, cs231n, 리뷰, 정리, 요약, 정의, 란, 딥러닝, 컴퓨터 비전, 객체 인식, object detection, region proposal, selective search, r cnn, fast r cnn, roi, pool, align, faster r cnn,]
toc: true 
toc_sticky: true
sidebar_main: true

last_modified_at: 2022-06-16
---

<img align='right' width='400' src='https://user-images.githubusercontent.com/78655692/172999209-71994ac3-c8a9-4ac2-82b5-84b18c83cdbb.png'>
본 글은 2022년 4월에 강의한 스탠포드 대학의 "Convolutional Neural Networks for Visual Recognition" 2022년 강의를 듣고 정리한 내용입니다. <br> 개인 공부 목적으로 작성되었으며, 설명이 맞지 않거나 글 오타가 있으면 알려주시길 바랍니다.
{: .notice--info}

원본 링크 : [cs231n.stanford.edu](http://cs231n.stanford.edu/)<br> 한글 번역 링크 : [aikorea.org - cs231n](http://aikorea.org/cs231n/) <br> 강의 링크 : [youtube - 2017 Spring (English)](https://www.youtube.com/playlist?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv)
{: .notice--warning}

<br>
<br>
<br>

## Object Detection

<img src='https://user-images.githubusercontent.com/78655692/173877271-8dafbf1d-4d44-4282-94da-5cbd92cc7f27.png' width=670>

- **Object Detection**은 이미지에 있는 모든 물체를 찾아내어 각각을 분류하고 각각의 좌표를 출력하는 task이다. [^1]
- object의 클래스를 classification할 뿐만 아니라 localization까지 함께 수행한다.
- **Localization** : 이미지 내에 하나의 object가 있을 때 그 object의 위치를 특정하는 것 [^2]
- **Detection** : 여러 개의 object가 존재할 때 각 object의 존재 여부를 파악하고 위치를 특정하여 classification을 수행하는 것 [^2]

<br>

<img src='https://user-images.githubusercontent.com/78655692/173885026-5b4b4f0c-aeec-4438-814b-1029bc13a89a.png' width=670>

- 우선 하나의 object만 있다면 하나의 2D bounding box를 찾으면 된다.
  - **bound box**는 $(X,Y,W,H)$ 또는 $(X_1,Y_1,X_2,Y_2)$ 4개의 파라미터로 구성된다.
    - $(X,Y,W,H)$에서 X,Y는 왼쪽 상단의 코너 위치를 나타내고, W,H는 box의 크기를 설정한다.
- **multitask loss**는 서로 다른 loss 조건의 크기(magnitude)를 balance하기 위해 모든 task별 손실의 가중치 합(weighted sum)이다.
- 따라서, classification을 위한 레이블과 localization을 위한 bounding box를 사용하여 네트워크를 학습시킨다.
  - Loss = Softmax Loss + L2 Loss

<br>
<br>

## Multiple Object Detection

<img src='https://user-images.githubusercontent.com/78655692/173979345-805c286a-558f-4ebc-84be-8b1ff7f4292e.png' width=670>

- 위 그림에서 볼 수 있듯이 object가 많을수록 고려해야할 파라미터 수 또한 많아진다.

<br>

<img src='https://user-images.githubusercontent.com/78655692/173980459-10045970-260a-4ba1-b619-4f1321667504.gif' width=670>

- 그렇다면 어떻게 해야 할까?
- semantic segmentation에서 언급한 방법처럼 슬라이딩 윈도우를 사용하는 것이다.
- 슬라이드하는 각 이미지 cut마다 image classifier를 사용하여 슬라이드 창에 object가 있는지를 파악한다. 
- 하지만 계산 효율성의 문제뿐만 아니라 슬라이딩 윈도우가 이미지 내의 다른 위치로 이동해야 하며 다양한 크기의 객체(object)를 고려해야 한다는 것이다.

<br>

### Region Proposals: Selective Search

<img src='https://user-images.githubusercontent.com/78655692/173981174-832e0ae2-d69b-4d7f-a27a-e4f81d8bd7c7.png' width=670>

- 어떻게 하면 모든 가능한 box를 거치지 않고도 모든 객체를 포착할 수 있는가이다.
- 객체를 포함할 가능성이 있는 윈도우를 선택하는 것을 region proposal 문제라고 한다.
- 이 문제에 대한 매우 효과적인 접근법으로 selective search 기법을 쓴다.

<br>
<br>

## R-CNN

<img src='https://user-images.githubusercontent.com/78655692/173989398-48b0023a-32d4-4f0e-94ad-27840e23ce63.gif' width=670>

- R-CNN은 첫 번째로 객체를 포함할 가능성이 있는 관심 영역(RoI)을 찾기 위해 selective search를 사용한다.
  - 수천 개(약 2000개)의 region을 생성한다.
- 이러한 RoI의 크기를 동일한 크기의 이미지 자르기(cropping)로 조정한다.(224x224)
- ConvNet을 통해 각 지역을 forward한다.
- SVM 손실이 있는 선형 분류기로 학습한다. 
- 하지만, 각 이미지의 forward pass가 2천개 이상이 필요하는 등 매우 느린 단점이 있다.

<br>

### Slow R-CNN

<img src='https://user-images.githubusercontent.com/78655692/173990231-cbc6a7ac-2d3c-4188-9e2c-9020ce121b93.png' width=670>

- 이미지를 자르기 전에 convnet으로 전달하고 대신 conv feature를 자른다.
- 이러한 아이디어의 개선된 버전을 기본적으로 제안하는 현재 거의 모든 딥 러닝 기반 2D 객체 감지 모델에서 여전히 사용되고 있다. [^3]

<br>
<br>

## Fast R-CNN

<img src='https://user-images.githubusercontent.com/78655692/173991042-427c3acb-4303-4757-9834-1ee02caf3791.gif' width=670>

- Fast R-CNN은 이미지를 자르고 크기를 조정한 후 ConvNet에 전달하는 RNN 방식 대신 ConvNet에 전체 이미지를 먼저 전달한 다음 region proposal과 관련된 conv feature를 자르고 크기를 조정한다.
  - 이렇게 하면 단일 이미지를 처리하기 위해 비싼 변환 레이어를 수천번 통과해야 하는 것을 피할 수 있다.
- Fast R-CNN 아키텍처는 end-to-end로 학습된다. 자르기 및 크기 조정은 이미지 내에서가 아니라 feature map에서 직접 수행된다. 
- 그럼 다음 질문으로 feature 자르기 및 크기 조정 작업은 무엇일지 살펴보는 것이다.

<br>
<br>

### Cropping Features: RoI Pool

<img src='https://user-images.githubusercontent.com/78655692/173993206-6e1eb14a-706c-4e0c-974d-eab565a88290.gif' width=670>

- **RoI Pool** : 자르기(cropping) 및 크기 조정(resizing) 작업
  - 목표 : 주어진 영역의 활성화 맵에서 feature를 추출하고 downstream 레이어가 처리할 고정 크기 텐서로 새 feature 맵을 줄이는 것이다.
- 위 그림의 녹색 box는 region proposal을 강조 표시한 것이다.
  - 목표는 동일한 영역을 사용하지만 feature 맵에서 잘라내는 것이다.
- 그 다음, 파란색 box처럼 대략적으로 동일한 하위 영역의 그리드에 맞춘다.
- maxpooling을 사용하여 텐서의 공간 크기를 2x2로 줄인다.
  - 따라서, 입력 영역의 크기가 다른 경우에도 region feature는 항상 같은 크기이다.
  - 하지만, 이 영역 feature가 약간 잘못 정렬될 수 있다.


<br>
<br>

### Cropping Features: RoI Align

<img src='https://user-images.githubusercontent.com/78655692/173993253-0be61c6e-e3a2-47fa-bc60-70aec19fcd39.gif' width=670>

- (향후 업데이트)



<br>
<br>

### R-CNN vs Fast R-CNN

<img src='https://user-images.githubusercontent.com/78655692/173993345-f63963c9-bf93-46e6-9454-98813ea9bd63.png' width=670>

- Fast R-CNN은 공유 컨볼루션 백본을 사용하여 이미지 처리를 훨씬 더 효율적으로 만들었다.
- 학습 때 Fast R-CNN은 R-CNN보다 약 10배 빠르다. 그리고 테스트 때는 약 20배 더 빠른 것을 알 수 있다.
- 하지만, Fast R-CNN이 실제로 selective search를 사용한 region proposal를 계산하고 있다는 것을 알 수 있다.
- 따라서, 전체 작업은 region proposal의 경우 2초로 분할되는 2.3초와 region proposal를 얻은 후 객체를 분류하는 데 0.32초가 소요된다.
  - 따라서 런타임은 region proposal에 의해 좌우된다.

<br>
<br>


## Faster R-CNN (1) 

<img src='https://user-images.githubusercontent.com/78655692/173998014-1f72ccfd-7c69-422c-93ba-f4cc726bd640.png' width=670>

- **Faster R-CNN**은 region proposal 부분을 더 빠르게 만들기 위해 노력했다. 
  - 외부 selective search 알고리즘에 의존하는 대신 네트워크에 실제로 컨볼루션 feature를 사용하여 객체가 주어진 영역(region)에 존재하는지 여부를 결정하고 네트워크 내부에서 region proposal을 생성하는 **내장형 RPN(Region Proposal Network)**가 있다.

<br>
<br>

### Region Proposal Network

<img src='https://user-images.githubusercontent.com/78655692/173993754-e89b8629-82af-4978-b043-8f26bdf3ab23.gif' width=670>

- RPN의 첫 번째 단계는 **앵커 박스(anchor box)** 세트를 지정하는 것이다.
  - 이러한 anchor box는 슬라이딩 윈도우와 유사한 다른 위치, 크기, 비율로 샘플링된 영역이다.
  - 앵커는 이전 R-CNN과 Fast R-CNN 에서 사용된 다수의 슬라이딩 윈도우와 달리 상대적으로 드문드문 샘플링된다는 점에서 슬라이딩 윈도우와 다르다. 즉, 수가 훨씬 적다.
- 이러한 앵커는 객체가 일반적으로 natural 이미지에서 발생하는 영역만 덮고 모든 입력 이미지에 대해 동일한 앵커 세트를 사용한다.
- RPN이 하는 일은 앵커 상자에 객체가 있는지 여부를 예측하는 매우 얕은 convnet을 통해 이러한 영역의 중심에 있는 feature를 전달하는 것이다.
  - feature map의 각 지점은 해당 지점을 중심으로 앵커 상자에서 객체를 감지하는 역할을 한다. 

<br>
<br>

## Faster R-CNN (2)

<img src='https://user-images.githubusercontent.com/78655692/173998124-5aa614bd-2fc4-4d12-9e08-844d696919ae.png' width=670>

- 다시 돌아와 Faster R-CNN 과정을 살펴본다. [^1]
  - Faster R-CNN에는 총 4개의 loss가 있다.

1. 입력 이미지가 CNN을 통과항 전체 이미지에 대한 feature map을 생성한다.
2. RPN이 feature map에서 region proposal을 예측한다.
3. RPN에서 예측한 region proposal에 대해 물체가 있는지 없는지에 대한 여부의 classification와 bounding box regression을 진행한다.
4. RPN에서 예측한 region proposal에 ROI Pooling을 진행하고 FC 레이어를 거쳐 classification과 bounding box regression을 진행한다.

<br>

<img src='https://user-images.githubusercontent.com/78655692/174003529-13e6a7ee-0fc4-4831-8864-9bbe10b6497b.png' width=470>

- 계산 효율성 측면에서 Faster R-CNN은 단 몇 0.2초만에 이미지를 처리할 수 있고, Fast R-CNN보다 약 10배 빠르다.

<br>

<img src='https://user-images.githubusercontent.com/78655692/174003842-b60782bf-89bb-4150-b53b-bfdb01ebd536.png' width=670>

- Object Detection은 1-stage와 2-stage로 나뉜다. [^4]
  - Object Detection은 객체의 위치를 찾는 Localization 문제와 객체를 실별하는 classification 문제를 풀어야 한다.
  - **1-stage** : 위 두 문제를 동시에 행하는 방법
    - fast but low accuracy
    - ex. YOLO, SSD, RetinaNet
  - **2-stage** : 위 두 문제를 순차적으로 행하는 방법
    - slow but high accuracy
    - ex. R-CNN, Fast R-CNN, Faster R-CNN
- Faster R-CNN은 1-stage가 이미지를 특징짓고 region proposal의 초기 배치를 생성하는 2-stage 객체 감지기다. 2번째 단계는 각 proposal를 개별적으로 처리하여 이러한 영역을 분류하고 세분화하여 최종 예측을 산출하는 것이다.
- 그렇다면 2-stage가 정말로 필요한 가에 대해 질문이 있을 수 있다. 

<br>
<br>

## Others

<img src='https://user-images.githubusercontent.com/78655692/174004511-c8df8df9-a14c-44bd-bab1-37306a713a32.png' width=670>

- **YOLO(You Only Look Once)**는 기본적으로 입력 이미지를 그리드로 나누는 단일 단계 객체 감지기(single-stage object detector)이다.
- 각 그릴 셀에 대해 다수의 "베이스 박스"(Faster R-CNN이 "앵커 박스"라고 부름)를 사용하고 각 베이스 박스에 대한 경계 박스 수정을 출력한다. 
- 또한 각 셀에 대한 클래스를 예측하고 이를 예측 상자에 대한 클래스 예측으로 사용한다.
- YOLO는 모든 경계 상자와 해당 클래스를 한 번에 하나의 전진 패스로 출력하므로 두 번째 단계가 없다. 따라서 단일 단계 검출기라고 한다.

<br>
<br>
<br>
<br>

## References

[^1]: [CS231n | Lecture11. Detection and Segmentation - Jayden Lee](https://ai-rtistic.com/2021/12/02/cs231n-lecture11/#object-detection)
[^2]: [딥러닝 Object Detection(1) - 개념과 용어 정리 - cha-suyeon](https://velog.io/@cha-suyeon/%EB%94%A5%EB%9F%AC%EB%8B%9D-Object-Detection-%EA%B0%9C%EB%85%90%EA%B3%BC-%EC%9A%A9%EC%96%B4-%EC%A0%95%EB%A6%AC)
[^3]: [CS231n • Detection and Segmentation - Distilled AI](https://aman.ai/cs231n/detection/#single-object-detection-localization--classification)
[^4]: [[Object Detection] 1. Object Detection 논문 흐름 및 리뷰 - 제이스핀](https://nuggy875.tistory.com/20)

