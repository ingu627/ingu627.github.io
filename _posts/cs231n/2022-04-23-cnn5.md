---
layout: single
title: "[CS231n] 강의6. Hardware and Software with Pytorch 리뷰"
excerpt: "본 글은 2021년 4월에 강의한 스탠포드 대학의 Convolutional Neural Networks for Visual Recognition 2021년 강의를 듣고 정리한 내용입니다. Lecture6 소프트웨어, 특히 pytorch에 대해 정리했습니다."
categories: cs231n
tag : [이미지, cnn, cs231n, 리뷰, 정리, 요약, 정의, 란, 딥러닝, 머신러닝, 파이토치, pytorch, 의미]
toc: true 
toc_sticky: true
sidebar_main: true

last_modified_at: 2022-04-25
---

<img align='right' width='400' src='https://user-images.githubusercontent.com/78655692/164892920-005033a1-391b-44e4-8614-963b53783eb9.png'>
본 글은 2021년 4월에 강의한 스탠포드 대학의 "Convolutional Neural Networks for Visual Recognition" 2021년 강의를 듣고 정리한 내용입니다. <br> 개인 공부 목적으로 작성되었으며, 설명이 맞지 않거나 글 오타가 있으면 알려주시길 바랍니다.
{: .notice--info}

원본 링크 : [cs231n.stanford.edu](http://cs231n.stanford.edu/)<br> 한글 번역 링크 : [aikorea.org - cs231n](http://aikorea.org/cs231n/) <br> 강의 링크 : [youtube - 2017 Spring (English)](https://www.youtube.com/playlist?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv)
{: .notice--warning}

<br>
<br>
<br>

- 이 강의에서는 CPU와 GPU같은 하드웨어 내용이 자세히 나오지만, 여기서는 생략하도록 하겠다.
- 또한 파이토치와 텐서플로 등 다양한 딥러닝 프레임워크들이 나오지만, 여기서는 파이토치만 집중적으로 다루겠다.

<br>

## Computational Graphs

- 먼저 계산 그래프를 numpy로 작성해본다.

  <img src='https://user-images.githubusercontent.com/78655692/165030650-5c1ce77b-6baa-4219-ac5c-bb823b78542e.png' width='200'>

  ```python
  import numpy as np
  np.random.seed(0)

  N, D = 3, 4

  x = np.random.randn(N, D)
  y = np.random.randn(N, D)
  z = np.random.randn(N, D)

  a = x * y
  b = a + z
  c = np.sum(b)

  grad_c = 1.0
  grad_b = grad_c * np.ones((N, D))
  grad_a = grad_b.copy()
  grad_z = grad_b.copy()
  grad_x = grad_a * y
  grad_y = grad_a * x
  ```

- Good
  - API가 깔끔하다.
  - 수치적 코드를 쓰기 편하다.
- Bad
  - 고유의 그레이디언트를 계산하지 못한다.
  - GPU에서 실행할 수 없다.

<br>
<br>


## PyTorch

- **torch.Tensor** : 넘파이(numpy) 어레이(array)와 비슷하지만, GPU에서 실행할 수 있다.
- **torch.autograd** : 텐서로 계산 그래프(computational graph)를 만들고, 그레이디언트(gradient)를 자동으로 계산하는 패키지이다.
- **torch.nn.Module** : 신경망(neural network) 레이어; 상태(state) 또는 학습 가능한 가중치를 저장할 수 있다.

<br>
<br>

### Pytorch: Tensors

- 다음은 torch.Tensor에 관한 예제이다.
- **torch.randn** : 평균이 0이고, 표준편차가 1인 가우시안 정규분포를 이용해 생성한다.
- **torch.mm** : 행렬의 행렬 곱셈을 수행한다.
- **torch.clamp(min, max)** : 모든 요소를 [min, max] 범위로 고정한다. [^1]
- **torch.t(input)** : input이 2차원 텐서보다 작을 것으로 예상하고 차원 0과 1을 전치(transpose)한다. 

```python
import torch

# device 정의 = gpu가 가능하면 cuda, 아니면 cpu
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# 데이터와 가중치를 위해 랜덤 텐서 생성
  # H : hidden의 약어
N, D_in, H, D_out = 64, 1000, 100, 10
x = torch.randn(N, D_in, device=device)
x = torch.randn(N, D_out, device=device)
x = torch.randn(D_in, H, device=device)
x = torch.randn(H, D_out, device=device)

learning_rate = 1e-6
for t in range(500):
    # 정방향 전달(forward pass) : 예측(prediction)과 손실(loss)을 계산한다.
    h = x.mm(w1)
    h_relu = h.clamp(min=0)
    y_pred = h_relu.mm(w2)
    loss = (y_pred - y).pow(2).sum()
    
    # 역방향 전달(backward pass) : 수동으로 그레이디언트를 계산한다.
    grad_y_pred = 2.0 * (y_pred - y)
    grad_w2 = h_relu.t().mm(grad_y_pred)
    grad_h_relu = grad_y_pred.mm(w2.t())
    grad_h = grad_h_relu.clone()
    grad_h[h < 0] = 0
    grad_w1 = x.t().mm(grad_h)

    # 가중치에 대한 경사 하강(gradient descent) 단계(step)
    w1 -= learning_rate * grad_w1
    w2 -= learning_rate * grad_w2
```

<br>
<br>

### PyTorch: Autograd

- `requires_grad=True`로 텐서를 생성하면 autograd가 활성화된다.
  - 모든 연산 추적
- `requires_grad=True`가 있는 텐서에 대한 연산(operation)은 pytorch가 계산 그래프(computational graph)를 빌드한다. 
- **torch.randn(requires_grad=True)** : autograd가 반환된 텐서의 연산을 기록해야 하는 경우 쓰인다.
- **torch.Tensor.backward** : 현재 w,r,t 그래프가 남아있는 텐서의 그레이디언트를 계산한다.
  - 텐서에 대한 그레이디언트는 `.grad` 속성에 누적된다.
- **torch.autograd.grad** : 입력에 대한 출력의 그레이디언트 합계를 계산하고 반환한다.
- **torch.no_grad** : 이 부분에서는 계산 그래프를 만들지 말라는 의미이다.
- **grad.zero_** : _로 끝나는 파이토치 메서드는 텐서를 제자리에서 수정하고, 새로운 텐서를 반환하지 않는 메서드이다.

```python
import torch

N, D_in, H, D_out = 64, 1000, 100, 10
x = torch.randn(N, D_in)
y = torch.randn(N, D_out)
w1 = torch.randn(D_in, H, requires_grad=True)
w2 = torch.randn(H, D_out, requires_grad=True)

learning_rate = 1e-6
for t in range(500):
    # forward pass이지만, pytorch가 그래프에서 자동으로 추적해준다.
    y_pred = x.mm(w1).clamp(min=0).mm(w2)
    loss = (y_pred - y).pow(2).sum()

    # w1 및 w2에 대한 손실 그레이디언트 계산
    loss.backward()

    # 가중치에 gradient step을 만든 다음 0으로 만든다.
    with torch.no_grad():
        w1 -= learning_rate * w1.grad
        w2 -= learning_rate * w2.grad
        w1.grad.zero_()
        w2.grad.zero_()
```

<br>
<br>

### PyTorch: New Autograd Functions

- 텐서에 대한 정방향(forward) 및 역방향(backward) 함수를 만들어 자체 autograd 함수를 정의해본다.
- **torch.autograd.Function** : 사용자 지정 autograd.Function를 만들 기본 클래스이다.
  - 서브클래스 함수 및 **forward** 와 **backward** 메서드 구현
  - **forward** : 연산 수행 
  - **backward** : 역방향 모드 자동 미분를 하는 연산을 하는 공식을 정의
- **ctx(=context method)** 인수에 대한 적절한 메서드 호출한다.
  - **function.FunctionCtx.save_for_backward** : 나중에 backward()에 호출할 때 사용할 텐서를 저장한다.

```python
import torch

class MyReLU(torch.autograd.Function):
    @staticmethod
    def forward(ctx, x):
        ctx.save_for_backward(x)
        return x.clamp(min=0)

    @staticmethod
    def backward(ctx, grad_y):
        x, = ctx.saved_tensors
        grad_input = grad_y.clone()
        grad_input[x < 0] = 0
        return grad_input

def my_relu(x):
    return MyReLU.apply(x)

N, D_in, H, D_out = 64, 1000, 100, 10
x = torch.randn(N, D_in)
y = torch.randn(N, D_out)
w1 = torch.randn(D_in, H, requires_grad=True)
w2 = torch.randn(H, D_out, requires_grad=True)

learning_rate = 1e-6
for t in range(500):
    # 정방향에 새로운 autograd function 사용
    y_pred = my_relu(x.mm(w1)).mm(w2)
    loss = (y_pred - y).pow(2).sum()

    # w1 및 w2에 대한 손실 그레이디언트 계산
    loss.backward()

    # 가중치에 gradient step을 만든 다음 0으로 만든다.
    with torch.no_grad():
        w1 -= learning_rate * w1.grad
        w2 -= learning_rate * w2.grad
        w1.grad.zero_()
        w2.grad.zero_()
```

<br>
<br>

### PyTorch: nn

- 신경망 작업을 위한 상위 레벨 래퍼(wrapper)이다.

```python
import torch

N, D_in, H, D_out = 64, 1000, 100, 10
x = 
```



<br>
<br>
<br>
<br>

## References

[^1]: [torch.clamp](https://runebook.dev/ko/docs/pytorch/generated/torch.clamp)





