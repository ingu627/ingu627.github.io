---
layout: single
title: "[CS231n] 강의7. Training Neural Networks (1) Activation Function 리뷰"
excerpt: "본 글은 2021년 4월에 강의한 스탠포드 대학의 Convolutional Neural Networks for Visual Recognition 2021년 강의를 듣고 정리한 내용입니다. Lecture7 신경망 학습 중 활성화 함수에 대해 정리했습니다."
categories: cs231n
tag : [이미지, cnn, cs231n, 리뷰, 정리, 요약, 정의, 란, 딥러닝, 머신러닝, activation, 활성화, 함수, 활성화 함수, sigmoid, tanh, relu, leaky relu, prelu, elu, selu, maxout, swish, saturated, saturation, 의미, 설명]
toc: true 
toc_sticky: true
sidebar_main: false

last_modified_at: 2022-04-23
---

<img align='right' width='400' src='https://user-images.githubusercontent.com/78655692/164872911-dae78c5d-6dc2-4e57-9cf2-19056acd7466.png'>
본 글은 2021년 4월에 강의한 스탠포드 대학의 "Convolutional Neural Networks for Visual Recognition" 2021년 강의를 듣고 정리한 내용입니다. <br> 개인 공부 목적으로 작성되었으며, 설명이 맞지 않거나 글 오타가 있으면 알려주시길 바랍니다.
{: .notice--info}

원본 링크 : [cs231n.stanford.edu](http://cs231n.stanford.edu/)<br> 한글 번역 링크 : [aikorea.org - cs231n](http://aikorea.org/cs231n/) <br> 강의 링크 : [youtube - 2017 Spring (English)](https://www.youtube.com/playlist?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv)
{: .notice--warning}

<br>
<br>
<br>

## Mini-batch SGD

- 미니 배치 SGD의 과정은 다음과 같다.
  - **배치(batch)** : GPU가 한번에 처리하는 데이터의 묶음 [^1]

1. 배치 단위의 데이터를 샘플링한다.
2. 그래프 신경망(graph network)를 통해 정방향(forward)으로 전달하여, 손실(loss)을 얻는다.
3. 그레이디언트를 계산하는 역전파(backpropagation)
4. 그레이디언트로 파라미터 값을 업데이트한다.

- 위 과정을 반복한다.

<br>
<br>

## Training Neural Networks

- Part 1에서 다루는 내용은 다음과 같다.
  - **활성화 함수(Activation Function)**
  - **데이터 전처리(Data Preprocessing)**
  - **가중치 초기화(Weight Initialization)**
  - **배치 정규화(Batch Normalization)**
  - **전이 학습(Transfer Learning)**

<br>

## Activation Functions

<img src='https://user-images.githubusercontent.com/78655692/164880256-8011aa8e-54c5-47d8-b443-103b7a1163da.png' width=500>

- 입력 이미지가 들어와 가중치 $W$와 곱해진다.
- 이후, 활성화 함수를 통과한다.

<br>

- 다음 그림은 활성화 함수의 종류를 보여주고 있다.

<img src='https://user-images.githubusercontent.com/78655692/164880572-9236e8b8-54c2-4049-8a9a-a2d3145944ed.png' width=650>

<br>
<br>

### Sigmoid

<img src='https://user-images.githubusercontent.com/78655692/164880984-f3050944-79bc-4b0b-a696-29ebbf57dbd4.png' width=650>

- **시그모이드(Sigmoid)**는 x값을 받아서 0~1사이의 값으로 뭉갠다(squash).
- 시그모이드는 뉴런의 발사 주기(firing rate)를 해석하기에 적합하기에 역사적으로 자주 쓰였다.
  - 발사하지 않으면 0, 포화된(saturated) 발사는 1로 가정한다.

<br>
<br>

- 하지만 시그모이드는 3가지 문제점이 존재한다.

- **1. Saturated neurons kill the gradients**
  - 시그모이드는 포화가 되면 그레이디언트를 없앤다.
  - 활성화가 0 또는 1의 어느 한쪽 끝에서 포화되면, 그레이디언트는 거의 0이 된다.

<img src='https://user-images.githubusercontent.com/78655692/164882434-97d2ce7f-87d0-45cc-a4f2-a7755e5fbb77.png' width=650>

- 먼저 x=-10일 때 그레이디언트는 한없이 0에 가까워지고 시그모이드는 평평하게 된다.
- x=0일 때는 역전파가 잘 작동된다.
- x=10일 때도 x=-10일 때와 마찬가지로 그레이디언트는 한없이 0에 가까워지고 시그모이드는 평평하게 된다.

    <img src='https://user-images.githubusercontent.com/78655692/164882516-be1963e7-a1cc-4109-81e4-c32e574619e8.png' width=250>

- 결국 역방향으로 가는 모든 그레이디언트는 0이 되고 가중치는 변경되지 않을 것이다.

<br>
<br>

- **2. Sigmoid outputs are not zero-centered**
  - 뉴런의 뒤쪽 레이어 뉴런들이 0이 중심이 아닌 데이터를 얻기 때문에 경사하강법의 역학(dynamics)에 영향을 미치게 된다.

<img src='https://user-images.githubusercontent.com/78655692/164882583-ee7041b6-9d5c-48e1-b40a-d066fc24687f.png' width=650>

- 뉴런의 입력이 항상 양수이면 어떻게 될까?
- 모든 $w_i$에 대한 그레이디언트 부호는 upstream scalar gradient 부호와 동일해진다.

<br>

<img src='https://user-images.githubusercontent.com/78655692/164882712-321db404-0fc8-4d82-955f-fb0e0eab07a4.png' width=650>

- 즉, 모든 x가 양수라면 local gradient는 **전부 양수** 또는 **전부 음수**가 된다.
- 가중치에 대한 그레이디언트 업데이트를 할 때 원치 않은 지그재그 경로가 될 수 있다.
- 따라서, **입력 x가 양수와 음수를 모두 가지고 있다면, 전부 같은 방향으로 업데이트되는 일은 발생하지 않을 것이다.** [^2]

<br>
<br>

- **3. exp() is a bit compute expensive**
  - 세번째 문제점으로 지수(exponential)함수를 쓰기 때문에 계산 비용이 커진다는 점이다.
  - 하지만, 이는 그리 큰 문제는 아니다.

<br>
<br>

### tanh

<img src='https://user-images.githubusercontent.com/78655692/164882927-a4dfe311-c9b5-4508-8436-e728cef3bb1e.png' width=650>

- tanh 활성화 함수는 시그모이드와 유사하지만 -1~1의 범위를 가진다.
- 단, 주요 차이점으로 tanh는 **zero centered**이라는 점이다.
- 하지만, saturated되면 여전히 그레이디언트는 낮거나 높은 값에서 여전히 0에 가까워진다.

<br>
<br>

### ReLU

<img src='https://user-images.githubusercontent.com/78655692/164883011-66904e86-0dac-440c-80d4-be2c0197012c.png' width=650>

- 다음은 ReLU(Rectfied Linear Unit) 활성화 함수이다. 식은 $f(x)=max(0,x)$이다.
- 이 함수는 요소별 연산을 수행하면서 입력이 0미만의 음수면 0으로 출력하고, 양수이면 출력은 입력값 그대로 출력한다.
- sigmoid, tanh와 비교해보았을 때 ReLU는
  1. 양수 범위에서는 saturation되지 않는다.
  2. 계산 효율이 뛰어나다. (단순 연산)
  3. 실제로 sigmoid, tanh보다 수렴속도가 약 6배 더 빠르다.

- 실제로 신경과학적 실험을 통해 뉴런을 보면 ReLU가 더 잘 맞았다.

<br>

<img src='https://user-images.githubusercontent.com/78655692/164883755-bf65abc6-83c4-4612-b8cd-07731fa6069a.png' width=650>

- 하지만, **zero-centered**한 문제는 풀지 못했다.
- ReLU는 음수 범위에서는 모두 0이 되기 때문에 그레이디언트의 절반을 죽인다.
- 그래서 업데이트 하지 못하는 dead ReLU 현상이 나타난다. 

<br>

- 그래서 실제로 ReLU를 초기화할 때 positive biases(0.01)를 추가해 주기도 한다.
  - 이는 가중치를 업데이트 할 때 active ReLU가 될 가능성을 높여주기 위함이다.

<br>
<br>

### Leaky ReLU

<img src='https://user-images.githubusercontent.com/78655692/164883885-25fbb3f9-3be7-4de0-b30b-ff518416c187.png' width=650>

- Leaky ReLU는 ReLU에서 살짝 수정된 버전이다.
- 기울기를 줌으로써 음의 영역에도 더 이상 0이 아니게 되었다. 
  - 따라서, 음의 영역에도 saturation되지 않는다.
- 나머지 특징은 ReLU와 똑같지만, dead ReLU 현상이 없어진게 ReLU와 차이점이다.

<br>

- **PReLU**는 기울기를 $\alpha$로 결정한다. 이는 역전파를 통해 학습이 된다.

<br>
<br>

### ELU

<img src='https://user-images.githubusercontent.com/78655692/164884017-ab34cf6b-c747-41a2-895f-c458e0884a98.png' width=650>

- **ELU(Exponential Linear Units)**는 ReLU의 이점들을 모두 가져온다.
- zero-centered 특성에 가까워진다.
- 하지만, Leaky ReLU와 비교해볼 때 ELU는 음의 영역에서 saturation된다.
- 하지만 이런 saturation이 노이즈에 강인할 수 있다고 한다.
- 또한, exp() 연산이 필요하다는 단점이 있다.

<br>
<br>

### SELU

<img src='https://user-images.githubusercontent.com/78655692/164884173-efc99f93-83a8-4658-a536-6df4f69bc59c.png' width=650>

- **SELU(Scaled Exponential Linear Units)**은 심층 신경망의 **자기 정규화(Self-normalizing)**에 더 잘 작동하는 ELU의 확장 버전이다.
- 배치 정규화(Batch Normalization)없이 심층 SELU 신경망을 학습할 수 있다(고 한다).

<br>
<br>

### Maxout

<img src='https://user-images.githubusercontent.com/78655692/164884278-03227576-2094-49c7-9222-a18db0ed19aa.png' width=650>

- 이 활성화 함수에서는 내적의 기본적인 형태를 미리 정의하지 않는다.
- 대신에, $max(w_1^Tx+b_1, w_2^Tx+b_2)$를 사용한다.
- **maxout**은 둘 중에 최대값을 취한다.
  - 이는 saturation되지 않고, 그레이디언트를 잘 계산할 수 있다.
- 하지만, 파라미터의 수가 두배가 되는 문제점이 있다.

<br>
<br>

### Swish

<img src='https://user-images.githubusercontent.com/78655692/164884489-0826b103-83f3-4d54-acab-db779c22799c.png' width=650>

- 다음은 **Swish** 활성화 함수이다.
- 서론 다른 비선형성을 생성하고 테스트하기 위해 신경망을 학습시킨다.
- 깊은 레이어를 학습시킬 때 ReLU보다 더 뛰어난 성능을 보인다고 한다. [^3]


<br>
<br>

### Summary

- Default는 **ReLU**를 쓴다. (하지만 학습률을 잘 따져본다.)
- **Leaky ReLU/Maxout/ELU/SELU** 등을 시도해본다.
- **sigmoid/tanh**는 쓰지 마라!

<br>
<br>
<br>
<br>

## References

[^1]: [배치와 미니 배치, 확률적 경사하강법 (Batch, Mini-Batch and SGD) - skyil](https://skyil.tistory.com/68)
[^2]: [Lecture 6: Training Neural Networks I - 조마조마](https://gjghks.tistory.com/77?category=887163)
[^3]: [갈아먹는 딥러닝 기초 [1] Activation Function(활성화 함수) 종류 - 염창동형준킴](https://yeomko.tistory.com/39)

