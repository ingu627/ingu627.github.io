---
layout: single
title: "[CS231n] 강의7. Training Neural Networks (2) Weight Initialization, Batch Normalization, Transfer Learning 리뷰"
excerpt: "본 글은 2021년 4월에 강의한 스탠포드 대학의 Convolutional Neural Networks for Visual Recognition 2021년 강의를 듣고 정리한 내용입니다. Lecture7 신경망 학습 중 데이터 전처리부터 전이 학습까지에 대해 정리했습니다."
categories: cs231n
tag : [이미지, cnn, cs231n, 리뷰, 정리, 요약, 정의, 란, 딥러닝, 머신러닝, 가중치 초기화, 활성화, xavier, kaiming, he, initialization, 배치 정규화, 전이 학습, 의미]
toc: true 
toc_sticky: true
sidebar_main: true

last_modified_at: 2022-04-25
---

<img align='right' width='400' src='https://user-images.githubusercontent.com/78655692/164872911-dae78c5d-6dc2-4e57-9cf2-19056acd7466.png'>
본 글은 2021년 4월에 강의한 스탠포드 대학의 "Convolutional Neural Networks for Visual Recognition" 2021년 강의를 듣고 정리한 내용입니다. <br> 개인 공부 목적으로 작성되었으며, 설명이 맞지 않거나 글 오타가 있으면 알려주시길 바랍니다.
{: .notice--info}

원본 링크 : [cs231n.stanford.edu](http://cs231n.stanford.edu/)<br> 한글 번역 링크 : [aikorea.org - cs231n](http://aikorea.org/cs231n/) <br> 강의 링크 : [youtube - 2017 Spring (English)](https://www.youtube.com/playlist?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv)
{: .notice--warning}

<br>
<br>
<br>

## Training Neural Networks

- 활성화 함수에 이어 Part 1의 내용을 다뤄본다.
  - ~~**활성화 함수(Activation Function)**~~ **(Completed)**
  - **데이터 전처리(Data Preprocessing)**
  - **가중치 초기화(Weight Initialization)**
  - **배치 정규화(Batch Normalization)**
  - **전이 학습(Transfer Learning)**

<br>
<br>

## Data Preprocessing

<img src='https://user-images.githubusercontent.com/78655692/164887698-79895814-800e-42ac-bfaa-d2db1c3d4ccd.png' width=650>

- 입력 데이터(original data)를 학습하려면 **전처리(preprocessing)** 과정이 필요하다.
- 2번째 경우를 **평균 차감(Mean Subtraction)**이라고 하는데, 데이터의 모든 특징(feature)에 각각에 대해서 평균값 만큼 차감하는 방법이다.
  - 데이터 군집을 모든 차원에 대하여 원점으로 이동시킨다.
- 다음 3번째 경우를 **정규화(Normalization)**이라고 하는데, 각 차원의 데이터가 동일한 범위내의 값을 갖도록 하는 전처리 기법을 의미한다.
  - **Before normalization** : 분류 손실(classification loss)은 가중치 행렬의 변화에 매우 민감하며, 최적화하기 어렵다.
  - **After normalization** : 가중치의 작은 변화에 덜 민감하고, 최적화하기 쉽다. 
  - **정규화**를 하는 이유는 모든 차원이 동일한 범위를 갖게 함으로써 전부 동등한 **기여(contribution)**를 할 수 있게 해주기 위해서다. 

    <img src='https://user-images.githubusercontent.com/78655692/164888074-8c7bb37a-e870-44f3-a9c2-cdbf8b5a28cb.png' width=450>

    <br>

- 입력 데이터가 이미지일 경우 zero-centered만 해줘도 된다.
  - 이미지는 이미 각 차원 간의 크기(scale)가 맞추어져있기 때문이다.
- 일반적으로 이미지를 다룰 때는 PCA같이 더 낮은 차원으로 정사영(projection)시키지 않는다.
- **CNN에서는 원본 이미지(original data) 자체의 공간 정보를 활용해서 이미지의 공간 구조를 확보할 수 있다.**

<br>
<br>

## Weight Initialization

<img src='https://user-images.githubusercontent.com/78655692/164888156-bbb4c52b-8ac5-4e74-9ece-814e70f01d41.png' width=650>

- 우리가 할 일은 그레이디언트를 계산해서 가중치 행렬 $W$의 값을 업데이트하는 것이다.
- 이때, 모든 가중치를 0으로 하면 동일한 연산을 수행하여 출력, 그레이디언트 모두 동일한 값으로 업데이트 될 것이다.
  - 즉, 바뀌는 건 아무것도 없다.
- $W_{i+1}=W_i-\gamma \nabla F(W_i)$
  - $\nabla F(W_i)=0$ when $W_i$ is initialized zero

<br>

<img src='https://user-images.githubusercontent.com/78655692/164888233-b6c5af9e-39d2-491a-b21f-189764ab18ed.png' width=450>

- 가중치 초기화를 해결하는 첫번째 방법으로 임의의 랜덤 값으로 초기화하는 것이다.
  - 가우시안(gaussian)에서 샘플링한다.
- 이 경우, 작은 신경망에서는 잘 작동하지만, 깊어질수록 문제가 발생한다.

<br>
<br>

### Activation Statistics

- 다음과 같은 코드로 실행했을 때 **활성화 통계(activation statistics)**를 보여준다.

```python
dims = [4096] * 7
hs = []
x = np.random.randn(16, dims[0])
for Din, Dout in zip(dims[:-1], dims[1:]):
    W = 0.01 * np.random.randn(Din, Dout)
    x = np.tanh(x.dot(W))
    hs.append(x)
```

<img src='https://user-images.githubusercontent.com/78655692/164888360-20c01825-22b9-4782-9a12-dc84fabb00b6.png' width=750>

- 모두 0가 되어 학습을 할 수 없게 된다.
- **Initialization too small!**

<br>

- 만약 0.01 부분을 0.05로 바꾸면 어떻게 될까?

<img src='https://user-images.githubusercontent.com/78655692/164888413-f6598e27-54d2-4eba-ab02-3ecc32a6ac48.png' width=750>

- local gradient가 모두 0이 되어 학습을 할 수 없게 된다.
- **Initialization too big!**

<br>
<br>

### Xavier Initialization

- 그래서 어떻게 하면 가중치를 잘 초기화할 수 있을까?
- **Xavier Initialization** 방법이 있다.
  - 공식 : **std=1/sqrt(Din)**
  - 컨볼루션 레이어에서 Din은 $filterSize^2\times inputChannels$ 이다.

```python
dims = [4096] * 7
hs = []
x = np.random.randn(16, dims[0])
for Din, Dout in zip(dims[:-1], dims[1:]):
    W = np.random.randn(Din, Dout) / np.sqrt(Din)
    x = np.tanh(x.dot(W))
    hs.append(x)
```

<img src='https://user-images.githubusercontent.com/78655692/164888665-141cd6d6-f005-4bfe-b4af-86af5b901de5.png' width=750>

- 잘 작동하는 것을 확인할 수 있다.
- **Initialization just right!**

<br>
<br>

### Kaiming / MSRA Initialization

- 하지만, **ReLU**에서는 이것이 잘 작동하지 않는다.

```python
dims = [4096] * 7
hs = []
x = np.random.randn(16, dims[0])
for Din, Dout in zip(dims[:-1], dims[1:]):
    W = np.random.randn(Din, Dout) / np.sqrt(Din)
    x = np.maximum(0, x.dot(W))
    hs.append(x)
```

<img src='https://user-images.githubusercontent.com/78655692/164888807-eef1898f-c855-41c6-8349-c95c92296a7e.png' width=750>

<br>
<br>

- 어떻게 해야 되나?
- Kaiming의 **He Initialization**을 쓴다!!
  - 공식 : **std=sqrt(2/Din)**

```python
dims = [4096] * 7
hs = []
x = np.random.randn(16, dims[0])
for Din, Dout in zip(dims[:-1], dims[1:]):
    W = np.random.randn(Din, Dout) / np.sqrt(2/Din)
    x = np.maximum(0, x.dot(W))
    hs.append(x)
```

<img src='https://user-images.githubusercontent.com/78655692/164888871-512ea519-86ce-43a1-aade-12c46e87dd63.png' width=750>

- 잘 작동하는 것을 확인할 수 있다.

<br>
<br>

## Batch Normalization

- **배치 정규화(Batch Normalization)**는 레이어로부터 나온 배치 단위만큼의 활성화가 있을 때, 이 값들이 단위 가우시안(unit gaussian)이기를 원할 때 쓰인다. 
  - 각 차원을 평균이 0, 단위 분산으로 만들기 위해서 다음을 적용
  - $\hat x^{(k)}=\frac{x^{(k)}-E[x^{(k)}]}{\sqrt{Var[x^{(k)}]}}$
- 배치 단위로 한 레이어에 입력으로 들어오는 모든 값들을 이용해서 평균(mean)과 분산(variance)를 계산해 정규화(normalization)를 해준다.

<br>

<img src='https://user-images.githubusercontent.com/78655692/164889626-0cdd59c6-4993-4e78-aabc-13eb2d101f7e.png' width=650>

- 배치당 NxD 형태의 x가 있다고 하자.
  - 정확히는 N개의 학습 데이터가 있고 각 데이터는 D차원이다.
- 각 차원별로 평균(mean)을 각각 계산한다.
- 하나의 배치 내에서 전부 계산하여 정규화한다.
- 따라서 정규화된 x는 다음과 같다.
  - $\hat x_{i,j}=\frac{x_{i,j-\mu_j}}{\sigma_j^2+\epsilon}$
  - 여기서 $\epsilon$은 계산할 때 0으로 나눠지는 문제가 발생하는 것을 막기 위한 수치적 안정성을 보장하기 위한 아주 작은 숫자이다. [^1]
- 즉, 각 차원에 대한 경험적 평균과 분산을 독립적으로 계산한다.

<br>
<br>

- 하지만 평균이 0, 단위 분산이 너무 어려운 제약 조건이면 어떻게 해야 될까?

<img src='https://user-images.githubusercontent.com/78655692/164889799-d786852a-f07f-404a-8642-d50e313c5916.png' width=650>

- 입력 데이터 $x$는 $N\times D$
- 학습가능한 매개변수인 $\gamma$와 $\beta$를 사용한다.
  - $\gamma$ : $\sqrt{Var(x)}$ (Scale에 대한 값)
  - $\beta$ : $E(x)$ (Shift Transform에 대한 값)
- 중간체(intermediates)
  - $\mu,\sigma : D$
  - $\hat x : N\times D$
- 출력 데이터 $y$는 $N\times D$
- 데이터를 계속 정규화하게 되면 활성화 함수의 비선형 특성을 잃게 되는데, 이것들을 연산하면 이런 문제를 완화할 수 있다. [^1]

<br>

- 보통 배치 정규화는 다음 순서로 쓴다.
  - FC $\to$ **BN** $\to$ tanh $\to$ FC $\to$ **BN** $\to$ tanh ...

<br>
<br>

### Pros and cons

- **Pros**
  - 심층 신경망을 훨씬 더 쉽게 학습할 수 있다.
  - 그레이디언트 흐름을 개선시켜준다.
  - 더 높은 학습률(learning rate)과 더 빠른 수렴(convergence)를 가능하게 해준다.
  - 신경망은 초기화에 더욱 강건해진다.
  - 학습 중 규제(regularization) 역할 가능
  - 테스트 시 오버헤드가 없어 합성곱 층과 잘 융화할 수 있다.

- **Cons**
  - 학습과 테스트 중 다르게 행동하여 에러가 날 수 있다.

<br>
<br>

## Transfer learning

- **전이 학습(transfer learning)** : imageNet과 같이 아주 큰 데이터셋에 훈련된 모델의 가중치를 가지고 와서 우리가 해결하고자 하는 과제에 맞게 재보정해서 사용하는 것을 말한다. [^2]
- **low level** : edge, color...
- **high level** : special class, classifcation

<img src='https://user-images.githubusercontent.com/78655692/164890319-65f475f3-efc1-441f-82da-10a176eceaea.png' width=650>

- 1번은 imageNet에 학습시킨 cnn이다.
- 만약, c 클래스를 가진 작은 데이터셋을 학습시키려면 2번과 같이 `FC-C`부분을 재초기화(reinitialize)한후 학습한다.
- 만약, 더 큰 데이터셋을 학습시키려면 3번과 같이 `FC-4096`x2, `FC-C` 부분을 재초기화해서 학습을 진행한다.


<br>
<br>
<br>
<br>

## References

[^1]: [AI Research Topic/Deep Learning[Deep Learning] Batch Normalization (배치 정규화) - Enough is not enough](https://eehoeskrap.tistory.com/430)
[^2]: [전이학습(transfer learning) 재밌고 쉽게 이해하기 - bskyvision](https://bskyvision.com/698)

