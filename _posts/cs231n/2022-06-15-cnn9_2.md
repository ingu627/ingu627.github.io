---
layout: single
title: "[CS231n] 강의11 Attention and Transformers (2) 리뷰"
excerpt: "본 글은 2022년 5월에 강의한 스탠포드 대학의 Attention and Transformers 2022년 강의를 듣고 정리한 내용입니다. attention, self-attention, transformer 등이 그 예입니다."
categories: cs231n
tag : [이미지, cnn, cs231n, 리뷰, 정리, 요약, 정의, 란, 딥러닝, 설명, 원리, transformer, 트랜스포머, attention, image caption, self attention, encoder, decoder, multi head, positional encoding, attention is all you need, vision]
toc: true 
toc_sticky: true
sidebar_main: false

last_modified_at: 2022-06-15
---

<img align='right' width='400' src='https://user-images.githubusercontent.com/78655692/173409001-fde6002c-8070-42b3-8906-2679a8e9f801.png'>
본 글은 2022년 5월에 강의한 스탠포드 대학의 Attention and Transformers 2022년 강의를 듣고 정리한 내용입니다. <br> 개인 공부 목적으로 작성되었으며, 설명이 맞지 않거나 글 오타가 있으면 알려주시길 바랍니다.
{: .notice--info}

원본 링크 : [cs231n.stanford.edu](http://cs231n.stanford.edu/)<br> 한글 번역 링크 : [aikorea.org - cs231n](http://aikorea.org/cs231n/) <br> 강의 링크 : [youtube - 2017 Spring (English)](https://www.youtube.com/playlist?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv)
{: .notice--warning}

<br>
<br>
<br>

## Attention is all you need

<img src='https://user-images.githubusercontent.com/78655692/173737236-7fbd0a7e-8b7c-44c2-a10b-69b05a12ce63.png' width=700>

- 2017년 google에서 발표한 transformer는 rnn이나 lstm을 사용하지 않고 attention 만으로도 자연어 처리 모델을 학습할 수 있음을 보여주었다.
- transformer 모델은 encoder-decoder 구조뿐만 아니라, self-attention, multi-head attention, positional encoding 등이 나오는데, 이전 글에서 다룬 적이 있다.
  - [[CS231n] 강의11 Attention and Transformers (1) 리뷰](https://ingu627.github.io/cs231n/cnn9/)
- "attention is all you need" 논문 리뷰는 추후에 올리도록 하겠다.

<br>
<br>

## Image Captioning using Transformers

<img src='https://user-images.githubusercontent.com/78655692/173739271-a49bf918-ed7c-485f-997e-d513ba754dad.png' width=700>

- attention 원리를 이해했다면 위 그림도 쉽게 이해가 될 것이다.
- one-to-many 모델로, encoder에서 입력은 이미지 $I$, 출력은 시퀀스 $y=y_1,y_2,...,y_T$가 될 것이다.
- $c=T_w(z)$은 encoder 식이다.
  - $c$ : context vector의 약어
    - **context vector** : 인코더가 입력 이미지 $I$를 받아 이해하고 만든 결과를 벡터화 [^1]
  - $T_w(.)$ : transformer encoder 모델
  - $z$ : spatial CNN features

<br>

<img src='https://user-images.githubusercontent.com/78655692/173747198-fcb013d8-9d4c-4955-9497-6719b6a7eaa9.png' width=700>

- decoder는 encoder에서 생성한 context vector와 출력 시퀀스 $y_t$ 정보를 이용해 새로운 시퀀스를 내보내는 과정이다.
- $y_t=T_D(y_{0:t-1}, c)$은 decoder 식이다.
  - $T_D(.)$ : transformer decoder 모델
- 출력 시퀀스에서 [START]와 [END]를 지정해 한 블록 단위가 언제 시작하고 언제 끝나는지를 명시한다.
  - [END] 표시가 나올 때까지 decoder가 작동된다.

<br>
<br>

### The Transformer encoder block

<img src='https://user-images.githubusercontent.com/78655692/173765563-2591c821-b0b0-431c-bfa9-961a707a01f5.png' width=700>

- transformer의 encoder를 자세히 살펴본다.
- N개의 encoder 블록을 만들어 놓는다.
  - In vaswani et al. 논문에서는 $N=6, D_q=512$로 설정했다.
- 입력 시퀀스의 벡터화는 위치 정보를 포함하기 위해 **positional encoding**를 거친다.
- **Multi-head self-attention**에서 **Attention**이 모든 벡터들을 어텐션한다.
  - 이 방식은 학습 파라미터 수를 늘려 모델의 복잡도를 올리는 방법이다. [^2]
- **Residual connection**을 통해 computational cost를 줄이고, 속도도 빠르게 forward 한다.
- 각 벡터마다 **레이어 정규화(Layer Normalization)**를 적용한다.
- **다층 퍼셉트론(MLP)**이 각 벡터마다 적용된다.
- 그 전에 다시 **Residual connection**을 거쳐 성능을 향상시킨다.

<br>

<img src='https://user-images.githubusercontent.com/78655692/173768726-67d448da-6374-4b41-a18a-01e9a995edc0.png' width=700>

- transformer encoder block이 반복되는데, 여기서 각 레이어는 서로 다른 파라미터를 가진다.
- transformer encoder는 높은 확장성(scalable), 높은 병렬성(parallel), 그렇지만 높은 메모리 사용량(memory usage)이 특징이다.

<br>
<br>

### The Transformer decoder block

<img src='https://user-images.githubusercontent.com/78655692/173773422-9870c894-e1f1-4ef3-9ca2-b2aa2e152919.png' width=700>

- 다음은 트랜스포머의 decoder block 이다.
- 구조를 보면 알겠지만, 대부분의 레이어가 encoder와 거의 같다는 걸 확인할 수 있다.

<br>

<img src='https://user-images.githubusercontent.com/78655692/173773776-ccf718b7-b351-4a9d-94fb-1978c55db64c.png' width=700>

- decoder에서 **multi-head attention** block은 transformer encoder에서 나온 출력값(output)들을 모두 어텐션한다.
  - 출력값은 context vectors이다.
  - 여기서, key(k), value(v), query(q)가 작동한다.
  - 즉, decoder의 multi-head attention은 self-attention가 아니다.

<br>

<img src='https://user-images.githubusercontent.com/78655692/173777420-6494bb2d-6ad0-40c2-bfdd-96a3fed4bc92.png' width=700> <br> 이미지출처 [^3]

- 위 그림은 multi-head attention 구조를 좀 더 구제적으로 나타낸 것이다.
- scaled dot-product attendtion = $Attention(Q, K, V)=$$softmax(\sqrt{\frac{QK^T}{\sqrt{d_k}}})V$

    <img src='https://user-images.githubusercontent.com/78655692/173779007-3d56a8da-72f8-447d-ac26-c7dfa692f142.png' width=400> <br> 이미지출처 [^3]

<br>
<br>

## Comparing RNNs to Transformer

- transformer를 RNN과 비교해보면 다음과 같다.

### RNN

- **Pros**
  - LSTM은 긴 문장인 입력에 잘 작동한다.
- **Cons**
  - 순서가 지정된 입력의 순서를 예상한다.
  - sequential computation : 이후의 hidden 상태는 이전 값이 끝난 후에만 계산할 수 있다.

### Transformer

- **Pros**
  - 긴 시퀀스에서 잘 작동한다. 각 attention 계산은 모든 입력을 살펴본다.
  - positional encoding을 이용하여 순서가 정해져있거나 정해져있지 않은 시퀀스에서 잘 작동한다.
  - parallel computation : 모든 입력에 대한 alignment와 attention은 병렬로 잘 수행된다.
- **Cons**
  - 많은 메모리가 필요하다. NxM의 alignment와 attention 스칼라가 계산되어야 하고 단일 self-attention head가 저장되어야 한다.
    - 하지만, GPU가 크면 클수록 이 단점을 보완할 수 있다.

<br>
<br>

## Image Captioning using ONLY transformers

<img src='https://user-images.githubusercontent.com/78655692/173780166-b5b2f1d9-d29a-454e-8baa-2ace04542441.png' width=700>

- transformer를 사용하면 CNN을 굳이 안 써도 된다.

<br>

<img src='https://user-images.githubusercontent.com/78655692/173780339-a2a30f45-57ba-4a9f-a759-48fc4ad64a43.png' width=700>

- 이미지 픽셀을 나눠 자연어 처리하듯이 하면 된다.
- 말 그대로 **attention** 만 사용하면 된다.!!

<br>

<img src='https://user-images.githubusercontent.com/78655692/173780715-cdef69b2-c389-4515-8a72-a27bc02710b8.png' width=700>

- 위 실험 결과를 봐도 알 수 잇듯이 이미 vision transformer(ViT)는 기존 CNN 아키텍처의 성능을 앞질렀다.

<br>

<img src='https://user-images.githubusercontent.com/78655692/173781012-5b3da94f-5dbc-41f4-a2b7-9a5fc3ac8a4a.png' width=700>

- Vision Transformer의 다양한 형태가 나오고 있으며, 굉장히 핫하게 연구되고 있다.

<br>

<img src='https://user-images.githubusercontent.com/78655692/173781254-7fac71e6-ca2a-4bf9-a79b-f57b344a35a2.png' width=700>

- 위 실험 결과는 ViT 중 SOTA를 기록한 DeiT이다. CNN의 SOTA인 EfficientNet보다 성능이 월등히 높은 걸 알 수 있다.

<br>
<br>
<br>
<br>

## References

[^1]: [Attention Mechanism이란? - inistory](https://inistory.tistory.com/123)
[^2]: [[머신러닝/딥러닝] 10-3. Transformer Model - 손쓰](https://sonsnotation.blogspot.com/2020/11/10-3-transformer-model.html)
[^3]: [Deep Learning Bible - wikidocs](https://wikidocs.net/162098)

