---
layout: single
title: "[CS231n] 강의4. Neural Networks and Backpropagation 리뷰"
excerpt: "본 글은 2021년 4월에 강의한 스탠포드 대학의 Convolutional Neural Networks for Visual Recognition 2021년 강의를 듣고 정리한 내용입니다. Lecture4 신경망과 역전파 알고리즘에 대해 정리했습니다."
categories: cs231n
tag : [이미지, cnn, cs231n, 리뷰, 정리, 요약, 정의, 란, 역전파, backpropagation, 신경망, max, 활성화, 함수, activation, relu, 시그모이드, 연쇄법칙, 미분, 도함수, 딥러닝, 머신러닝, 의미]
toc: true 
toc_sticky: true
sidebar_main: true

last_modified_at: 2022-04-24
---

<img align='right' width='400' src='https://user-images.githubusercontent.com/78655692/164648045-e74a8d37-4054-471a-aa68-8dda5e7ac374.png'>
본 글은 2021년 4월에 강의한 스탠포드 대학의 "Convolutional Neural Networks for Visual Recognition" 2021년 강의를 듣고 정리한 내용입니다. <br> 개인 공부 목적으로 작성되었으며, 설명이 맞지 않거나 글 오타가 있으면 알려주시길 바랍니다.
{: .notice--info}

원본 링크 : [cs231n.stanford.edu](http://cs231n.stanford.edu/)<br> 한글 번역 링크 : [aikorea.org - cs231n](http://aikorea.org/cs231n/) <br> 강의 링크 : [youtube - 2017 Spring (English)](https://www.youtube.com/playlist?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv)
{: .notice--warning}

<br>
<br>
<br>

## Neural Networks

- **신경망(Neural Networks)** : 인공 뉴런이나 노드로 구성된 인공 신경망 [^1]

1. **Linear score function**
  - $f=Wx$
2. **2-layer Neural Network**
  - $f=W_2max(0, W_1x)$

    <img src='https://user-images.githubusercontent.com/78655692/164676159-ba3e2cfd-3111-41f1-be84-2a53c7d8dc3a.png' width=500>

    - 여기서 비선형 함수인 $max(0,x)$를 이용한다. 
    - 만약, 비선형 변환없이 선형 함수만 계속 추가되면 다시 입력의 선형 함수가 되어버린다.

    <img src='https://user-images.githubusercontent.com/78655692/164676985-ab7302ea-8023-4ae3-b514-f100bde00e16.png' width=500>

    - 기존 linear layer 행렬곱에 의한 스코어 함수 $f=Wx$에서 가중치 행렬 $W$의 각 행이 클래스 각각의 탬플릿과 비슷한지 비교하였다.
    - 하지만, 각 클래스마다 오직 하나의 템플릿만 가지고 있는데, 다중 레이어 신경망은 다양하게 이미지를 분류할 수 있게 해준다.


3. **3-layer Neural Network**
  - $f=W_3max(0, W_2max(0,W_1x))$

<br>
<br>

### Activation functions

- **활성화 함수(activation function)** : 입력된 데이터의 가중 합을 출력 신호로 변환하는 함수 [^2]
  - 이전 레이어에 대한 가중 합의 크기에 따라 활성 여부가 결정된다.
- 다음은 활성화 함수의 종류이다.

<img src='https://user-images.githubusercontent.com/78655692/164677574-a466cf0d-b794-4bcb-aefa-55ebb9d74e87.png' width=650>

<br>
<br>

### Example feed-foward computation of a neural network

<img src='https://user-images.githubusercontent.com/78655692/164678777-66c18881-5ff9-49a6-9750-6c39ca27c7c6.png' width=500>

```python
import numpy as np

f = lambda x: 1.0/(1.0+np.exp(-x)) # 활성화 함수 정의(시그모이드)
x = np.random.randn(3, 1)
h1 = f(np.dot(W1, x) + b1)
h2 = f(np.dot(W2, h1) + b2)
out = np.dot(W3, h2) + b3
```

<br>

- 다음은 계산 과정의 전반적인 그림이다.

<img src='https://user-images.githubusercontent.com/78655692/164680494-a0550988-eabf-4cf2-bf03-7d48bd7e2320.png' width=550>

> 여기서 **편향(bias)**를 추가해 주는 이유는 파라미터의 선호도를 조절해주기 위해서이다.

<br>

### Setting the number of layers and their sizes

<img src='https://user-images.githubusercontent.com/78655692/164680162-328d4d54-809b-4b65-a5dc-26321275852b.png' width=650>

<br>

### Types of regularization

<img src='https://user-images.githubusercontent.com/78655692/164680340-ac21cae2-58f8-47b1-b0ef-377eae4f0a21.png' width=650>

<br>
<br>

## Backpropagation

- 이제 어떻게 그레이디언트를 계산할 것인지가 남아 있다.
- Analytic gradient를 계산할 때 computational graph를 이용하면 어떠한 함수도 표현이 가능하다.

> 미분은 각 변수가 해당 값에서 전체 함수(expression)의 결과 값에 영향을 미치는 민감도와 같은 개념이다.

<img src='https://user-images.githubusercontent.com/78655692/164698481-596f561d-abf5-4c59-8169-b537f2be7668.jpg' width=650>

- 초록색 숫자는 피드포워드 값, 빨간색 숫자는 미분값이다.
- $\frac{\partial f}{\partial y}$는 **연쇄법칙(chain-rule)**을 이용해 계산할 수 있다.
- $\frac{\partial f}{\partial x}$는 **연쇄법칙(chain-rule)**을 이용해 계산할 수 있다.

<br>

<img src='https://user-images.githubusercontent.com/78655692/164699177-d9800903-4364-4bc9-be5b-085e25090414.png' width=650>

- 위 그림과 같이 각 노드가 local gradient를 갖고 있으며, backpropagation을 통해 상위 노드 방향으로 전달되고, 이것을 이전 노드가 전달받아 그 노드의 local gradient와 곱해주면서 나아간다. 
- **upstream gradient** : 노드의 output에 대한 gradient (이미 계산)
- **local gradient** : 해당 노드내에서만 계산되는 gradient
- **downstream gradient** : 노드의 input에 있는 변수에 대한 gradient [^3]

<br>

- 다음은 역전파를 계산하는 다른 예시이다.

![lecture_4-84](https://user-images.githubusercontent.com/78655692/164878929-4d99f55e-2890-43fe-b250-f6160bf95c22.jpg)

1. **1/x gate**

   - $f(x)=1$ $\to$ $\frac{df}{dx}=-1/x^2$
   - $(1.00)(\frac{-1}{1.37^2})=-0.53$

2. **+1 gate**

   - $f_c(x)=c+x\to$ $\frac{df}{dx}=1$
   - $(-0.53)(1)=0.53$

3. **exp gate**

   - $f(x)=e^x\to$ $\frac{df}{dx}=e^x$
   - $(-0.53)(e^{-1})=-0.20$

4. **\*-1 gate**

   - $f_a(x)=ax\to$ $\frac{df}{dx}=a$
   - $(-0.20)(-1)=0.20$

5. **+ gate**

   - $(0.20)(1.00)=0.20$
   - $(0.20)(1.00)=0.20$

6. **+ gate**

   - $(0.20)(1.00)=0.20$
   - $(0.20)(1.00)=0.20$

7. **\* gate**

   - $w_0=(0.20)(-1.00)=-0.20$
   - $x_0=(0.20)(2.00)=0.40$

8. **\* gate**

   - $w_1=(0.20)(-2.00)=-0.40$
   - $x_1=(0.20)(-3.00)=-0.60$

<br>
<br>

### Patterns in gradient flow

1. **add gate** : 항상 출력의 미분을 전방 전달(forward pass)에 상관없이 모든 입력에 균등하게 분배한다.

    <img src='https://user-images.githubusercontent.com/78655692/164703747-6c4555b0-ecf9-4b9a-be33-3c4d8105d5c6.png' width=350>

2. **mul gate** : local gradient는 입력 값이고 이는 chain rule 동안 출력에 대한 그레이디언트로 곱해진다.

    <img src='https://user-images.githubusercontent.com/78655692/164704080-06e44acf-6f39-4bee-9d46-7e3ecec2b398.png' width=350>

   - 이 과정을 forward와 backward API로 직접 구현하면 다음과 같다. (Pytorch)

    <img src='https://user-images.githubusercontent.com/78655692/164878381-7bb5f4a2-b0b5-4396-a748-8349d8bcdcd4.png' width=250>

    ```python
    class Multiply(torch.autograd.Function):
      @staticmethod
      def forward(ctx, x, y):
        # backward에서 사용하기 위해 일부 값을 저장해야 한다.
        ctx.save_for_backward(x, y) 
        z = x * y
        return z 
      @staticmethod
      def backward(ctx, grad_z): # upstream gradient
        x, y = ctx.saved_tensors
        # upstream gradient와 local gradient를 곱셈
        grad_x = y * grad_z # dz/dx * dL/dz
        grad_y = x * grad_z # dz/dy * dL/dz
        return grad_x, grad_y
    ```


3. **max gate** : 미분을 정확히 하나의 입력(앞으로 전방 전달하는 동안 가장 높은 값을 가진 입력)으로 분배한다. 

    <img src='https://user-images.githubusercontent.com/78655692/164704500-c64777fc-dcc7-4374-8b42-d2902fcb4ded.png' width=350>

<br>
<br>

### Gradients add at branches

   <img src='https://user-images.githubusercontent.com/78655692/165063183-4520b0f5-7e10-43cf-9292-ca8905bece68.png' width=350>

- 만약 그레이디언트가 양쪽에 있다면, 각각을 더해주기만 하면 된다.

<br>
<br>

## Summary

- **완전 연결망(Fully-connected) Neural Networks** : 선형 함수와 비선형 함수의 스택(stack)으로, 선형 분류기보다 훨씬 많은 표현력(representational power)를 가지고 있다.
- **역전파(backpropagation)** : 모든 입력(input)/매개변수(parameter), 그 사이의(intermediate)의 그레이디언트(gradient)를 계산하기 위해 계산 그래프를 따라 연쇄규칙(chain-rule)를 재귀적(recursive)으로 적용한다.
- 구현은 노드가 정방향(forward), 역방향(backward) API를 구현하는 그래프 구조를 유지한다.
- **정방향(forward)** : 연산 결과를 계산하고 그 사이사이 계산에 필요한 값을 메모리에 저장한다.
- **역방향(backward)** : 연쇄규칙(chain-rule)을 적용하여 입력에 대한 손실 함수(loss function)의 그레이디언트를 계산한다.




<br>
<br>
<br>
<br>

## References

[^1]: [신경망 - 위키백과](https://ko.wikipedia.org/wiki/%EC%8B%A0%EA%B2%BD%EB%A7%9D)
[^2]: [활성화함수 - CoDOM](http://www.incodom.kr/%ED%99%9C%EC%84%B1%ED%99%94%ED%95%A8%EC%88%98)
[^3]: [한 문장으로 정리하는 computer vision (5) - opcho](https://velog.io/@opcho/%ED%95%9C-%EB%AC%B8%EC%9E%A5%EC%9C%BC%EB%A1%9C-%EC%A0%95%EB%A6%AC%ED%95%98%EB%8A%94-computer-vision-5)