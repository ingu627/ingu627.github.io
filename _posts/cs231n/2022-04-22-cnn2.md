---
layout: single
title: "[CS231N] 강의3. Loss Functions and Optimization (1) 리뷰"
excerpt: "본 글은 2021년 4월에 강의한 스탠포드 대학의 Convolutional Neural Networks for Visual Recognition 2021년 강의를 듣고 정리한 내용입니다. Lecture3 손실함수에 대해 정리했습니다."
categories: cs231n
tag : [이미지, cnn, cs231n, 리뷰, 정리, 요약, 손실, 최적화, 서포트 벡터 머신, svm, 힌지 로스, hinge loss, 규제, 소프트맥스, softmax]
toc: true
toc_sticky: true
sidebar_main: true

last_modified_at: 2022-04-22
---

<img align='right' width='400' src='https://user-images.githubusercontent.com/78655692/164591233-fbae4704-a1e2-4394-bc36-bdeae75c879b.png'>
본 글은 2021년 4월에 강의한 스탠포드 대학의 "Convolutional Neural Networks for Visual Recognition" 2021년 강의를 듣고 정리한 내용입니다. <br> 개인 공부 목적으로 작성되었으며, 설명이 맞지 않거나 글 오타가 있으면 알려주시길 바랍니다.
{: .notice--info}

원본 링크 : [cs231n.stanford.edu](http://cs231n.stanford.edu/)<br> 한글 번역 링크 : [aikorea.org - cs231n](http://aikorea.org/cs231n/) <br> 강의 링크 : [youtube - 2017 Spring (English)](https://www.youtube.com/playlist?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv)
{: .notice--warning}

<br>
<br>
<br>

## Loss Function

- 이전 내용에서 이미지의 픽셀 값에서 클래스 점수를 산출하는 함수를 정의했으며, 가중치 세트로 파라미터화했다.
- 가중치를 제어하며 예측된 클래스 점수가 학습 데이터의 정답 레이블과 일치하도록 가중치를 설정해본다.
  - 스코어 차이를 정량화하는 **손실 함수(loss function)** 정의
- 그 다음, 손실 함수를 최소화하는 매개변수(parameter)를 효율적으로 찾을 수 있는 방법을 생각한다.
  - **최적화(optimization)** 정의

<br>

- **손실 함수(loss function)**는 현재 분류기(classifier)가 얼마나 좋은지를 알려준다. 
  - 주어진 데이터셋의 샘플 : $\{(x_i,y_i)\}_{i=1}^N$
  - $x_i$ : 이미지
  - $y_i$ : (정수) 라벨(label)
    - 즉, 입력 이미지 x에 대한 정답 카테고리
- 데이터셋에 대한 손실은 각 N개의 샘플에 대한 손실의 평균이다.
  - $L=\frac{1}{N}\sum_iL_i(f(x_i, W),y_i)$
  - $f$ : 입력 이미지 x와 가중치 행렬 w를 입력으로 받아서 새로운 테스트 이미지에 대해 y를 예측
- 딥러닝 알고리즘 : 어떤 x와 y가 존재하고, 가중치 w가 얼마나 좋을지를 정량화하는 손실 함수를 만드는 것 

<br>
<br>

### Multiclass Support Vector Machine loss

- **SVM 손실**은 서포드 벡터 머신이 각 이미지에 대한 올바른 클래스를 원하여 고종된 마진 $\Delta$에 의한 잘못된 클래스보다 다소 높은 점수를 얻도록 설정한다.
  - **서포트 벡터 머신(SVM)** : 결정 경계(Decision Boundary), 즉 분류를 위한 기준 선을 정의하는 모델[^1]
  - **서포트 벡터(support vector)** : 결정 경계에 가장 가까운 각 클래스의 점들
  - **마진(Margin)** : 결정 경계와 서포트 벡터 사이의 거리를 의미

<br>

<img src='https://user-images.githubusercontent.com/78655692/164592587-3cefd141-d50c-439e-9188-e540095067fe.png' width=500>

- 위에서 손실 함수를 정의한 걸 이용한다.
- 스코어 벡터 $s$로 간결하게 나타낸다.
  - $s=f(x_i,W)$
  - 예를 들어 j번째 클래스의 점수 j번째 요소이다.
  - $s_j=f(x_i,W)_j$
- SVM 손실은 다음과 같다.
  - $L_i=\sum_{i\ne j}max(0,s_j-s_{y_i}+1)$
  - $s_j$ : 정답이 아닌 클래스의 스코어
  - $s_{y_i}$ : 정답 클래스의 스코어
  - $1$ : safety margin
  - 여기서 $s_{y_i}$가 $s_j+1$보다 크면 loss는 0이 된다.
  - **힌지 로스(Hinge loss)**

    <img src='https://user-images.githubusercontent.com/78655692/164599325-d3edb19a-b9be-4e94-8d87-179b1ad0a74a.png' width=300>

<br>

- 실제 계산 과정을 알아본다.

<img src='https://user-images.githubusercontent.com/78655692/164614907-e3cf13f0-1487-4c69-a487-5c3d2a4a9279.png' width=650>

- 고양이의 loss는 SVM loss 공식에 따라 계산이 된다.
- max(0, 5.1-3.2+1) + max(0, -1.7-3.2+1)
- = max(0, 2.9) + max(0, -3.9)
- = 2.9

<br>

<img src='https://user-images.githubusercontent.com/78655692/164615179-ef73aff0-4003-4caf-9fe7-dad56e615d87.png' width=650>

- 차의 loss도 위처럼 똑같이 계산한다.
- max(0, 1.3-4.9+1) + max(0, 2.0-4.9+1)
- = max(0, -2.6) + max(0, -1.9)
- = 0

<br>

<img src='https://user-images.githubusercontent.com/78655692/164615400-dda94a76-25d6-4011-8341-c6e4079de89b.png' width=650>

- 다음은 개구리이다.
- max(0, 2.2-(-3.1)+1) + max(0, 2.5-(-3.1)+1)
- = max(0, 6.3) + max(0, 6.6)
- = 12.9

<br>

<img src='https://user-images.githubusercontent.com/78655692/164615643-810c0085-540e-4297-aaa9-d2ea69478d27.png' width=650>

- 각 loss 값을 구했으니 이를 평균낸다.
- L = (2.9 + 0 + 12.9)/3 = 5.27

<br>
<br>

### Question & Answer

- 다음은 강의노트에 나온 질문의 답이다.

1. What happens to loss if car scores change a bit?

   - 자동차의 점수를 4.9에서 3.9로 바꾸어도, loss는 0으로 변함이 없다.
   - 여기서 알 수 있는 점은, SVM 힌지 로스는 데이터에 민감하지 않다.
   - 단지, $s_{y_i}$가 $s_j$보다 높은가에만 관심이 있다.

2. What is the min/max possible loss?

   - min : 손실의 최소는 0이다.
   - max : 손실의 최대는 무한대 값이다.

3. At initialization W is small so all $s\approx0$. What is the loss?

   - 스코어가 0에 가까워 지면 saftety margin만 남게 되고, 각 로스는 1+1=2가 된다.
   - loss = nums of class - 1
   - 이건 디버그 용도로 많이 사용된다. (= sanity check)

4. What if the sum was over all classes? (including $j=y_i$)

   - 만약 정답 클래스 점수를 포함해서 계산하면 어떻게 될까
   - loss 평균이 1이 증가하게 된다.
   - 우리가 찾고 싶은건 loss가 0이 되는 것이다.

5. What if we used mean instead of sum?

   - 합 대신 평균을 낸다면 loss의 스케일만 작아질 뿐이다.
   - No difference!!

6. What if we used $L_i=\sum_{i\ne j}max(0,s_j-s_{y_i}+1)^2$

   - 이 때는 제곱을 하기 때문에 non-linear하게 된다.
   - 따라서 hinge loss 에서 직선이 아닌 곡선으로 올라가게 된다.
   - 하지만, 일반적으로 제곱을 사용하지 않는다.

7. Supposed that we found a W such that L=0. Is this W unique?

   - 아니다. 가중치에 2배를 해도 loss는 0을 갖게 된다.
   - 즉, W는 여러 개가 될 수 있다.

<br>
<br>

## Regularization

<img src='https://user-images.githubusercontent.com/78655692/164605346-8de7abef-4f2b-457c-998c-ff3bae730ae9.png' width=600>

- 학습된 가중치 w는 학습 데이터셋에 맞추어 진행되었다.
- 학습 데이터셋에만 너무 많이 학습하게 되면 **과적합(Overfitting)**이 일어날 수 있다.
- 즉, 테스트셋에도 알맞은 W를 찾아야 한다.
  - **규제(regularization)**을 도입하는 이유이다.
- **Data loss** : 모델 예측은 학습 데이터와 비교한다.
- **Regularization** : 학습 데이터셋에만 맞는 가중치 W를 학습하려고 할때, 어느정도 패널티를 부여한다. 
  - **Occam's Razar** : 가설을 세울 때 단순한 것이 가장 좋다.
  - $\lambda$ : 규제 강도 (hyper parameter)
- **왜 규제를 해야 할까?**
  1. 가중치에 대한 선호도 표현
  2. 모델을 단순하게 만들어 테스트 데이터에서 작동
  3. 곡면성(curvature)을 추가하여 최적화 향상

<br>

- **L1 규제** : $R(W)=\sum_k\sum_l\vert W_{k,l}\vert$
- **L2 규제** : $R(W)=\sum_k\sum_l W_{k,l}^2$
- **Elastic net (L1+L2)**
  - $\sum_k\sum_l\beta W_{k,l}^2+\vert W_{k,l}\vert$

<br>

- 그외 방법들이 있다.
  - 드롭아웃(Dropout)
  - 배치 정규화(Batch normalization)
  - Stochastic depth
  - Fractional pooling

<br>
<br>

## Softmax classifier

- **소프트맥스(Softmax)**는 multinomial logistic regression이라고도 불린다.
- 이전 multiclass SVM loss에서는 스코어 자체는 신경을 안 쓰고 safety margin을 포함해 높은지 낮은지만 판단했다.
- softmax function은 스코어 자체에 추가적인 의미(=확률 분포)를 부여한다.
  - 각 클래스 스코어를 확률 분포 계산
- 소프트맥스 함수
  - $P(Y=k\vert X=x_i)=\frac{e^sk}{\sum_je^sj}$
  - where $f(x_i;W)$ 
- 옳은 클래스의 최대 확률
  - $L_i=-logP(Y=y_i\vert X=x_i)$
- 최종 수식
  - $L_i=-log(\frac{e^sk}{\sum_je^sj})$
  - 다음은 $y=-logx$의 그래프 (-log를 붙이는 이유)
  
    <img src='https://user-images.githubusercontent.com/78655692/164608509-0d86a609-22d6-4166-9a86-ba88cf193310.png' width=350>

<br>

<img src='https://user-images.githubusercontent.com/78655692/164608914-7c732b94-e3f4-42db-8feb-19d322e0990b.png' width=650>

<br>
<br>

## Recap

- 배운 내용을 정리해본다.
- 데이터셋 (dataset)이 있다.
  - $(x,y)$
- 입력 $x$로부터 스코어를 얻기 위해 스코어 함수(score function)를 사용한다.
  - $s=f(x;W)=Wx$
- 손실 함수(loss function)를 사용해서 모델의 예측 값이 정답에 비해 얼마나 차이나는지 측정한다.
- 그리고 모델의 **복잡함**과 **단순함**을 통제하기 위해 규제(regularization)를 추가한다.
  - **softmax**: $L_i=-log(\frac{e^sk}{\sum_je^sj})$
  - **SVM**: $L_i=\sum_{i\ne j}max(0,s_j-s_{y_i}+1)$
  - **Full loss** : $\frac{1}{N}\sum_{i=1}^NL_i+R(W)$

    <img src='https://user-images.githubusercontent.com/78655692/164609487-42e18b4f-c958-47f6-84c6-6a8f071d677c.png' width=450>

- 이 모든걸 합쳐서 최종 손실 함수가 최소가 되게 하는 가중치 행렬 $W$를 구한다.



<br>
<br>
<br>
<br>

## References

[^1]: [서포트 벡터 머신(Support Vector Machine) 쉽게 이해하기 - 아무튼 워라밸](https://hleecaster.com/ml-svm-concept/)