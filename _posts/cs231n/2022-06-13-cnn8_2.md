---
layout: single
title: "[CS231n] 강의10 Long Short Term Memory 리뷰"
excerpt: "본 글은 2022년 4월에 강의한 스탠포드 대학의 Recurrent Neural Networks 2022년 강의를 듣고 정리한 내용입니다. Lecture 10 RNN, LSTM 등이 그 예입니다."
categories: cs231n
tag : [이미지, cnn, cs231n, 리뷰, 정리, 요약, 정의, 란, 딥러닝, rnn, lstm, long short term memory, gru, gradient, gate, input, forget, output, update, reset, gated recurrent unit]
toc: true 
toc_sticky: true
sidebar_main: true

last_modified_at: 2022-06-14
---

<img align='right' width='400' src='https://user-images.githubusercontent.com/78655692/173232201-2db4fd2b-327f-4810-89f0-d4dc12c9acbf.png'>
본 글은 2022년 4월에 강의한 스탠포드 대학의 Recurrent Neural Networks 2022년 강의를 듣고 정리한 내용입니다. <br> 개인 공부 목적으로 작성되었으며, 설명이 맞지 않거나 글 오타가 있으면 알려주시길 바랍니다.
{: .notice--info}

원본 링크 : [cs231n.stanford.edu](http://cs231n.stanford.edu/)<br> 한글 번역 링크 : [aikorea.org - cs231n](http://aikorea.org/cs231n/) <br> 강의 링크 : [youtube - 2017 Spring (English)](https://www.youtube.com/playlist?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv)
{: .notice--warning}

<br>
<br>
<br>

## Introduction to LSTM

<img src='https://user-images.githubusercontent.com/78655692/173285856-8d9fad7d-5c98-412e-8080-64e37e710599.png' width=650>

- LSTM은 RNN과 다르게 4개의 변수가 존재한다.
  - RNN 내용 참고 : [[CS231n] 강의10 Recurrent Neural Networks 리뷰](https://ingu627.github.io/cs231n/cnn8/)
- LSTM의 구조는 위 그림과 같으며, 왜 탄생되었는지에 대한 해답으로 RNN의 gradient 흐름부터 살펴본다.

<br>
<br>

## Vanilla RNN Gradient Flow

<img src='https://user-images.githubusercontent.com/78655692/173324160-43124512-d6e0-46f6-9099-c2fb54be9239.png' width=650>

- 먼저 입력 $x_t$와 이전 hidden state $h_{t-1}$가 들어와 값을 쌓는다(stack). 가중치 행렬 $W$와 곱해진 뒤 tanh를 거쳐 현재 hidden state $h_t$가 된다.
- 빨간색 선이 backpropagation 하는 과정인데, 먼저 $h_t$에 대한 loss gradient 값을 얻어 최종적으로 $h_{t-1}$의 gradient를 계산한다. 하지만, backpropagation 과정에서 tanh와 mul gate를 지나가게 되는데 이는 계산하는 데 비효율적이다.

<br>

<img src='https://user-images.githubusercontent.com/78655692/173348340-3b6c0e73-a073-4df8-bdf9-cc9b50e1ed28.png' width=650>

- 따라서 gradient는 여러 timestep에 걸치게 된다. 위 그림은 출력값을 순차적인 RNN 구조를 따라가 gradient를 계산하는 과정이다.

<br>

<img src='https://user-images.githubusercontent.com/78655692/173351158-084f6d49-831e-4893-901c-99448b8b2310.png' width=650>

- 만약 비선형이 아니고, 곱해지는 값이 1보다 크다면 **exploding gradients**가 발생한다.
- 하지만, exploding gradient는 **gradient clipping**을 둠으로써 어느 정도 해결할 수 있다.
- 문제는, 곱해지는 값이 1보다 작을 경우, **vanishing gradients**가 발생하는데, 이를 해결하기 위해서는 RNN 아키텍처를 바꾸는 수 밖에 없다. 그래서 나온 아키텍처가 LSTM이다.

<br>
<br>

## Long Short Term Memory

<img src='https://user-images.githubusercontent.com/78655692/173353178-d7307edd-d958-4626-af84-4200bfac6b35.png' width=650>

- 다시 LSTM으로 돌아오면, step $t$에는 hidden state $h_t$와 cell state $c_t$가 있다.
  - 모두 n 크기의 벡터이다.
- LSTM은 바로 장기적으로 정보를 저장하는 cell state $c_t$ 를 가지고 있는 것이 Vanilla RNN과의 차이점이다.
- LSTM은 1) 4개의 gate (**i**nput gate, **f**orget gate, **o**utput gate, **g**ate gate), 2) cell state, 3) hidden state로 나눠진다.
- LSTM이 cell을 바꾸는 방법으로 위 gate, input gate, forget gate, output gate에 해당하는 i, f, o를 통해 이루어진다.

<br>

<img src='https://user-images.githubusercontent.com/78655692/173400245-c84cbf6b-e6ba-4c9b-b5d6-fa6a4069f658.png' width=650>

- $x_t$ : 입력 벡터
- $h_{t-1}$ : 이전 hidden 상태
- $c_{t-1}$ : 이전 cell 상태
- $h_t$ : 다음 hidden 상태
- $c_t$ : 다음 cell 상태
- $\odot$ : 요소별 hadamard 내적

<br>

- **g(gate gate)** : output gate와 함께 사용되는 중간 계산 캐시 [^1]
  - 얼마나 셀에 쓸지, 즉 얼마나 input을 들여보낼지
- **i(input gate)** : 이전의 hidden 상태 $h_{t-1}$과 입력 벡터 $x_t$에서 다음 cell 상태 $c_t$에 얼마나 많은 정보를 **추가**해야 하는지를 제어한다.
  - 시그모이드 함수를 가지며 입력을 0~1 사이의 값으로 변환한다.
  - input gate는 gate gate에 의해 생성된 RNN 출력을 취할 것인지 여부를 결정하고 출력을 input gate와 곱한다. ($i\odot g$에 해당)
- **f(forget gate)** : 이전 cell 상태 $c_{t-1}$에서 **제거**해야 하는 정보의 양을 제어한다. ($f\odot c_{t-1}$에 해당)
- **o(output gate)** : 현재 hidden 상태 $h_t$에서 출력으로 **표시**해야 하는 정보의 양을 제어한다.

<br>

<img src='https://user-images.githubusercontent.com/78655692/173401853-e38682d6-87ef-46f0-a804-7ba349f0db13.png' width=650>

- 설명들을 다시 그림으로 나타내면 위와 같다.

<br>

<img src='https://user-images.githubusercontent.com/78655692/173402128-8b9c2b35-76b6-4369-a7a6-bbe092e5593f.png' width=650>

- cell state를 두었기 때문에, $c_t$에서 $c_{t-1}$로 가는 backpropagation 과정에서 기존 RNN과는 달리 가중치 행렬 $W$를 곱하지 않고, 오직 **f** 요소만 요소별 곱셈을 하면 된다.

<br>

<img src='https://user-images.githubusercontent.com/78655692/173402942-d453d109-ea8a-418e-9159-f8da9b98403e.png' width=700>

- recurrent timestep 사이에 흐르는 수평선인 cell 상태를 정보의 흐름이 빠른 고속도로라고 생각하면 된다.
- forget gate의 적절한 파라미터 업데이트를 사용하여 gradient 값을 더 잘 제어할 수 있다.
- 따라서, LSTM 구조는 모델이 정보를 더 오래 기억할 수 있게 제공한다.

<br>
<br>

## Gated Recurrent Unit

- RNN의 다른 변형으로 **GRU (Gated Recurrent Unit)**이 있다.
- 정확히는 LSTM의 간소화 버전이라 할 수 있다.

<img src='https://user-images.githubusercontent.com/78655692/173405664-014044f4-05c7-4f88-8205-726f5646596c.png' width=650> <br> 이미지출처 [^2]

- **update gate** : LSTM의 forget gate와 input gate 역할 모두를 담당한다.
  - 과거의 정보를 얼마만큼 가져가고($z_t$) 현재 새로운 정보를 얼마만큼 가져갈지($1-z_t$)를 정해준다. [^2]
  - **forget gate** : $z_t=\sigma(W_{xz}x_t+W_{hz}h_{t-1}+b_z)$
    - 여기서 $b$는 편향(bias) 벡터를 뜻한다.
  - **input gate** : $1-z_t$

- **reset gate** : 이전 정보에서 얼마만큼 선택해서 내보낼지를 결정한다.
  - $r_t=\sigma(W_{xr}x_t+W_{hr}h_{t-1}+b_r)$
- **hidden 상태** : reset gate, update gate를 모두 적용해서 hidden 상태를 계산한다. 
  - $\tilde h_t = tanh(W_{xh}x_t)$ $+W_{hh}(r_t\odot h_{t-1})+b_h$
  - $h_t = z_t \odot h_{t-1}$$+(1-z_t)\odot \tilde h_t$

<br>
<br>

## Overview

- 다음은 RNN, LSTM, GRU의 아키텍처를 비교한 그림이다.

<img src='https://user-images.githubusercontent.com/78655692/173408222-20a919dc-b195-4917-9bd9-1539f1ab5cc0.png' width=750> <br> 이미지출처 [^3]

<br>
<br>
<br>
<br>

## References

[^1]: [CS231n Convolutional Neural Networks for Visual Recognition](https://cs231n.github.io/rnn/)
[^2]: [LSTM & GRU - Soohwan Kim](https://sooftware.io/lstm_gru/)
[^3]: [RNN, LSTM & GRU - dProgrammer lopez](http://dprogrammer.org/rnn-lstm-gru)
